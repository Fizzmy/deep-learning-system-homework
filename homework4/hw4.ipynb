{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-714 Homework 4\n",
    "\n",
    "In this homework, you will leverage all of the components built in the last three homeworks to solve some modern problems with high performing network structures. We will start by adding a few new ops leveraging our new CPU/CUDA backends. Then, you will implement convolution, and a convolutional neural network to train a classifier on the CIFAR-10 image classification dataset. Then, you will implement recurrent and long-short term memory (LSTM) neural networks, and do word-level prediction language modeling on the Penn Treebank dataset.\n",
    "\n",
    "As always, we will start by copying this notebook and getting the starting code.\n",
    "Reminder: __you must save a copy in drive__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to set up the assignment\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/\n",
    "!mkdir -p 10714\n",
    "%cd /content/drive/MyDrive/10714\n",
    "!git clone https://github.com/dlsys10714/hw4.git\n",
    "%cd /content/drive/MyDrive/10714/hw4\n",
    "\n",
    "!pip3 install --upgrade --no-deps git+https://github.com/dlsys10714/mugrade.git\n",
    "!pip3 install pybind11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Found pybind11: /home/gehao/anaconda3/envs/lxy/lib/python3.7/site-packages/pybind11/include (found version \"2.10.1\")\n",
      "-- Found cuda, building cuda backend\n",
      "Tue Dec  6 14:45:28 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.39       Driver Version: 460.39       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:1A:00.0 Off |                  Off |\n",
      "| N/A   28C    P0    30W / 250W |    831MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE...  Off  | 00000000:1B:00.0 Off |                  Off |\n",
      "| N/A   26C    P0    30W / 250W |   5482MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla P100-PCIE...  Off  | 00000000:1D:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    30W / 250W |   4408MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla P100-PCIE...  Off  | 00000000:1E:00.0 Off |                  Off |\n",
      "| N/A   28C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla P100-PCIE...  Off  | 00000000:3D:00.0 Off |                  Off |\n",
      "| N/A   58C    P0    46W / 250W |  10067MiB / 16280MiB |      8%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla P100-PCIE...  Off  | 00000000:3E:00.0 Off |                  Off |\n",
      "| N/A   46C    P0    36W / 250W |   7201MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla P100-PCIE...  Off  | 00000000:41:00.0 Off |                  Off |\n",
      "| N/A   27C    P0    25W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla P100-PCIE...  Off  | 00000000:42:00.0 Off |                  Off |\n",
      "| N/A   26C    P0    25W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "-- Autodetected CUDA architecture(s):  6.0 6.0 6.0 6.0 6.0 6.0 6.0 6.0\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /home/gehao/lxy/dls/homework4/build\n",
      "make[1]: Entering directory '/home/gehao/lxy/dls/homework4/build'\n",
      "make[2]: Entering directory '/home/gehao/lxy/dls/homework4/build'\n",
      "make[3]: Entering directory '/home/gehao/lxy/dls/homework4/build'\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target ndarray_backend_cpu\u001b[0m\n",
      "make[3]: Leaving directory '/home/gehao/lxy/dls/homework4/build'\n",
      "[ 50%] Built target ndarray_backend_cpu\n",
      "make[3]: Entering directory '/home/gehao/lxy/dls/homework4/build'\n",
      "make[3]: Leaving directory '/home/gehao/lxy/dls/homework4/build'\n",
      "make[3]: Entering directory '/home/gehao/lxy/dls/homework4/build'\n",
      "[ 75%] \u001b[32m\u001b[1mLinking CXX shared module ../python/needle/backend_ndarray/ndarray_backend_cuda.cpython-37m-x86_64-linux-gnu.so\u001b[0m\n",
      "make[3]: Leaving directory '/home/gehao/lxy/dls/homework4/build'\n",
      "[100%] Built target ndarray_backend_cuda\n",
      "make[2]: Leaving directory '/home/gehao/lxy/dls/homework4/build'\n",
      "make[1]: Leaving directory '/home/gehao/lxy/dls/homework4/build'\n"
     ]
    }
   ],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the datasets you will be using for this assignment\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "!mkdir -p './data/ptb'\n",
    "# Download Penn Treebank dataset\n",
    "ptb_data = \"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.\"\n",
    "for f in ['train.txt', 'test.txt', 'valid.txt']:\n",
    "    if not os.path.exists(os.path.join('./data/ptb', f)):\n",
    "        urllib.request.urlretrieve(ptb_data + f, os.path.join('./data/ptb', f))\n",
    "\n",
    "# Download CIFAR-10 dataset\n",
    "if not os.path.isdir(\"./data/cifar-10-batches-py\"):\n",
    "    urllib.request.urlretrieve(\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\", \"./data/cifar-10-python.tar.gz\")\n",
    "    !tar -xvzf './data/cifar-10-python.tar.gz' -C './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish setting up the assignment, go ahead and fill in all the code in `python/needle/autograd.py` using your solution code from the previous homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: ND Backend [10 pts]\n",
    "\n",
    "Fill in the following classes in `python/needle/ops.py`:\n",
    "\n",
    "- `PowerScalar`\n",
    "- `EWiseDiv`\n",
    "- `DivScalar`\n",
    "- `Transpose`\n",
    "- `Reshape`\n",
    "- `BroadcastTo`\n",
    "- `Summation`\n",
    "- `MatMul`\n",
    "- `Negate`\n",
    "- `Log`\n",
    "- `Exp`\n",
    "- `ReLU`\n",
    "- `LogSumExp`\n",
    "- `Tanh` (new)\n",
    "- `Stack` (new)\n",
    "- `Split` (new)\n",
    "\n",
    "Note that for most of these, you already wrote the solutions in the previous homework and you should not need to change your previous solution, however `TanhOp`, `Stack`, and `Split` are newly added. `Stack` concatenates same-sized tensors along a new axis, and `Split` undoes this operation. The gradients of the two operations can be written in terms of each other. We do not directly test `Split`, and only test the backward pass of `Stack` (for which we assume you used `Split`).\n",
    "\n",
    "**Note:** You may want to make your Summation op support sums over multiple axes; you will likely need it for the backward pass of the BroadcastTo op if yours supports broadcasting over multiple axes at a time. However, this is more about ease of use than necessity, and we leave this decision up to you (there are no corresponding tests).\n",
    "\n",
    "**Note:** Depending on your implementations, you may want to ensure that you call `.compact()` before reshaping arrays. (If this is necessary, you will run into corresponding error messages later in the assignment.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.0, pytest-7.2.0, pluggy-1.0.0 -- /home/gehao/anaconda3/envs/lxy/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/gehao/lxy/dls/homework4\n",
      "plugins: hydra-core-1.0.7, anyio-3.6.2\n",
      "collected 1803 items / 1685 deselected / 118 selected                          \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_nd_backend.py::test_ewise_fn[cpu-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m        [  0%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cpu-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m      [  1%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cpu-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m        [  2%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cpu-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m      [  3%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cuda-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m       [  4%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cuda-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m     [  5%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cuda-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m       [  5%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cuda-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m     [  6%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cpu-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m       [  7%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cpu-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m     [  8%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cpu-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m       [  9%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cpu-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 10%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cuda-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m      [ 11%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cuda-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 11%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cuda-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m      [ 12%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cuda-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 13%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-16-16-16] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 14%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-8-8-8] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 15%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-1-2-3] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 16%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-3-4-5] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 16%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-5-4-3] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 17%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-16-16-32] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 18%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-64-64-64] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 19%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-72-72-72] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 20%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-72-73-74] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 21%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-74-73-72] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 22%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-128-128-128] \u001b[32mPASSED\u001b[0m\u001b[32m            [ 22%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-16-16-16] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 23%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-8-8-8] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 24%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-1-2-3] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 25%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-3-4-5] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 26%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-5-4-3] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 27%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-16-16-32] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 27%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-64-64-64] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 28%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-72-72-72] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 29%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-72-73-74] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 30%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-74-73-72] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 31%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-128-128-128] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 32%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_power[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 33%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_power[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 33%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_power[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 34%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_power[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 35%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_log[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 36%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_log[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 37%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_log[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 38%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_log[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 38%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_exp[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 39%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_exp[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 40%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_exp[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 41%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_exp[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 42%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_relu[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 43%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_relu[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 44%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_relu[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 44%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_relu[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 45%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 46%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 47%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 48%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 49%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh_backward[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 50%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh_backward[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 50%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh_backward[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m         [ 51%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh_backward[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m         [ 52%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cpu-shape0-0-1] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 53%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cpu-shape1-0-2] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 54%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cpu-shape2-2-5] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 55%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cuda-shape0-0-1] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 55%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cuda-shape1-0-2] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 56%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cuda-shape2-2-5] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 57%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cpu-shape0-0-1] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 58%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cpu-shape1-0-2] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 59%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cpu-shape2-2-5] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 60%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cuda-shape0-0-1] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 61%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cuda-shape1-0-2] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 61%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cuda-shape2-2-5] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 62%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cpu-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[32m         [ 63%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cpu-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[32m            [ 64%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cpu-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[32m            [ 65%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cpu-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[32m            [ 66%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cuda-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 66%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cuda-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 67%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cuda-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 68%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cuda-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 69%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cpu-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cpu-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 71%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cpu-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 72%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cpu-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 72%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cuda-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cuda-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[32m  [ 74%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cuda-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[32m  [ 75%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cuda-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[32m  [ 76%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_broadcast_to[cpu-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_broadcast_to[cpu-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_broadcast_to[cuda-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_broadcast_to[cuda-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 79%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_reshape[cpu-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[32m      [ 80%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_reshape[cpu-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[32m      [ 81%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_reshape[cuda-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 82%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_reshape[cuda-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 83%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-axes0-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 83%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-axes0-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 84%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-axes1-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 85%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-axes1-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 86%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-None-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m         [ 87%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-None-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m         [ 88%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-axes0-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m       [ 88%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-axes0-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m       [ 89%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-axes1-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m       [ 90%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-axes1-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m       [ 91%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-None-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 92%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-None-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 93%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cpu-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[32m         [ 94%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cpu-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[32m            [ 94%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cpu-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[32m            [ 95%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cpu-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[32m            [ 96%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cuda-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 97%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cuda-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 98%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cuda-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 99%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cuda-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[32m           [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m===================== \u001b[32m\u001b[1m118 passed\u001b[0m, \u001b[33m1685 deselected\u001b[0m\u001b[32m in 2.58s\u001b[0m\u001b[32m =====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest -l -v -k \"nd_backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.0, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /home/gehao/lxy/dls/homework4\n",
      "plugins: hydra-core-1.0.7, anyio-3.6.2\n",
      "collected 10 items / 9 deselected / 1 selected                                 \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_nd_backend.py \n",
      "Submitting new_nd_backend...\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "Grader test 3 passed\n",
      "Grader test 4 passed\n",
      "Grader test 5 passed\n",
      "Grader test 6 passed\n",
      "Grader test 7 passed\n",
      "Grader test 8 passed\n",
      "Grader test 9 passed\n",
      "Grader test 10 passed\n",
      "Grader test 11 passed\n",
      "Grader test 12 passed\n",
      "Grader test 13 passed\n",
      "Grader test 14 passed\n",
      "Grader test 15 passed\n",
      "Grader test 16 passed\n",
      "Grader test 17 passed\n",
      "Grader test 18 passed\n",
      "Grader test 19 passed\n",
      "Grader test 20 passed\n",
      "Grader test 21 passed\n",
      "Grader test 22 passed\n",
      "Grader test 23 passed\n",
      "Grader test 24 passed\n",
      "Grader test 25 passed\n",
      "Grader test 26 passed\n",
      "Grader test 27 passed\n",
      "Grader test 28 passed\n",
      "Grader test 29 passed\n",
      "Grader test 30 passed\n",
      "Grader test 31 passed\n",
      "Grader test 32 passed\n",
      "Grader test 33 passed\n",
      "Grader test 34 passed\n",
      "Grader test 35 passed\n",
      "Grader test 36 passed\n",
      "Grader test 37 passed\n",
      "Grader test 38 passed\n",
      "Grader test 39 passed\n",
      "Grader test 40 passed\n",
      "Grader test 41 passed\n",
      "Grader test 42 passed\n",
      "Grader test 43 passed\n",
      "Grader test 44 passed\n",
      "Grader test 45 passed\n",
      "Grader test 46 passed\n",
      "Grader test 47 passed\n",
      "Grader test 48 passed\n",
      "Grader test 49 passed\n",
      "Grader test 50 passed\n",
      "Grader test 51 passed\n",
      "Grader test 52 passed\n",
      "Grader test 53 passed\n",
      "Grader test 54 passed\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m9 deselected\u001b[0m\u001b[32m in 90.00s (0:01:29)\u001b[0m\u001b[32m ==================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"new_nd_backend\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: CIFAR-10 dataset [10 points]\n",
    "\n",
    "Next, you will write support for the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) image classification dataset, which consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50k training images and 10k test images. \n",
    "\n",
    "Start by implementing the `__init__` function in the `CIFAR10Dataset` class. You can read in the link above how to properly read the CIFAR-10 dataset files you downloaded at the beginning of the homework. Also fill in `__getitem__` and `__len__`. Note that the return shape of the data from `__getitem__` should be in order (3, 32, 32).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.0, pytest-7.2.0, pluggy-1.0.0 -- /home/gehao/anaconda3/envs/lxy/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/gehao/lxy/dls/homework4\n",
      "plugins: hydra-core-1.0.7, anyio-3.6.2\n",
      "collected 1803 items / 1791 deselected / 12 selected                           \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_dataset[True] \u001b[32mPASSED\u001b[0m\u001b[32m          [  8%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_dataset[False] \u001b[32mPASSED\u001b[0m\u001b[32m         [ 16%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_loader[cpu-True-1] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 25%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_loader[cpu-True-15] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 33%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_loader[cpu-False-1] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 41%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_loader[cpu-False-15] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 50%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_loader[cuda-True-1] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 58%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_loader[cuda-True-15] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 66%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_loader[cuda-False-1] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 75%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_loader[cuda-False-15] \u001b[32mPASSED\u001b[0m\u001b[32m  [ 83%]\u001b[0m\n",
      "tests/test_conv.py::test_train_cifar10[needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 91%]\u001b[0m\n",
      "tests/test_conv.py::test_train_cifar10[needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m________ test_train_cifar10[needle.backend_ndarray.ndarray_backend_cpu] ________\u001b[0m\n",
      "\n",
      "device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_train_cifar10\u001b[39;49;00m(device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        dataset = ndl.data.CIFAR10Dataset(\u001b[33m\"\u001b[39;49;00m\u001b[33m./data/cifar-10-batches-py\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, train=\u001b[94mTrue\u001b[39;49;00m)\n",
      "        dataloader = ndl.data.DataLoader(\\\n",
      "                 dataset=dataset,\n",
      "                 batch_size=\u001b[94m128\u001b[39;49;00m,\n",
      "                 shuffle=\u001b[94mFalse\u001b[39;49;00m\n",
      "                 \u001b[90m# collate_fn=ndl.data.collate_ndarray,\u001b[39;49;00m\n",
      "                 \u001b[90m# drop_last=False,\u001b[39;49;00m\n",
      "                 \u001b[90m# device=device,\u001b[39;49;00m\n",
      "                 \u001b[90m# dtype=\"float32\"\u001b[39;49;00m\n",
      "                 )\n",
      "        \u001b[94mfrom\u001b[39;49;00m \u001b[04m\u001b[96mapps\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mmodels\u001b[39;49;00m \u001b[94mimport\u001b[39;49;00m ResNet9\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      ">       model = ResNet9(device=device, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "ResNet9    = <class 'apps.models.ResNet9'>\n",
      "dataloader = <needle.data.DataLoader object at 0x7ff27c4a1ba8>\n",
      "dataset    = <needle.data.CIFAR10Dataset object at 0x7ff27c4a1da0>\n",
      "device     = cpu()\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:466: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <apps.models.ResNet9 object at 0x7ff27c4a1c50>, device = cpu()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION ###\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m() \u001b[90m###\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'apps.models.ResNet9'>\n",
      "device     = cpu()\n",
      "dtype      = 'float32'\n",
      "self       = <apps.models.ResNet9 object at 0x7ff27c4a1c50>\n",
      "\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:14: NotImplementedError\n",
      "\u001b[31m\u001b[1m_______ test_train_cifar10[needle.backend_ndarray.ndarray_backend_cuda] ________\u001b[0m\n",
      "\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_train_cifar10\u001b[39;49;00m(device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        dataset = ndl.data.CIFAR10Dataset(\u001b[33m\"\u001b[39;49;00m\u001b[33m./data/cifar-10-batches-py\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, train=\u001b[94mTrue\u001b[39;49;00m)\n",
      "        dataloader = ndl.data.DataLoader(\\\n",
      "                 dataset=dataset,\n",
      "                 batch_size=\u001b[94m128\u001b[39;49;00m,\n",
      "                 shuffle=\u001b[94mFalse\u001b[39;49;00m\n",
      "                 \u001b[90m# collate_fn=ndl.data.collate_ndarray,\u001b[39;49;00m\n",
      "                 \u001b[90m# drop_last=False,\u001b[39;49;00m\n",
      "                 \u001b[90m# device=device,\u001b[39;49;00m\n",
      "                 \u001b[90m# dtype=\"float32\"\u001b[39;49;00m\n",
      "                 )\n",
      "        \u001b[94mfrom\u001b[39;49;00m \u001b[04m\u001b[96mapps\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mmodels\u001b[39;49;00m \u001b[94mimport\u001b[39;49;00m ResNet9\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      ">       model = ResNet9(device=device, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "ResNet9    = <class 'apps.models.ResNet9'>\n",
      "dataloader = <needle.data.DataLoader object at 0x7ff27c562390>\n",
      "dataset    = <needle.data.CIFAR10Dataset object at 0x7ff27c52c3c8>\n",
      "device     = cuda()\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:466: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <apps.models.ResNet9 object at 0x7ff27c5624e0>, device = cuda()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION ###\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m() \u001b[90m###\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'apps.models.ResNet9'>\n",
      "device     = cuda()\n",
      "dtype      = 'float32'\n",
      "self       = <apps.models.ResNet9 object at 0x7ff27c5624e0>\n",
      "\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:14: NotImplementedError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_train_cifar10[needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_train_cifar10[needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31m================ \u001b[31m\u001b[1m2 failed\u001b[0m, \u001b[32m10 passed\u001b[0m, \u001b[33m1791 deselected\u001b[0m\u001b[31m in 14.62s\u001b[0m\u001b[31m ================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest -l -v -k \"cifar10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.0, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /home/gehao/lxy/dls/homework4\n",
      "plugins: hydra-core-1.0.7, anyio-3.6.2\n",
      "collected 10 items / 9 deselected / 1 selected                                 \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_cifar_ptb_data.py \n",
      "Submitting cifar10...\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "Grader test 3 passed\n",
      "Grader test 4 passed\n",
      "Grader test 5 passed\n",
      "Grader test 6 passed\n",
      "Grader test 7 passed\n",
      "Grader test 8 passed\n",
      "Grader test 9 passed\n",
      "Grader test 10 passed\n",
      "Grader test 11 passed\n",
      "Grader test 12 passed\n",
      "Grader test 13 passed\n",
      "Grader test 14 passed\n",
      "Grader test 15 passed\n",
      "Grader test 16 passed\n",
      "Grader test 17 passed\n",
      "Grader test 18 passed\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m9 deselected\u001b[0m\u001b[32m in 39.99s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"cifar10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 3: Convolutional neural network [40 points]\n",
    "\n",
    "Here's an outline of what you will do in this task.\n",
    "\n",
    "In `python/needle/backend_ndarray/ndarray.py`, implement:\n",
    "- `flip`\n",
    "- `pad`\n",
    "\n",
    "In `python/needle/ops.py`, implement (forward and backward):\n",
    "- `Flip`\n",
    "- `Dilate`\n",
    "- `UnDilate`\n",
    "- `Conv`\n",
    "\n",
    "In `python/needle/nn.py`, implement:\n",
    "- `Flatten`\n",
    "- `Conv`\n",
    "\n",
    "In `python/apps/models.py`, fill in the `ResNet9` class.  \n",
    "\n",
    "In `apps/simple_training.py`, fill in:\n",
    "- `epoch_general_cifar10`,\n",
    "- `train_cifar10`\n",
    "- `evaluate_cifar10`\n",
    "\n",
    "We have provided a `BatchNorm2d` implementation for you as a wrapper around your previous `BatchNorm1d` implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding ndarrays\n",
    "\n",
    "Convolution as typically implemented in deep learning libraries cuts down the size of inputs;\n",
    "e.g., a (1, 32, 32, 3) image convolved with a 3x3 filter would give a (1, 30, 30, c) output.\n",
    "A way around this is to pad the input ndarray before performing convolution, e.g., pad with zeros to get a (1, 34, 34, 3) ndarray so that the result is (1, 32, 32, 3). \n",
    "\n",
    "Padding is also required for the backward pass of convolution.\n",
    "\n",
    "You should implement `pad` in `ndarray.py` to closely reflect the behavior of `np.pad`.\n",
    "That is, `pad` should take a tuple of 2-tuples with length equal to the number of dimensions of the array,\n",
    "where each element in the 2-tuple corresponds to \"left padding\" and \"right padding\", respectively.\n",
    "\n",
    "For example, if `A` is a (10, 32, 32, 8) ndarray (think NHWC), then `A.pad( (0, 0), (2, 2), (2, 2), (0, 0) )` would be a (10, 36, 36, 8) ndarray where the \"spatial\" dimension has been padded by two zeros on all sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.0, pytest-7.2.0, pluggy-1.0.0 -- /home/gehao/anaconda3/envs/lxy/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/gehao/lxy/dls/homework4\n",
      "plugins: hydra-core-1.0.7, anyio-3.6.2\n",
      "collected 1803 items / 1801 deselected / 2 selected                            \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_conv.py::test_pad_forward[params0-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_conv.py::test_pad_forward[params1-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m1801 deselected\u001b[0m\u001b[32m in 1.70s\u001b[0m\u001b[32m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest -l -v -k \"pad_forward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flipping ndarrays & FlipOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ctypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utility code for a demonstration below which you can probably ignore. It might be instructive to check out the `offset` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads off the underlying data array in order (i.e., offset 0, offset 1, ..., offset n)\n",
    "# i.e., ignoring strides\n",
    "def raw_data(X):\n",
    "    X = np.array(X) # copy, thus compact X\n",
    "    return np.frombuffer(ctypes.string_at(X.ctypes.data, X.nbytes), dtype=X.dtype, count=X.size)\n",
    "\n",
    "# Xold and Xnew should reference the same underlying data\n",
    "def offset(Xold, Xnew):\n",
    "    assert Xold.itemsize == Xnew.itemsize\n",
    "    # compare addresses to the beginning of the arrays\n",
    "    return (Xnew.ctypes.data - Xold.ctypes.data)//Xnew.itemsize\n",
    "\n",
    "def strides(X):\n",
    "    return ', '.join([str(x//X.itemsize) for x in X.strides])\n",
    "\n",
    "def format_array(X, shape):\n",
    "    assert len(shape) == 3, \"I only made this formatting work for ndims = 3\"\n",
    "    def chunks(l, n):\n",
    "        n = max(1, n)\n",
    "        return (l[i:i+n] for i in range(0, len(l), n))\n",
    "    a = [str(x) if x >= 10 else ' ' + str(x) for x in X]\n",
    "    a = ['(' + ' '.join(y) + ')' for y in [x for x in chunks(a, shape[-1])]]\n",
    "    a = ['|' + ' '.join(y) + '|' for y in [x for x in chunks(a, shape[-2])]]\n",
    "    return '  '.join(a)\n",
    "\n",
    "def inspect_array(X, *, is_a_copy_of):\n",
    "    # compacts X, then reads it off in order\n",
    "    print('Data: %s' % format_array(raw_data(X), X.shape))\n",
    "    # compares address of X to copy_of, thus finding X's offset\n",
    "    print('Offset: %s' % offset(is_a_copy_of, X))\n",
    "    print('Strides: %s' % strides(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In order to implement the backwards pass of 2D convolution, we will (probably) need a function which _flips_\n",
    "axes of ndarrays. We say \"probably\" because you could probably cleverly implement your convolution forward\n",
    "function to avoid this. However, we think it is easiest to think about this if you have the ability to \"flip\" the kernel along its vertical and horizontal dimensions.\n",
    "\n",
    "We will try to build up your intuition for the \"flip\" operation below in order to help you figure out how to implement it in `ndarray.py`. To do that, we explore numpy's `np.flip` function below. One thing to note is that\n",
    "`flip` is typically implemented by using negative strides and changing the _offset_ of the underlying array.\n",
    "\n",
    "For example, flipping an array on _all_ of its axes is equivalent to reversing the array. In this case, you can imagine that we would want all the strides to be negative, and the offset to be the length of the array (to start at the end of the array and \"stride\" backwards).\n",
    "\n",
    "Since we did not explicitly support negative strides in our implementation for the last homework, we will merely call `NDArray.make` with them to make our \"flipped\" array and then immediately call `.compact()`. Other than changing unsigned ints to signed ints in a few places, we suspect your existing `compact` function should not have to change at all to accomodate negative strides. In the .cc and .cu files we distributed, we have already changed the function signatures to reflect this.\n",
    "\n",
    "Alternatively, you could simply implement `flip` in the CPU backend by copying memory, which you _may_ find more intuitive. We suggest following our mini tutorial below to keep your implementation Python-focused, since we believe it is involves approximately the same amount of effort to implement it slightly more naively in C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this array as reference for the other examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: |( 1  2  3  4) ( 5  6  7  8)|  |( 9 10 11 12) (13 14 15 16)|  |(17 18 19 20) (21 22 23 24)|\n",
      "Offset: 0\n",
      "Strides: 8, 4, 1\n"
     ]
    }
   ],
   "source": [
    "A = np.arange(1, 25).reshape(3, 2, 4)\n",
    "inspect_array(A, is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have put brackets around each axis of the array. Notice that for this array, the offset is 0 and the strides are all positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what happens when you flip the array along the last axis below. \n",
    "Note that the `inspect_array` function compacts the array after flipping it so you can see the\n",
    "\"logical\" order of the data, and the offset is calculated by comparing the address of the **non**-compacted\n",
    "flipped array with that of `is_copy_of`, i.e., the array `A` we looked at above.\n",
    "\n",
    "That is, we are looking at how numpy calculates the strides and offset for flipped arrays in order\n",
    "to copy this behavior in our own implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: |( 4  3  2  1) ( 8  7  6  5)|  |(12 11 10  9) (16 15 14 13)|  |(20 19 18 17) (24 23 22 21)|\n",
      "Offset: 3\n",
      "Strides: 8, 4, -1\n"
     ]
    }
   ],
   "source": [
    "inspect_array(np.flip(A, (2,)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So flipping the last axis reverses the order of the elements within each 4-dimensional \"cell\", as you can see above. The stride corresponding to the axis we flipped has been negated. And the offset is 3 -- this makes sense, e.g., because we want the new \"first\" element of the array to be 4, which was at index 3 in `A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: |( 5  6  7  8) ( 1  2  3  4)|  |(13 14 15 16) ( 9 10 11 12)|  |(21 22 23 24) (17 18 19 20)|\n",
      "Offset: 4\n",
      "Strides: 8, -4, 1\n"
     ]
    }
   ],
   "source": [
    "inspect_array(np.flip(A, (1,)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again for the middle axis: we negate the middle stride, and the offset is 4, which seems reasonable since we now want the first element to be 5, which was at index 4 in the original array `A`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: |(17 18 19 20) (21 22 23 24)|  |( 9 10 11 12) (13 14 15 16)|  |( 1  2  3  4) ( 5  6  7  8)|\n",
      "Offset: 16\n",
      "Strides: -8, 4, 1\n"
     ]
    }
   ],
   "source": [
    "inspect_array(np.flip(A, (0,)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to infer the more general algorithm for computing the offset given the axis to flip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe what happens when we flip _all_ axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: |(24 23 22 21) (20 19 18 17)|  |(16 15 14 13) (12 11 10  9)|  |( 8  7  6  5) ( 4  3  2  1)|\n",
      "Offset: 23\n",
      "Strides: -8, -4, -1\n"
     ]
    }
   ],
   "source": [
    "inspect_array(np.flip(A, (0,1,2)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, the offset is then sufficient to point to the last element of the array, and this is just the \"reverse order\" version of `A`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we flip just axes 1 and 0..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: |(21 22 23 24) (17 18 19 20)|  |(13 14 15 16) ( 9 10 11 12)|  |( 5  6  7  8) ( 1  2  3  4)|\n",
      "Offset: 20\n",
      "Strides: -8, -4, 1\n"
     ]
    }
   ],
   "source": [
    "inspect_array(np.flip(A, (0,1)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The offset is 20. Looking back on our previous offset computations, do you notice something?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "\n",
    "With this exploration of numpy's ndarray flipping functionality, which uses negative strides and a custom offset,\n",
    "try to implement `flip` in `ndarray.py`. You also must implement \"flip\" forward and backward functions in `ops.py`; note that these should be extremely short.\n",
    "\n",
    "**Important:** You should call NDArray.make with the new strides and offset, and then immediately `.compact()` this array. The resulting array is then copied and has positive strides. We want this (less-than-optimal) behavior because we did not account for negative strides in our previous implementation. _Aside:_ If you want, consider where/if negative strides break your implementation. `__getitem__` definitely doesn't work due to how we processed slices; is there anything else? (_Note_: this isn't graded.)\n",
    "\n",
    "Also, if you want to instead add a `flip` operator on the CPU/CUDA backends, that's also okay.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.0, pytest-7.2.0, pluggy-1.0.0 -- /home/gehao/anaconda3/envs/lxy/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/gehao/lxy/dls/homework4\n",
      "plugins: hydra-core-1.0.7, anyio-3.6.2\n",
      "collected 1803 items / 1763 deselected / 40 selected                           \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_conv.py::test_flip_forward[params0-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params0-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params1-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params1-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params2-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params2-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params3-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params3-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params4-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params4-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params5-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params5-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params6-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params6-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params7-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params7-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params8-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params8-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params9-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params9-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params0-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params0-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params1-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params1-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params2-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params2-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params3-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params3-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params4-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params4-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params5-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params5-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params6-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params6-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params7-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params7-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params8-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 92%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params8-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 95%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params9-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params9-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m===================== \u001b[32m\u001b[1m40 passed\u001b[0m, \u001b[33m1763 deselected\u001b[0m\u001b[32m in 2.98s\u001b[0m\u001b[32m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest -l -v -k \"flip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dilation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dilation operator puts zeros between elements of an ndarray. We will need it for computing the backward pass of convolution when the stride of the convolution is greater than 1. As an example, dilation should do the following to a 2x2 matrix when dilated by 1 on both axes:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}\n",
    "\\Longrightarrow\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 2 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "3 & 0 & 4 & 0 \\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To get some intuition for why we need dilation for the backward pass of strided convolution, consider a  `stride=2`, `padding=\"same\"`, `input_channels=output_channels=8` convolution applied to an input of size (10, 32, 32, 8). The resulting output will be of size (10, 16, 16, 8) due to the stride, and thus `out_grad` will have shape (10, 16, 16, 8). Yet, the gradient of the input needs to, of course, have shape (10, 32, 32, 8) -- so we must need to increase the size of `out_grad` in some way. Consider also that you could implement strided convolution as `Conv(x)[:, ::2, ::2, :]`, i.e., only keeping every other pixel in the spatial dimension.\n",
    "\n",
    "\n",
    "Implement `Dilate` in `ops.py`. This function takes two additional parameters (in attrs): the `dilation` amount and the `axes` to dilate. You must also implement the corresponding op `UnDilate`, whose forward pass will be used to implement the gradient of `Dilate`. (This is so we do not have to implement `GetItem` and `SetItem` ops, which can be highly inefficient to backprop through without additional optimizations.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.0, pytest-7.2.0, pluggy-1.0.0 -- /home/gehao/anaconda3/envs/lxy/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/gehao/lxy/dls/homework4\n",
      "plugins: hydra-core-1.0.7, anyio-3.6.2\n",
      "collected 1803 items / 1777 deselected / 26 selected                           \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_conv.py::test_dilate_forward[needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_forward[needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params0-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params0-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params1-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params1-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params2-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params2-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params3-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params3-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params4-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params4-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params5-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params5-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params6-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params6-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params7-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params7-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 69%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params8-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params8-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 76%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params9-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params9-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params10-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 88%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params10-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 92%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params11-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params11-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m===================== \u001b[32m\u001b[1m26 passed\u001b[0m, \u001b[33m1777 deselected\u001b[0m\u001b[32m in 2.81s\u001b[0m\u001b[32m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"dilate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit new ops (flip/dilation) to mugrade [10 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.0, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /home/gehao/lxy/dls/homework4\n",
      "plugins: hydra-core-1.0.7, anyio-3.6.2\n",
      "collected 10 items / 9 deselected / 1 selected                                 \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_conv.py \n",
      "Submitting new_ops...\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "Grader test 3 passed\n",
      "Grader test 4 passed\n",
      "Grader test 5 passed\n",
      "Grader test 6 passed\n",
      "Grader test 7 passed\n",
      "Grader test 8 passed\n",
      "Grader test 9 passed\n",
      "Grader test 10 passed\n",
      "Grader test 11 passed\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m9 deselected\u001b[0m\u001b[32m in 18.90s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"new_ops\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution forward\n",
    "\n",
    "Implement the forward pass of 2D multi-channel convolution in `ops.py`. You should probably refer to [this notebook](https://github.com/dlsyscourse/public_notebooks/blob/main/convolution_implementation.ipynb) from lecture, which implements 2D multi-channel convolution using im2col in numpy.\n",
    "\n",
    "**Note:** Your convolution op should accept tensors in the NHWC format, as in the example above, and weights in the format (kernel_size, kernel_size, input_channels, output_channels).\n",
    "\n",
    "However, you will need to add two additional features. Your convolution function should accept arguments for `padding` (default 0) and `stride` (default 1). For `padding`, you should simply apply your padding function to the spatial dimensions (i.e., axes 1 and 2). \n",
    "\n",
    "Implementing strided convolution should consist of a relatively small set of changes to your plain convolution implementation.\n",
    "\n",
    "We recommend implementing convolution without stride first, ensuring you pass some of the tests below, and then adding in stride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.0, pytest-7.2.0, pluggy-1.0.0 -- /home/gehao/anaconda3/envs/lxy/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/gehao/lxy/dls/homework4\n",
      "plugins: hydra-core-1.0.7, anyio-3.6.2\n",
      "collected 1803 items / 1769 deselected / 34 selected                           \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape0-W_shape0-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape1-W_shape1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape2-W_shape2-1-2] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape3-W_shape3-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape4-W_shape4-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape5-W_shape5-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape6-W_shape6-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape7-W_shape7-2-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape8-W_shape8-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape9-W_shape9-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape10-W_shape10-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape11-W_shape11-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape12-W_shape12-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape13-W_shape13-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape14-W_shape14-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape15-W_shape15-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape16-W_shape16-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape0-W_shape0-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape1-W_shape1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape2-W_shape2-1-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape3-W_shape3-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape4-W_shape4-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape5-W_shape5-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape6-W_shape6-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape7-W_shape7-2-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape8-W_shape8-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 76%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape9-W_shape9-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 79%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape10-W_shape10-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape11-W_shape11-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape12-W_shape12-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 88%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape13-W_shape13-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 91%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape14-W_shape14-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 94%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape15-W_shape15-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape16-W_shape16-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m===================== \u001b[32m\u001b[1m34 passed\u001b[0m, \u001b[33m1769 deselected\u001b[0m\u001b[32m in 5.23s\u001b[0m\u001b[32m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"op_conv and forward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the gradients of 2D multi-channel convolution can be technically quite challenging (especially \"rigorously\"). We will try to provide some useful hints here. Basically, we encourage you to make use of the surprising fact that _whatever makes the dimensions work out is typically right_.\n",
    "\n",
    "Ultimately, the backward pass of convolution can be done in terms of the convolution operator itself, with some clever manipulations using `flip`, `dilate`, and multiple applications of `transpose` to both the arguments and the results.\n",
    "\n",
    "In the last section, we essentially implemented convolution as a matrix product: ignoring the various restride and reshape operations, we basically have something like `X @ W`, where `X` is the input and `W` is the weight. We also have `out_grad`, which is the same shape as `X @ W`. Now, you have already implemented the backward pass of matrix multiplication in a previous assignment, and we can use this knowledge to get some insight into the backward pass of convolution. In particular, referencing your matmul backward implementation, you may notice (heuristically speaking here):\n",
    "\n",
    "`X.grad = out_grad @ W.transpose` \\\n",
    "`W.grad = X.transpose @ out_grad`\n",
    "\n",
    "Surprisingly enough, things work out if we just assume that these are also convolutions (and now assuming that `out_grad`, `W`, and `X` are tensors amenable to 2D multi-channel convolution instead of matrices):\n",
    "\n",
    "`X.grad = ≈conv(≈out_grad, ≈W)` \\\n",
    "`W.grad = ≈conv(≈X, ≈out_grad)`\n",
    "\n",
    "In which the \"≈\" indicates that you need to apply some additional operators to these terms in order to get the dimensions to work out, such as permuting/transposing axes, dilating, changing the `padding=` argument to the convolution function, or permuting/transposing axes of the resulting convolution.\n",
    "\n",
    "As we saw on the [last few slides here](https://dlsyscourse.org/slides/conv_nets.pdf) in class, the transpose of a convolution can be found by simply flipping the kernel. Since we're working in 2D instead of 1D, this means flipping the kernel both vertically and horizontally (thus why we implemented `flip`).\n",
    "\n",
    "Summarizing some hints for both `X.grad` and `W.grad`:\n",
    "\n",
    "`X.grad`\n",
    "- The convolution of `out_grad` and `W`, with some operations applied to those\n",
    "- `W` should be flipped over both the kernel dimensions\n",
    "- If the convolution is strided, increase the size of `out_grad` with a corresponding dilation\n",
    "- Do an example to analyze dimensions: note the shape you want for `X.grad`, and think about how you must permute/transpose the arguments and add padding to the convolution to achieve this shape \n",
    "    - This padding depends on both the kernel size and the `padding` argument to the convolution\n",
    "\n",
    "`W.grad`\n",
    "- The convolution of `X` and `out_grad`, with some operations applied to those\n",
    "- The gradients of `W` must be accumulated over the batches; how can you make the conv operator itself do this accumulation?\n",
    "    - Consider turning batches into channels via transpose/permute\n",
    "- Analyze dimensions: how can you modify `X` and `out_grad` so that the shape of their convolution matches the shape of `W`? You may need to transpose/permute the result.\n",
    "    - Remember to account for the `padding` argument passed to convolution\n",
    "\n",
    "General tips\n",
    "- Deal with strided convolutions last (you should be able to just drop in `dilate` when you've passed most of the tests)\n",
    "- Start with the case where `padding=0`, then consider changing `padding` arguments\n",
    "- You can \"permute\" axes with multiple calls to `transpose`\n",
    "\n",
    "It might also be useful to skip ahead to nn.Conv, pass the forward tests, and then use both the tests below and the nn.Conv backward tests to debug your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.0, pytest-7.2.0, pluggy-1.0.0 -- /home/gehao/anaconda3/envs/lxy/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/gehao/lxy/dls/homework4\n",
      "plugins: hydra-core-1.0.7, anyio-3.6.2\n",
      "collected 1803 items / 1769 deselected / 34 selected                           \u001b[0m\u001b[1m\u001b[1m\n",
      "\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape0-W_shape0-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape1-W_shape1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape2-W_shape2-1-2] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape3-W_shape3-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape4-W_shape4-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape5-W_shape5-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape6-W_shape6-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape7-W_shape7-2-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape8-W_shape8-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape9-W_shape9-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape10-W_shape10-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape11-W_shape11-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape12-W_shape12-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape13-W_shape13-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape14-W_shape14-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape15-W_shape15-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape16-W_shape16-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape0-W_shape0-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape1-W_shape1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape2-W_shape2-1-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape3-W_shape3-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape4-W_shape4-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape5-W_shape5-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape6-W_shape6-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape7-W_shape7-2-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape8-W_shape8-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 76%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape9-W_shape9-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 79%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape10-W_shape10-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape11-W_shape11-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape12-W_shape12-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 88%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape13-W_shape13-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 91%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape14-W_shape14-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 94%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape15-W_shape15-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape16-W_shape16-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m===================== \u001b[32m\u001b[1m34 passed\u001b[0m, \u001b[33m1769 deselected\u001b[0m\u001b[32m in 28.16s\u001b[0m\u001b[32m =====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"op_conv and backward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Fixing init._calculate_fans for convolution\n",
    "Previously, we have implemented Kaiming uniform/normal initializations, where we essentially assigned `fan_in = input_size` and `fan_out = output_size`.\n",
    "For convolution, this becomes somewhat more detailed, in that you should multiply both of these by the \"receptive field size\", which is in this case just the product of the kernel sizes -- which in our case are always going to be the same, i.e., $k\\times k$ kernels.\n",
    "\n",
    "**You will need to edit your `kaiming_uniform`, etc. init functions to support multidimensional arrays.** In particular, you should add a new `shape` argument which is then passed to, e.g., the underlying `rand` function.\n",
    "\n",
    "You can test this below; though it is not _directly_ graded, it must match ours to pass the nn.Conv mugrade tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.0, pytest-7.2.0, pluggy-1.0.0 -- /home/gehao/anaconda3/envs/lxy/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/gehao/lxy/dls/homework4\n",
      "plugins: hydra-core-1.0.7, anyio-3.6.2\n",
      "collected 1803 items / 1801 deselected / 2 selected                            \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_conv.py::test_init_kaiming_uniform[needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_conv.py::test_init_kaiming_uniform[needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m1801 deselected\u001b[0m\u001b[32m in 1.99s\u001b[0m\u001b[32m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"kaiming_uniform\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing nn.Conv\n",
    "\n",
    "Essentially, nn.Conv is just a wrapper of the convolution operator we previously implemented\n",
    "which adds a bias term, initializes the weight and bias, and ensures that the padding is set so that the input and output dimensions are the same (in the `stride=1` case, anyways). \n",
    "\n",
    "Importantly, nn.Conv should support NCHW format instead of NHWC format. In particular, we think this makes more sense given our current BatchNorm implementation. You can implement this by applying `transpose` twice to both the input and output.  \n",
    "\n",
    "- Ensure nn.Conv works for (N, C, H, W) tensors even though we implemented the conv op for (N, H, W, C) tensors\n",
    "- Initialize the (k, k, i, o) weight tensor using Kaiming uniform initialization with default settings\n",
    "- Initialize the (o,) bias tensor using uniform initialization on the interval $\\pm$`1.0/(in_channels * kernel_size**2)**0.5`\n",
    "- Calculate the appropriate padding to ensure input and output dimensions are the same\n",
    "- Calculate the convolution, then add the properly-broadcasted bias term if present\n",
    "\n",
    "You can now test your nn.Conv against PyTorch's nn.Conv2d with the two PyTest calls below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.0, pytest-7.2.0, pluggy-1.0.0 -- /home/gehao/anaconda3/envs/lxy/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/gehao/lxy/dls/homework4\n",
      "plugins: hydra-core-1.0.7, anyio-3.6.2\n",
      "collected 1803 items / 1793 deselected / 10 selected                           \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cpu-4-8-16-3-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cpu-32-8-16-3-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cpu-32-8-8-3-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cpu-32-16-8-3-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cpu-32-16-8-3-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-4-8-16-3-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-32-8-16-3-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-32-8-8-3-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-32-16-8-3-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-32-16-8-3-2] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m===================== \u001b[32m\u001b[1m10 passed\u001b[0m, \u001b[33m1793 deselected\u001b[0m\u001b[32m in 2.98s\u001b[0m\u001b[32m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"nn_conv_forward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.0, pytest-7.2.0, pluggy-1.0.0 -- /home/gehao/anaconda3/envs/lxy/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/gehao/lxy/dls/homework4\n",
      "plugins: hydra-core-1.0.7, anyio-3.6.2\n",
      "collected 1803 items / 1789 deselected / 14 selected                           \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-4-1-1-3-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-8-16-3-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-8-16-3-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-8-8-3-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-8-8-3-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-16-8-3-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-16-8-3-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-4-1-1-3-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-8-16-3-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-8-16-3-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-8-8-3-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-8-8-3-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-16-8-3-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 92%]\u001b[0m\n",
      "tests/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-16-8-3-2] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m===================== \u001b[32m\u001b[1m14 passed\u001b[0m, \u001b[33m1789 deselected\u001b[0m\u001b[32m in 2.67s\u001b[0m\u001b[32m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"nn_conv_backward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit nn.Conv to mugrade [20 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.0, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /home/gehao/lxy/dls/homework4\n",
      "plugins: hydra-core-1.0.7, anyio-3.6.2\n",
      "collected 10 items / 9 deselected / 1 selected                                 \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_conv.py \n",
      "Submitting conv_forward...\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "Grader test 3 passed\n",
      "Grader test 4 passed\n",
      "Grader test 5 passed\n",
      "Grader test 6 passed\n",
      "Grader test 7 passed\n",
      "Grader test 8 passed\n",
      "Grader test 9 passed\n",
      "Grader test 10 passed\n",
      "Grader test 11 passed\n",
      "Grader test 12 passed\n",
      "Grader test 13 passed\n",
      "Grader test 14 passed\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m9 deselected\u001b[0m\u001b[32m in 26.58s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"conv_forward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.0, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /home/gehao/lxy/dls/homework4\n",
      "plugins: hydra-core-1.0.7, anyio-3.6.2\n",
      "collected 10 items / 9 deselected / 1 selected                                 \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_conv.py \n",
      "Submitting conv_backward...\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "Grader test 3 passed\n",
      "Grader test 4 passed\n",
      "Grader test 5 passed\n",
      "Grader test 6 passed\n",
      "Grader test 7 passed\n",
      "Grader test 8 passed\n",
      "Grader test 9 passed\n",
      "Grader test 10 passed\n",
      "Grader test 11 passed\n",
      "Grader test 12 passed\n",
      "Grader test 13 passed\n",
      "Grader test 14 passed\n",
      "Grader test 15 passed\n",
      "Grader test 16 passed\n",
      "Grader test 17 passed\n",
      "Grader test 18 passed\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m9 deselected\u001b[0m\u001b[32m in 39.52s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"conv_backward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Implementing \"ResNet9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now use your convolutional layer to implement a model similar to _ResNet9_, which is known to be a reasonable model for getting good accuracy on CIFAR-10 quickly (see [here](https://github.com/davidcpage/cifar10-fast)). Our main change is that we used striding instead of pooling and divided all of the channels by 4 for the sake of performance (as our framework is not as well-optimized as industry-grade frameworks).\n",
    "\n",
    "In the figure below, before the linear layer, you should \"flatten\" the tensor. We have added a module called `Flatten` in `nn.py` that you can complete and use, or you can simply use `.reshape` in the `forward()` method of your ResNet9.\n",
    "\n",
    "Make sure that you pass the device to all modules in your model; otherwise, you will get errors about mismatched devices when trying to run with CUDA.\n",
    "\n",
    "<center><img src=\"https://github.com/dlsyscourse/hw4/blob/main/ResNet9.png?raw=true\" alt=\"ResNet9\" style=\"width: 400px;\" /></center>\n",
    "\n",
    "We have tried to make it easier to pass the tests here than for previous assignments where you have implemented models. In particular, we are just going to make sure it has the right number of parameters and similar accuracy and loss after 1 or 2 batches of CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.0, pytest-7.2.0, pluggy-1.0.0 -- /home/gehao/anaconda3/envs/lxy/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/gehao/lxy/dls/homework4\n",
      "plugins: hydra-core-1.0.7, anyio-3.6.2\n",
      "collected 1803 items / 1801 deselected / 2 selected                            \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_conv.py::test_resnet9[needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_conv.py::test_resnet9[needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m1801 deselected\u001b[0m\u001b[32m in 1.97s\u001b[0m\u001b[32m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"resnet9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.0, pytest-7.2.0, pluggy-1.0.0 -- /home/gehao/anaconda3/envs/lxy/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/gehao/lxy/dls/homework4\n",
      "plugins: hydra-core-1.0.7, anyio-3.6.2\n",
      "collected 1803 items / 1801 deselected / 2 selected                            \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_conv.py::test_train_cifar10[needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_conv.py::test_train_cifar10[needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[33m [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "tests/test_conv.py::test_train_cifar10[needle.backend_ndarray.ndarray_backend_cpu]\n",
      "tests/test_conv.py::test_train_cifar10[needle.backend_ndarray.ndarray_backend_cuda]\n",
      "  /home/gehao/lxy/dls/homework4/tests/test_conv.py:468: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "    assert np.linalg.norm(np.array(list(out)) - np.array([0.09375, 3.5892258])) < 1e-2\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m================ \u001b[32m2 passed\u001b[0m, \u001b[33m\u001b[1m1801 deselected\u001b[0m, \u001b[33m\u001b[1m2 warnings\u001b[0m\u001b[33m in 7.63s\u001b[0m\u001b[33m ================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"train_cifar10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit ResNet9 to mugrade [10 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.0, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /home/gehao/lxy/dls/homework4\n",
      "plugins: hydra-core-1.0.7, anyio-3.6.2\n",
      "collected 10 items / 9 deselected / 1 selected                                 \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_conv.py \n",
      "Submitting resnet9...\n",
      "Grader test 1 passed\n",
      "0.0859375 [4.163111]\n",
      "0.11328125 [3.7954755]\n",
      "Grader test 2 passed\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "tests/test_conv.py::submit_resnet9\n",
      "  ./python/needle/backend_ndarray/ndarray.py:113: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "    array = NDArray(np.array(other), device=device)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m================= \u001b[32m1 passed\u001b[0m, \u001b[33m\u001b[1m9 deselected\u001b[0m, \u001b[33m\u001b[1m1 warning\u001b[0m\u001b[33m in 11.84s\u001b[0m\u001b[33m ==================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"resnet9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can train your model on CIFAR-10 using the following code. Note that this is likely going to be quite slow, and also  not all that accurate due to the lack of data augmentation. You should expect it to take around 500s per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  correct: 0.38844  loss: [1.7023154]\n",
      "1  correct: 0.494  loss: [1.3993567]\n",
      "2  correct: 0.54346  loss: [1.2712132]\n",
      "3  correct: 0.57732  loss: [1.1791761]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_179683/2225046042.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet9\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m train_cifar10(model, dataloader, n_epochs=10, optimizer=ndl.optim.Adam,\n\u001b[0;32m---> 20\u001b[0;31m       lr=0.001, weight_decay=0.001)\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mevaluate_cifar10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gehao/lxy/dls/homework4/apps/simple_training.py\u001b[0m in \u001b[0;36mtrain_cifar10\u001b[0;34m(model, dataloader, n_epochs, optimizer, lr, weight_decay, loss_fn)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_general_cifar10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" correct:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" loss:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m### END YOUR SOLUTION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gehao/lxy/dls/homework4/apps/simple_training.py\u001b[0m in \u001b[0;36mepoch_general_cifar10\u001b[0;34m(dataloader, model, loss_fn, opt)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mtotnum\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotnum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gehao/lxy/dls/homework4/python/needle/autograd.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, out_grad)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mout_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_grad\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mout_grad\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mcompute_gradient_of_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gehao/lxy/dls/homework4/python/needle/autograd.py\u001b[0m in \u001b[0;36mcompute_gradient_of_variables\u001b[0;34m(output_tensor, out_grad)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_as_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode_to_output_grads_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gehao/lxy/dls/homework4/python/needle/autograd.py\u001b[0m in \u001b[0;36mgradient_as_tuple\u001b[0;34m(self, out_grad, node)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgradient_as_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_grad\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Value\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;34m\"\"\" Convenience method to always return a tuple from gradient call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gehao/lxy/dls/homework4/python/needle/ops.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, out_grad, node)\u001b[0m\n\u001b[1;32m    658\u001b[0m         \u001b[0mgrad_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW_flip\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrad_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m         \u001b[0;31m### END YOUR SOLUTION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gehao/lxy/dls/homework4/python/needle/ops.py\u001b[0m in \u001b[0;36mconv\u001b[0;34m(a, b, stride, padding)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/gehao/lxy/dls/homework4/python/needle/autograd.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_from_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gehao/lxy/dls/homework4/python/needle/autograd.py\u001b[0m in \u001b[0;36mmake_from_op\u001b[0;34m(op, inputs)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealize_cached_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gehao/lxy/dls/homework4/python/needle/autograd.py\u001b[0m in \u001b[0;36mrealize_cached_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m# note: data implicitly calls realized cached data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         self.cached_data = self.op.compute(\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealize_cached_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         )\n\u001b[1;32m    103\u001b[0m         \u001b[0;31m# for i in range(len(self.inputs)):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gehao/lxy/dls/homework4/python/needle/ops.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, A, B)\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0m_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNDArray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnH\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0m_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0m_A\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0m_B\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m         \u001b[0;31m### END YOUR SOLUTION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gehao/lxy/dls/homework4/python/needle/backend_ndarray/ndarray.py\u001b[0m in \u001b[0;36m__matmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    558\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNDArray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             self.device.matmul(\n\u001b[0;32m--> 560\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             )\n\u001b[1;32m    562\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./python')\n",
    "sys.path.append('./apps')\n",
    "import needle as ndl\n",
    "from models import ResNet9\n",
    "from simple_training import train_cifar10, evaluate_cifar10\n",
    "\n",
    "device = ndl.cpu()\n",
    "dataset = ndl.data.CIFAR10Dataset(\"data/cifar-10-batches-py\", train=True)\n",
    "dataloader = ndl.data.DataLoader(\\\n",
    "         dataset=dataset,\n",
    "         batch_size=128,\n",
    "         shuffle=True,\n",
    "            )\n",
    "      #    collate_fn=ndl.data.collate_ndarray,\n",
    "      #    device=device,\n",
    "      #    dtype=\"float32\")\n",
    "model = ResNet9(device=device, dtype=\"float32\")\n",
    "train_cifar10(model, dataloader, n_epochs=10, optimizer=ndl.optim.Adam,\n",
    "      lr=0.001, weight_decay=0.001)\n",
    "evaluate_cifar10(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Recurrent neural network [10 points]\n",
    "\n",
    "**Note:** In the following sections, you may find yourself wanting to index into tensors, i.e., to use getitem or setitem. However, we have not implemented these for tensors in our library; instead, you should use `stack` and `split` operations.\n",
    "\n",
    "In `python/needle/nn.py`, implement `RNNCell`.\n",
    "\n",
    "$h^\\prime = \\text{tanh}(xW_{ih} + b_{ih} + hW_{hh} + b_{hh})$. If nonlinearity is 'relu', then ReLU is used in place of tanh.\n",
    "\n",
    "All weights and biases should be initialized from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ where $k=\\frac{1}{\\text{hidden_size}}$.\n",
    "\n",
    "In `python/needle/nn.py`, implement `RNN`.\n",
    "\n",
    "For each element in the input sequence, each layer computes the following function:\n",
    "\n",
    "$h_t = \\text{tanh}(x_tW_{ih} + b_{ih} + h_{(t-1)}W_{hh} + b_{hh})$\n",
    "\n",
    "where $h_t$ is the hidden state at time $t$, $x_t$ is the input at time $t$, and $h_{(t-1)}$ is the hidden state of the previous layer at time $t-1$ or the initial hidden state at time $0$. If nonlinearity is 'relu', then ReLU is used in place of tanh.\n",
    "\n",
    "In a multi-layer RNN, the input $x_t^{(l)}$ of the $l$-th layer ($l \\ge 2$) is the hidden state $h_t^{(l-1)}$ of the previous layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.0, pytest-7.2.0, pluggy-1.0.0 -- /home/gehao/anaconda3/envs/lxy/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/gehao/lxy/dls/homework4\n",
      "plugins: hydra-core-1.0.7, anyio-3.6.2\n",
      "collected 1803 items / 1163 deselected / 640 selected                          \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-True-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-True-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-True-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-True-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-True-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-True-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-True-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-True-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-False-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-False-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-False-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-False-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-False-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-False-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-False-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-True-False-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-True-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-True-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-True-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-True-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-True-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-True-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-True-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-True-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-False-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-False-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-False-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-False-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-False-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-False-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-False-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-tanh-False-False-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-True-True-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-True-True-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-True-True-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-True-True-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-True-True-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-True-True-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-True-True-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-True-True-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-True-False-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-True-False-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-True-False-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-True-False-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-True-False-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-True-False-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-True-False-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-True-False-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-False-True-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-False-True-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-False-True-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-False-True-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-False-True-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-False-True-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-False-True-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-False-True-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-False-False-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-False-False-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-False-False-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-False-False-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-False-False-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-False-False-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-False-False-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cpu-relu-False-False-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-True-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-True-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-True-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-True-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-True-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-True-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-True-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-True-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-False-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-False-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-False-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-False-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-False-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-False-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-False-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-True-False-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-True-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-True-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-True-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-True-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-True-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-True-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-True-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-True-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-False-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-False-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-False-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-False-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-False-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-False-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-False-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-tanh-False-False-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-True-True-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-True-True-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-True-True-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-True-True-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-True-True-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-True-True-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-True-True-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-True-True-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-True-False-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-True-False-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-True-False-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-True-False-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-True-False-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-True-False-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-True-False-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-True-False-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-False-True-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-False-True-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-False-True-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-False-True-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-False-True-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-False-True-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-False-True-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-False-True-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-False-False-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-False-False-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-False-False-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-False-False-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-False-False-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-False-False-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-False-False-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn_cell[cuda-relu-False-False-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-True-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-True-False-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-True-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 39%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 39%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 39%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 39%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 39%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 39%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-tanh-False-False-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-True-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 49%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 49%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 49%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 49%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 49%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 49%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-True-False-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 51%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 51%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 51%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 51%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 51%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 51%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 54%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 54%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 54%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 54%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 54%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 54%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-True-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cpu-relu-False-False-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 63%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 63%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 63%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 63%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 63%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 63%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-True-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 69%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 69%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 69%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 69%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 69%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 69%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-True-False-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 74%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 74%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 74%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 74%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 74%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 74%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-True-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 76%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 76%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 76%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 76%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 76%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 76%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 79%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 79%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 79%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 79%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 79%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 79%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-tanh-False-False-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 83%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 83%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 83%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 83%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 83%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 83%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-True-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 86%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 86%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 86%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 86%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 86%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 86%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 88%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 88%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 88%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 88%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 88%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 88%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 89%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 89%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 89%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 89%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 89%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 89%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-True-False-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 91%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 91%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 91%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 91%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 91%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 91%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 92%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 92%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 92%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 92%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 92%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 92%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 92%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 94%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 94%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 94%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 94%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 94%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 94%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-True-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 95%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 95%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 95%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 95%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 95%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 95%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 95%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 98%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 98%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 98%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 98%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 98%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 98%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 99%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 99%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 99%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 99%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 99%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 99%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_rnn[cuda-relu-False-False-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m==================== \u001b[32m\u001b[1m640 passed\u001b[0m, \u001b[33m1163 deselected\u001b[0m\u001b[32m in 15.83s\u001b[0m\u001b[32m =====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"test_rnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.0, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /home/gehao/lxy/dls/homework4\n",
      "plugins: hydra-core-1.0.7, anyio-3.6.2\n",
      "collected 10 items / 9 deselected / 1 selected                                 \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_sequence_models.py \n",
      "Submitting rnn...\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "Grader test 3 passed\n",
      "Grader test 4 passed\n",
      "Grader test 5 passed\n",
      "Grader test 6 passed\n",
      "Grader test 7 passed\n",
      "Grader test 8 passed\n",
      "Grader test 9 passed\n",
      "Grader test 10 passed\n",
      "Grader test 11 passed\n",
      "Grader test 12 passed\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m9 deselected\u001b[0m\u001b[32m in 22.52s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"rnn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Long short-term memory network [10 points]\n",
    "Implement - `Sigmoid`\n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1 + \\text{exp}(-x)}$\n",
    "\n",
    "In `python/needle/nn.py`, implement `Sigmoid`, `LSTMCell` and `LSTM`.\n",
    "\n",
    "\\begin{align}\n",
    "i &= \\sigma(xW_{ii} + b_{ii} + hW_{hi} + b_{hi}) \\\\\n",
    "f &= \\sigma(xW_{if} + b_{if} + hW_{hf} + b_{hf}) \\\\\n",
    "g &= \\text{tanh}(xW_{ig} + b_{ig} + hW_{hg} + b_{hg}) \\\\\n",
    "o &= \\sigma(xW_{io} + b_{io} + hW_{ho} + b_{ho}) \\\\\n",
    "c^\\prime &= f * c + i * g \\\\\n",
    "h^\\prime &= o * \\text{tanh}(c^\\prime)\n",
    "\\end{align}\n",
    "\n",
    "where $\\sigma$ is the sigmoid function, and $i$, $f$, $g$, $o$ are the input, forget, cell, and output gates, respectively. \n",
    "\n",
    "All weights and biases should be initialized from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ where $k=\\frac{1}{\\text{hidden_size}}$.\n",
    "\n",
    "Now implement `LSTM` in `python/needle/nn.py`, which applies a multi-layer LSTM RNN to an input sequence. For each element in the input sequence, each layer computes the following function:\n",
    "\n",
    "\\begin{align}\n",
    "i_t &= \\sigma(x_tW_{ii} + b_{ii} + h_{(t-1)}W_{hi} + b_{hi}) \\\\\n",
    "f_t &= \\sigma(x_tW_{if} + b_{if} + h_{(t-1)}W_{hf} + b_{hf}) \\\\\n",
    "g_t &= \\text{tanh}(x_tW_{ig} + b_{ig} + h_{(t-1)}W_{hg} + b_{hg}) \\\\\n",
    "o_t &= \\sigma(x_tW_{io} + b_{io} + h_{(t-1)}W_{ho} + b_{ho}) \\\\\n",
    "c_t &= f * c_{(t-1)} + i * g \\\\\n",
    "h_t &= o * \\text{tanh}(c_t)\n",
    "\\end{align},\n",
    "where $h_t$ is the hidden state at time $t$, $c_t$ is the cell state at time $t$, $x_t$ is the input at time $t$, $h_{(t-1)}$ is the hidden state of the layer at time $t-1$ or the initial hidden state at time $0$, and $i_t$, $f_t$, $g_t$, $o_t$ are the input, forget, cell, and output gates at time $t$ respectively. \n",
    "\n",
    "In a multi-layer LSTM, the input $x_t^{(l)}$ of the $l$-th layer ($l \\ge 2$) is the hidden state $h_t^{(l-1)}$ of the previous layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.0, pytest-7.2.0, pluggy-1.0.0 -- /home/gehao/anaconda3/envs/lxy/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/gehao/lxy/dls/homework4\n",
      "plugins: hydra-core-1.0.7, anyio-3.6.2\n",
      "collected 1803 items / 1483 deselected / 320 selected                          \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-True-True-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-True-True-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-True-True-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-True-True-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-True-True-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-True-True-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-True-True-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-True-True-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-True-False-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-True-False-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-True-False-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-True-False-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-True-False-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-True-False-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-True-False-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-True-False-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-False-True-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-False-True-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-False-True-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-False-True-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-False-True-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-False-True-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-False-True-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-False-True-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-False-False-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-False-False-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-False-False-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-False-False-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-False-False-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-False-False-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-False-False-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cpu-False-False-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-True-True-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-True-True-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-True-True-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-True-True-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-True-True-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-True-True-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-True-True-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-True-True-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-True-False-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-True-False-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-True-False-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-True-False-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-True-False-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-True-False-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-True-False-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-True-False-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-False-True-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-False-True-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-False-True-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-False-True-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-False-True-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-False-True-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-False-True-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-False-True-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-False-False-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-False-False-1-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-False-False-1-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-False-False-1-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-False-False-12-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-False-False-12-1-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-False-False-12-11-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm_cell[cuda-False-False-12-11-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-True-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 39%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 39%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 39%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-True-False-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 49%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 49%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 49%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-True-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 51%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 51%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 51%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 54%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 54%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 54%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cpu-False-False-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 63%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 63%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 63%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 69%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 69%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 69%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-True-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 74%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 74%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 74%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 76%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 76%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 76%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 79%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 79%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 79%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-True-False-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 83%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 83%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 83%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 86%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 86%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 86%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 88%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 88%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 88%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 89%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 89%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 89%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-True-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 91%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 91%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 91%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 92%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 92%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-1-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 92%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-1-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-1-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-1-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-1-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 94%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-1-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 94%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-1-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 94%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-1-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 95%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 95%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 95%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 95%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-12-11-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-12-11-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 98%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-12-11-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 98%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-12-11-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 98%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-12-11-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 99%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-12-11-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 99%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-12-11-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 99%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_lstm[cuda-False-False-12-11-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m==================== \u001b[32m\u001b[1m320 passed\u001b[0m, \u001b[33m1483 deselected\u001b[0m\u001b[32m in 23.29s\u001b[0m\u001b[32m =====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"test_lstm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.0, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /home/gehao/lxy/dls/homework4\n",
      "plugins: hydra-core-1.0.7, anyio-3.6.2\n",
      "collected 10 items / 9 deselected / 1 selected                                 \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_sequence_models.py \n",
      "Submitting lstm...\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "Grader test 3 passed\n",
      "Grader test 4 passed\n",
      "Grader test 5 passed\n",
      "Grader test 6 passed\n",
      "Grader test 7 passed\n",
      "Grader test 8 passed\n",
      "Grader test 9 passed\n",
      "Grader test 10 passed\n",
      "Grader test 11 passed\n",
      "Grader test 12 passed\n",
      "Grader test 13 passed\n",
      "Grader test 14 passed\n",
      "Grader test 15 passed\n",
      "Grader test 16 passed\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m9 deselected\u001b[0m\u001b[32m in 30.45s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"lstm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Penn Treebank dataset [10 points]\n",
    "\n",
    "In word-level language modeling tasks, the model predicts the probability of the next word in the sequence, based on the words already observed in the sequence. You will write support for the Penn Treebank dataset, which consists of stories from the Wall Street Journal, to train and evaluate a language model on word-level prediction.\n",
    "\n",
    "In `python/needle/data.py`, start by implementing the `Dictionary` class, which creates a dictionary from a list of words, mapping each word to a unique integer.\n",
    "\n",
    "Next, we will use this `Dictionary` class to create a corpus from the train and test txt files in the Penn Treebank dataset that you downloaded at the beginning of the notebook. Implement the `tokenize` function in the `Corpus` class to do this.\n",
    "\n",
    "In order to prepare the data for training and evaluation, you will next implement the `batchify` function. Starting from sequential data, batchify arranges the dataset into columns. For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
    "\n",
    "```\n",
    "┌ a g m s ┐\n",
    "│ b h n t │\n",
    "│ c i o u │\n",
    "│ d j p v │\n",
    "│ e k q w │\n",
    "└ f l r x ┘\n",
    "```\n",
    "\n",
    "These columns are treated as independent by the model, which means that the dependence of e. g. 'g' on 'f' cannot be learned, but allows more efficient batch processing.\n",
    "\n",
    "Next, implement the `get_batch` function. `get_batch` subdivides the source data into chunks of length `bptt`. If source is equal to the example output of the batchify function, with a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
    "```\n",
    "┌ a g m s ┐ ┌ b h n t ┐\n",
    "└ b h n t ┘ └ c i o u ┘\n",
    "```\n",
    "Note that despite the name of the function, the subdivison of data is not done along the batch dimension (i.e. dimension 1), since that was handled by the batchify function. The chunks are along dimension 0, corresponding to the seq_len dimension in the LSTM or RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.0, pytest-7.2.0, pluggy-1.0.0 -- /home/gehao/anaconda3/envs/lxy/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/gehao/lxy/dls/homework4\n",
      "plugins: hydra-core-1.0.7, anyio-3.6.2\n",
      "collected 1803 items / 1787 deselected / 16 selected                           \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_cifar_ptb_data.py::test_ptb_dataset[cpu-True-3-1] \u001b[32mPASSED\u001b[0m\u001b[32m      [  6%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_ptb_dataset[cpu-True-3-15] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 12%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_ptb_dataset[cpu-True-32-1] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 18%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_ptb_dataset[cpu-True-32-15] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 25%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_ptb_dataset[cpu-False-3-1] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 31%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_ptb_dataset[cpu-False-3-15] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 37%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_ptb_dataset[cpu-False-32-1] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 43%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_ptb_dataset[cpu-False-32-15] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 50%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_ptb_dataset[cuda-True-3-1] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 56%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_ptb_dataset[cuda-True-3-15] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 62%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_ptb_dataset[cuda-True-32-1] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 68%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_ptb_dataset[cuda-True-32-15] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 75%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_ptb_dataset[cuda-False-3-1] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 81%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_ptb_dataset[cuda-False-3-15] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 87%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_ptb_dataset[cuda-False-32-1] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 93%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_ptb_dataset[cuda-False-32-15] \u001b[32mPASSED\u001b[0m\u001b[32m  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m===================== \u001b[32m\u001b[1m16 passed\u001b[0m, \u001b[33m1787 deselected\u001b[0m\u001b[32m in 10.79s\u001b[0m\u001b[32m =====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"ptb_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.0, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /home/gehao/lxy/dls/homework4\n",
      "plugins: hydra-core-1.0.7, anyio-3.6.2\n",
      "collected 10 items / 8 deselected / 2 selected                                 \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_cifar_ptb_data.py \n",
      "Submitting cifar10...\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "Grader test 3 passed\n",
      "Grader test 4 passed\n",
      "Grader test 5 passed\n",
      "Grader test 6 passed\n",
      "Grader test 7 passed\n",
      "Grader test 8 passed\n",
      "Grader test 9 passed\n",
      "Grader test 10 passed\n",
      "Grader test 11 passed\n",
      "Grader test 12 passed\n",
      "Grader test 13 passed\n",
      "Grader test 14 passed\n",
      "Grader test 15 passed\n",
      "Grader test 16 passed\n",
      "Grader test 17 passed\n",
      "Grader test 18 passed\n",
      "\u001b[32m.\u001b[0m\n",
      "Submitting ptb...\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "Grader test 3 passed\n",
      "Grader test 4 passed\n",
      "Grader test 5 passed\n",
      "Grader test 6 passed\n",
      "Grader test 7 passed\n",
      "Grader test 8 passed\n",
      "Grader test 9 passed\n",
      "Grader test 10 passed\n",
      "Grader test 11 passed\n",
      "Grader test 12 passed\n",
      "Grader test 13 passed\n",
      "Grader test 14 passed\n",
      "Grader test 15 passed\n",
      "Grader test 16 passed\n",
      "Grader test 17 passed\n",
      "Grader test 18 passed\n",
      "Grader test 19 passed\n",
      "Grader test 20 passed\n",
      "Grader test 21 passed\n",
      "Grader test 22 passed\n",
      "Grader test 23 passed\n",
      "Grader test 24 passed\n",
      "Grader test 25 passed\n",
      "Grader test 26 passed\n",
      "Grader test 27 passed\n",
      "Grader test 28 passed\n",
      "Grader test 29 passed\n",
      "Grader test 30 passed\n",
      "Grader test 31 passed\n",
      "Grader test 32 passed\n",
      "Grader test 33 passed\n",
      "Grader test 34 passed\n",
      "Grader test 35 passed\n",
      "Grader test 36 passed\n",
      "Grader test 37 passed\n",
      "Grader test 38 passed\n",
      "Grader test 39 passed\n",
      "Grader test 40 passed\n",
      "Grader test 41 passed\n",
      "Grader test 42 passed\n",
      "Grader test 43 passed\n",
      "Grader test 44 passed\n",
      "Grader test 45 passed\n",
      "Grader test 46 passed\n",
      "Grader test 47 passed\n",
      "Grader test 48 passed\n",
      "Grader test 49 passed\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m================= \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m8 deselected\u001b[0m\u001b[32m in 118.24s (0:01:58)\u001b[0m\u001b[32m ==================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"ptb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Training a word-level language model [10 points]\n",
    "\n",
    "Finally, you will use the `RNN` and `LSTM` components you have written to construct a language model that we will train on the Penn Treebank dataset.\n",
    "\n",
    "First, in `python/needle/nn.py` implement `Embedding`. Consider we have a dictionary with 1000 words. Then for a word which indexes into this dictionary, we can represent this word as a one-hot vector of size 1000, and then use a linear layer to project this to a vector of some embedding size.\n",
    "\n",
    "In `apps/models.py`, you can now implement `LanguageModel`. Your language model should consist of \n",
    "\n",
    "- An embedding layer (which maps word IDs to embeddings) \n",
    "- A sequence model (either RNN or LSTM)\n",
    "- A linear layer (which outputs probabilities of the next word)\n",
    "\n",
    "In `apps/simple_training.py` implement `epoch_general_ptb`, `train_ptb`, and `evaluate_ptb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.0, pytest-7.2.0, pluggy-1.0.0 -- /home/gehao/anaconda3/envs/lxy/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/gehao/lxy/dls/homework4\n",
      "plugins: hydra-core-1.0.7, anyio-3.6.2\n",
      "collected 1803 items / 1291 deselected / 512 selected                          \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-1-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-True-12-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-1-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1-False-12-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-1-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-True-12-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-1-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-rnn-1000-False-12-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-1-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-True-12-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-1-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1-False-12-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 39%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 39%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 39%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 39%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 39%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-1-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-True-12-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-1-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 49%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 49%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 49%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 49%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 49%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cpu-lstm-1000-False-12-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 51%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 51%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 51%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 51%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 51%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-1-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 54%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 54%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 54%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 54%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 54%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-True-12-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-1-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1-False-12-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 63%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 63%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 63%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 63%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 63%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-1-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-True-12-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 69%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 69%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 69%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 69%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 69%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-34-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-34-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-34-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-34-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-34-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-1-34-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-1-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-1-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-1-1-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-1-1-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-1-15-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-1-15-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-1-15-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-1-15-2-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-34-1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-34-1-1-13] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-34-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 74%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-34-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 74%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-34-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 74%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-34-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 74%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-34-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 74%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-rnn-1000-False-12-34-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 75%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-1-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 75%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-1-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 75%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-1-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 75%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-1-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 75%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-1-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 75%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-1-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 76%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-1-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 76%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-1-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 76%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-34-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 76%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-34-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 76%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-34-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 77%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-34-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 77%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-34-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 77%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-34-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 77%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-34-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 77%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-1-34-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 78%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-1-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 78%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-1-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 78%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-1-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 78%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-1-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 78%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-1-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 79%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-1-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 79%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-1-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 79%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-1-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 79%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-34-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 79%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-34-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 80%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-34-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 80%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-34-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 80%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-34-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 80%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-34-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 80%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-34-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 81%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-True-12-34-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 81%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-1-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 81%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-1-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 81%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-1-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 81%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-1-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 82%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-1-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 82%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-1-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 82%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-1-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 82%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-1-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 82%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-34-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 83%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-34-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 83%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-34-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 83%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-34-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 83%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-34-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 83%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-34-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 83%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-34-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 84%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-1-34-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 84%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-1-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 84%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-1-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 84%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-1-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 84%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-1-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 85%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-1-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 85%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-1-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 85%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-1-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 85%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-1-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 85%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-34-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 86%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-34-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 86%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-34-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 86%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-34-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 86%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-34-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 86%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-34-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 87%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-34-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 87%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1-False-12-34-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 87%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-1-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 87%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-1-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 87%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-1-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 88%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-1-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 88%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-1-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 88%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-1-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 88%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-1-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 88%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-1-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 89%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-34-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 89%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-34-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 89%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-34-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 89%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-34-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 89%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-34-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 90%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-34-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 90%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-34-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 90%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-1-34-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 90%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-1-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 90%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-1-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 91%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-1-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 91%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-1-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 91%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-1-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 91%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-1-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 91%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-1-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 91%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-1-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 92%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-34-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 92%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-34-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 92%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-34-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 92%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-34-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 92%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-34-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 93%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-34-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 93%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-34-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 93%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-True-12-34-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 93%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-1-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 93%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-1-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 94%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-1-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 94%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-1-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 94%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-1-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 94%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-1-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 94%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-1-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 95%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-1-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 95%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-34-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 95%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-34-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 95%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-34-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 95%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-34-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 96%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-34-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 96%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-34-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 96%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-34-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 96%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-1-34-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 96%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-1-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 97%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-1-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 97%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-1-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 97%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-1-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 97%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-1-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 97%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-1-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 98%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-1-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 98%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-1-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 98%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-34-1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 98%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-34-1-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 98%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-34-1-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 99%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-34-1-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 99%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-34-15-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 99%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-34-15-1-13] \u001b[31mFAILED\u001b[0m\u001b[31m [ 99%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-34-15-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 99%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_implementation[cuda-lstm-1000-False-12-34-15-2-13] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-False-12-34-1-2-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      "        h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "        c0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "    \n",
      "        model = LanguageModel(embedding_size, output_size, hidden_size, num_layers, seq_model, device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m init_hidden:\n",
      "            \u001b[94mif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mlstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "                h = (h0, c0)\n",
      "            \u001b[94melif\u001b[39;49;00m seq_model == \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "                h = h0\n",
      "            output, h_ = model(ndl.Tensor(x, device=device), h)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      ">           output, h_ = model(ndl.Tensor(x, device=device), \u001b[94mNone\u001b[39;49;00m)\n",
      "\n",
      "batch_size = 1\n",
      "c0         = <[RuntimeError('an illegal memory access was encountered') raised in repr()] Tensor object at 0x7fed8ae4de80>\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "h0         = <[RuntimeError('an illegal memory access was encountered') raised in repr()] Tensor object at 0x7fed8ae4db70>\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "model      = <models.LanguageModel object at 0x7fed8ae4db00>\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[766.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:209: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:76: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\n",
      "        args       = (<[RuntimeError('an illegal memory access was encountered') raised in repr()] Tensor object at 0x7fed8b20aba8>, None)\n",
      "        kwargs     = {}\n",
      "        self       = <models.LanguageModel object at 0x7fed8ae4db00>\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:81: in forward\n",
      "    x = \u001b[96mself\u001b[39;49;00m.linear(x)\n",
      "        batch      = 1\n",
      "        h          = <[RuntimeError('an illegal memory access was encountered') raised in repr()] Tensor object at 0x7fed8d578cc0>\n",
      "        self       = <models.LanguageModel object at 0x7fed8ae4db00>\n",
      "        seq_len    = 1\n",
      "        x          = <[RuntimeError('an illegal memory access was encountered') raised in repr()] Tensor object at 0x7fed8b20af60>\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:76: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\n",
      "        args       = (<[RuntimeError('an illegal memory access was encountered') raised in repr()] Tensor object at 0x7fed8b20af60>,)\n",
      "        kwargs     = {}\n",
      "        self       = <needle.nn.Linear object at 0x7fed8d597dd8>\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:99: in forward\n",
      "    output = ops.matmul(X , \u001b[96mself\u001b[39;49;00m.weight)\n",
      "        X          = <[RuntimeError('an illegal memory access was encountered') raised in repr()] Tensor object at 0x7fed8b20af60>\n",
      "        self       = <needle.nn.Linear object at 0x7fed8d597dd8>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:317: in matmul\n",
      "    \u001b[94mreturn\u001b[39;49;00m MatMul()(a, b)\n",
      "        a          = <[RuntimeError('an illegal memory access was encountered') raised in repr()] Tensor object at 0x7fed8b20af60>\n",
      "        b          = <[RuntimeError('an illegal memory access was encountered') raised in repr()] Parameter object at 0x7fed8b20a940>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (<[RuntimeError('an illegal memory access was encountered') raised in repr()] Tensor object at 0x7fed8b20af60>, <[RuntimeError('an illegal memory access was encountered') raised in repr()] Parameter object at 0x7fed8b20a940>)\n",
      "        self       = <needle.ops.MatMul object at 0x7fed8b20a240>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (<[RuntimeError('an illegal memory access was encountered') raised in repr()] Tensor object at 0x7fed8b20af60>, <[RuntimeError('an illegal memory access was encountered') raised in repr()] Parameter object at 0x7fed8b20a940>)\n",
      "        op         = <needle.ops.MatMul object at 0x7fed8b20a240>\n",
      "        tensor     = <[RuntimeError('an illegal memory access was encountered') raised in repr()] Tensor object at 0x7fed8b179588>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:101: in realize_cached_data\n",
      "    *[x.realize_cached_data() \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.inputs]\n",
      "        self       = <[RuntimeError('an illegal memory access was encountered') raised in repr()] Tensor object at 0x7fed8b179588>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:296: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m a @ b\n",
      "        a          = <[RuntimeError('an illegal memory access was encountered') raised in repr()] NDArray object at 0x7fed8b20a1d0>\n",
      "        b          = <[RuntimeError('an illegal memory access was encountered') raised in repr()] NDArray object at 0x7fed8b20ae80>\n",
      "        self       = <needle.ops.MatMul object at 0x7fed8b20a240>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:558: in __matmul__\n",
      "    out = NDArray.make((m, p), device=\u001b[96mself\u001b[39;49;00m.device)\n",
      "        m          = 1\n",
      "        n          = 12\n",
      "        other      = <[RuntimeError('an illegal memory access was encountered') raised in repr()] NDArray object at 0x7fed8b20ae80>\n",
      "        p          = 1000\n",
      "        self       = <[RuntimeError('an illegal memory access was encountered') raised in repr()] NDArray object at 0x7fed8b20a1d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1000), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8b1795f8>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1000)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-False-12-34-1-2-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[778.],\n",
      "       [478.],\n",
      "       [627.],\n",
      "       [152.],\n",
      "       [374.],\n",
      "       [706.],\n",
      "       [545.],\n",
      "       [506.],\n",
      "       [364.],\n",
      "       [922.],\n",
      "       [400.],\n",
      "       [531.],\n",
      "       [128.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 1.1519611 ,  1.1013932 ,  0.06374785,  0.66055804,\n",
      "          0.88164186, -0.43243575, -0.6717065 ,  0.748925...9472574,  1.0968043 , -0.04165439,\n",
      "          0.23671655, -0.9600372 , -0.3447367 , -0.83284676]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8ae3ca58>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 1.1519611 ,  1.1013932 ,  0.06374785,  0.66055804,\n",
      "          0.88164186, -0.43243575, -0.6717065 ,  0.748925...9472574,  1.0968043 , -0.04165439,\n",
      "          0.23671655, -0.9600372 , -0.3447367 , -0.83284676]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 1.1519611 ,  1.1013932 ,  0.06374785,  0.66055804,\n",
      "          0.88164186, -0.43243575, -0.6717065 ,  0.748925...9472574,  1.0968043 , -0.04165439,\n",
      "          0.23671655, -0.9600372 , -0.3447367 , -0.83284676]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 1.1519611 ,  1.1013932 ,  0.06374785,  0.66055804,\n",
      "          0.88164186, -0.43243575, -0.6717065 ,  0.748925...9472574,  1.0968043 , -0.04165439,\n",
      "          0.23671655, -0.9600372 , -0.3447367 , -0.83284676]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8ae3cf98>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8ae3ccc0>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-False-12-34-15-1-1] _____\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[828., 738., 980., 187., 813., 376., 920., 321., 786., 623., 138.,\n",
      "        286., 136., 254., 396.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-2.46668673e+00, -9.53280926e-01, -1.74271226e-01,\n",
      "         -1.05522668e+00,  7.17027187e-01, -1.15390050e+00...,  7.74815023e-01, -1.86244273e+00,\n",
      "         -5.09404600e-01, -2.87708163e-01,  7.08759949e-02]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8ad8e048>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-2.46668673e+00, -9.53280926e-01, -1.74271226e-01,\n",
      "         -1.05522668e+00,  7.17027187e-01, -1.15390050e+00...,  7.74815023e-01, -1.86244273e+00,\n",
      "         -5.09404600e-01, -2.87708163e-01,  7.08759949e-02]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-2.46668673e+00, -9.53280926e-01, -1.74271226e-01,\n",
      "         -1.05522668e+00,  7.17027187e-01, -1.15390050e+00...,  7.74815023e-01, -1.86244273e+00,\n",
      "         -5.09404600e-01, -2.87708163e-01,  7.08759949e-02]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-2.46668673e+00, -9.53280926e-01, -1.74271226e-01,\n",
      "         -1.05522668e+00,  7.17027187e-01, -1.15390050e+00...,  7.74815023e-01, -1.86244273e+00,\n",
      "         -5.09404600e-01, -2.87708163e-01,  7.08759949e-02]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8ad8e080>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8ad8e2e8>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m____ test_language_model_implementation[cuda-rnn-1000-False-12-34-15-1-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[624., 933., 924., 733., 186., 402., 503., 180., 732., 189., 392.,\n",
      "        442., 331., 667., 605.],\n",
      "       [416...    [297., 808., 420., 444., 340., 166., 240., 745., 282., 120., 476.,\n",
      "        483., 391., 767., 901.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 2.14995313e+00,  7.69914865e-01, -1.14285834e-01,\n",
      "          4.17140245e-01,  6.00521863e-01, -1.77637708e+00...,  1.89579415e+00,  2.16680810e-01,\n",
      "         -1.17883301e+00,  1.78582895e+00,  1.37048173e+00]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d304a90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 2.14995313e+00,  7.69914865e-01, -1.14285834e-01,\n",
      "          4.17140245e-01,  6.00521863e-01, -1.77637708e+00...,  1.89579415e+00,  2.16680810e-01,\n",
      "         -1.17883301e+00,  1.78582895e+00,  1.37048173e+00]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 2.14995313e+00,  7.69914865e-01, -1.14285834e-01,\n",
      "          4.17140245e-01,  6.00521863e-01, -1.77637708e+00...,  1.89579415e+00,  2.16680810e-01,\n",
      "         -1.17883301e+00,  1.78582895e+00,  1.37048173e+00]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 2.14995313e+00,  7.69914865e-01, -1.14285834e-01,\n",
      "          4.17140245e-01,  6.00521863e-01, -1.77637708e+00...,  1.89579415e+00,  2.16680810e-01,\n",
      "         -1.17883301e+00,  1.78582895e+00,  1.37048173e+00]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d304f28>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d304fd0>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-rnn-1000-False-12-34-15-2-1] _____\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[617., 486., 503., 861., 930., 372., 318., 154., 356., 192., 908.,\n",
      "        296., 403., 927., 329.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 0.9339181 , -0.7224088 ,  0.10898851,  0.12341204,\n",
      "         -1.4132969 ,  0.62568825, -0.35488498,  0.155960...2437958, -0.27835155,  1.5577447 ,\n",
      "         -1.9810407 ,  1.3159426 ,  0.86794335,  0.25001824]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d701eb8>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 0.9339181 , -0.7224088 ,  0.10898851,  0.12341204,\n",
      "         -1.4132969 ,  0.62568825, -0.35488498,  0.155960...2437958, -0.27835155,  1.5577447 ,\n",
      "         -1.9810407 ,  1.3159426 ,  0.86794335,  0.25001824]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 0.9339181 , -0.7224088 ,  0.10898851,  0.12341204,\n",
      "         -1.4132969 ,  0.62568825, -0.35488498,  0.155960...2437958, -0.27835155,  1.5577447 ,\n",
      "         -1.9810407 ,  1.3159426 ,  0.86794335,  0.25001824]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 0.9339181 , -0.7224088 ,  0.10898851,  0.12341204,\n",
      "         -1.4132969 ,  0.62568825, -0.35488498,  0.155960...2437958, -0.27835155,  1.5577447 ,\n",
      "         -1.9810407 ,  1.3159426 ,  0.86794335,  0.25001824]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d7013c8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d701898>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m____ test_language_model_implementation[cuda-rnn-1000-False-12-34-15-2-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'rnn'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'rnn'\n",
      "x          = array([[464., 426., 823., 471., 583., 221., 455., 144.,  38., 824., 243.,\n",
      "        163., 991., 922., 718.],\n",
      "       [515...    [319., 776., 292., 961., 986., 451., 684., 942., 916., 502., 635.,\n",
      "        916., 935., 612., 237.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 1.68128800e+00, -2.74246264e+00, -4.51802194e-01,\n",
      "          7.41591871e-01, -9.27609131e-02,  8.97017419e-02...,  1.59918451e+00, -4.81928922e-02,\n",
      "          6.62968874e-01,  5.21427035e-01,  5.43304980e-01]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8b17af60>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 1.68128800e+00, -2.74246264e+00, -4.51802194e-01,\n",
      "          7.41591871e-01, -9.27609131e-02,  8.97017419e-02...,  1.59918451e+00, -4.81928922e-02,\n",
      "          6.62968874e-01,  5.21427035e-01,  5.43304980e-01]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 1.68128800e+00, -2.74246264e+00, -4.51802194e-01,\n",
      "          7.41591871e-01, -9.27609131e-02,  8.97017419e-02...,  1.59918451e+00, -4.81928922e-02,\n",
      "          6.62968874e-01,  5.21427035e-01,  5.43304980e-01]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 1.68128800e+00, -2.74246264e+00, -4.51802194e-01,\n",
      "          7.41591871e-01, -9.27609131e-02,  8.97017419e-02...,  1.59918451e+00, -4.81928922e-02,\n",
      "          6.62968874e-01,  5.21427035e-01,  5.43304980e-01]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8b17a470>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8b17a630>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m________ test_language_model_implementation[cuda-lstm-1-True-1-1-1-1-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[0.4921866]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d3aee10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[0.4921866]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[0.4921866]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[0.4921866]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d3ae588>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d3ae0f0>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-True-1-1-1-1-13] ________\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[0.8430874]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d3d2b38>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[0.8430874]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[0.8430874]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[0.8430874]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d3d2080>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d3d2cc0>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m________ test_language_model_implementation[cuda-lstm-1-True-1-1-1-2-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[0.16323788]],\n",
      "\n",
      "       [[0.06946375]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d31fc50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[0.16323788]],\n",
      "\n",
      "       [[0.06946375]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[0.16323788]],\n",
      "\n",
      "       [[0.06946375]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[0.16323788]],\n",
      "\n",
      "       [[0.06946375]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d31fcc0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d31f668>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-True-1-1-1-2-13] ________\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-1.6373868]],\n",
      "\n",
      "       [[-1.1573024]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d304fd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-1.6373868]],\n",
      "\n",
      "       [[-1.1573024]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-1.6373868]],\n",
      "\n",
      "       [[-1.1573024]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-1.6373868]],\n",
      "\n",
      "       [[-1.1573024]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d304da0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d304a90>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-True-1-1-15-1-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.1236819 ],\n",
      "        [-0.47882158],\n",
      "        [ 0.6612186 ],\n",
      "        [-0.14208105],\n",
      "        [ 2.2089136 ],\n",
      "   ...9907799 ],\n",
      "        [ 1.4701668 ],\n",
      "        [-0.49866   ],\n",
      "        [-1.4480437 ],\n",
      "        [-0.15532835]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d508208>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.1236819 ],\n",
      "        [-0.47882158],\n",
      "        [ 0.6612186 ],\n",
      "        [-0.14208105],\n",
      "        [ 2.2089136 ],\n",
      "   ...9907799 ],\n",
      "        [ 1.4701668 ],\n",
      "        [-0.49866   ],\n",
      "        [-1.4480437 ],\n",
      "        [-0.15532835]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.1236819 ],\n",
      "        [-0.47882158],\n",
      "        [ 0.6612186 ],\n",
      "        [-0.14208105],\n",
      "        [ 2.2089136 ],\n",
      "   ...9907799 ],\n",
      "        [ 1.4701668 ],\n",
      "        [-0.49866   ],\n",
      "        [-1.4480437 ],\n",
      "        [-0.15532835]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.1236819 ],\n",
      "        [-0.47882158],\n",
      "        [ 0.6612186 ],\n",
      "        [-0.14208105],\n",
      "        [ 2.2089136 ],\n",
      "   ...9907799 ],\n",
      "        [ 1.4701668 ],\n",
      "        [-0.49866   ],\n",
      "        [-1.4480437 ],\n",
      "        [-0.15532835]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d508a90>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d508d68>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-True-1-1-15-1-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.34423625],\n",
      "        [ 0.18511756],\n",
      "        [-0.34234223],\n",
      "        [ 0.00596126],\n",
      "        [-0.7125403 ],\n",
      "   ...0412518 ],\n",
      "        [-1.020593  ],\n",
      "        [-0.3295658 ],\n",
      "        [ 1.6142339 ],\n",
      "        [-0.47776064]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8b043710>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.34423625],\n",
      "        [ 0.18511756],\n",
      "        [-0.34234223],\n",
      "        [ 0.00596126],\n",
      "        [-0.7125403 ],\n",
      "   ...0412518 ],\n",
      "        [-1.020593  ],\n",
      "        [-0.3295658 ],\n",
      "        [ 1.6142339 ],\n",
      "        [-0.47776064]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.34423625],\n",
      "        [ 0.18511756],\n",
      "        [-0.34234223],\n",
      "        [ 0.00596126],\n",
      "        [-0.7125403 ],\n",
      "   ...0412518 ],\n",
      "        [-1.020593  ],\n",
      "        [-0.3295658 ],\n",
      "        [ 1.6142339 ],\n",
      "        [-0.47776064]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.34423625],\n",
      "        [ 0.18511756],\n",
      "        [-0.34234223],\n",
      "        [ 0.00596126],\n",
      "        [-0.7125403 ],\n",
      "   ...0412518 ],\n",
      "        [-1.020593  ],\n",
      "        [-0.3295658 ],\n",
      "        [ 1.6142339 ],\n",
      "        [-0.47776064]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8b043390>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8b043860>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-True-1-1-15-2-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 0.10955769],\n",
      "        [-0.04304204],\n",
      "        [ 0.8402648 ],\n",
      "        [-1.1021011 ],\n",
      "        [-0.15477255],\n",
      "   ...06175437],\n",
      "        [ 0.10102261],\n",
      "        [ 0.5009843 ],\n",
      "        [-1.0054926 ],\n",
      "        [-3.0337389 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d382da0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 0.10955769],\n",
      "        [-0.04304204],\n",
      "        [ 0.8402648 ],\n",
      "        [-1.1021011 ],\n",
      "        [-0.15477255],\n",
      "   ...06175437],\n",
      "        [ 0.10102261],\n",
      "        [ 0.5009843 ],\n",
      "        [-1.0054926 ],\n",
      "        [-3.0337389 ]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 0.10955769],\n",
      "        [-0.04304204],\n",
      "        [ 0.8402648 ],\n",
      "        [-1.1021011 ],\n",
      "        [-0.15477255],\n",
      "   ...06175437],\n",
      "        [ 0.10102261],\n",
      "        [ 0.5009843 ],\n",
      "        [-1.0054926 ],\n",
      "        [-3.0337389 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 0.10955769],\n",
      "        [-0.04304204],\n",
      "        [ 0.8402648 ],\n",
      "        [-1.1021011 ],\n",
      "        [-0.15477255],\n",
      "   ...06175437],\n",
      "        [ 0.10102261],\n",
      "        [ 0.5009843 ],\n",
      "        [-1.0054926 ],\n",
      "        [-3.0337389 ]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d3829e8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d3827b8>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-True-1-1-15-2-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 1.2786804 ],\n",
      "        [-0.4374068 ],\n",
      "        [ 0.23184574],\n",
      "        [-1.1115837 ],\n",
      "        [ 0.4227399 ],\n",
      "   ...137452  ],\n",
      "        [-1.0656445 ],\n",
      "        [-0.42411706],\n",
      "        [-0.23707254],\n",
      "        [-0.81541586]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d4b6ef0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 1.2786804 ],\n",
      "        [-0.4374068 ],\n",
      "        [ 0.23184574],\n",
      "        [-1.1115837 ],\n",
      "        [ 0.4227399 ],\n",
      "   ...137452  ],\n",
      "        [-1.0656445 ],\n",
      "        [-0.42411706],\n",
      "        [-0.23707254],\n",
      "        [-0.81541586]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 1.2786804 ],\n",
      "        [-0.4374068 ],\n",
      "        [ 0.23184574],\n",
      "        [-1.1115837 ],\n",
      "        [ 0.4227399 ],\n",
      "   ...137452  ],\n",
      "        [-1.0656445 ],\n",
      "        [-0.42411706],\n",
      "        [-0.23707254],\n",
      "        [-0.81541586]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 1.2786804 ],\n",
      "        [-0.4374068 ],\n",
      "        [ 0.23184574],\n",
      "        [-1.1115837 ],\n",
      "        [ 0.4227399 ],\n",
      "   ...137452  ],\n",
      "        [-1.0656445 ],\n",
      "        [-0.42411706],\n",
      "        [-0.23707254],\n",
      "        [-0.81541586]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d4b6ba8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d4b6198>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-True-1-34-1-1-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.9673809]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d6eba58>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.9673809]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.9673809]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.9673809]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d6ebba8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d6eb160>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-True-1-34-1-1-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[0.80695045]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d514ac8>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[0.80695045]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[0.80695045]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[0.80695045]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d514978>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d5142e8>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-True-1-34-1-2-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[0.80823994]],\n",
      "\n",
      "       [[1.7989076 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d6ebc18>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[0.80823994]],\n",
      "\n",
      "       [[1.7989076 ]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[0.80823994]],\n",
      "\n",
      "       [[1.7989076 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[0.80823994]],\n",
      "\n",
      "       [[1.7989076 ]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d6eb198>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d6ebe80>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-True-1-34-1-2-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 0.8421083 ]],\n",
      "\n",
      "       [[-0.39264178]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8b043da0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 0.8421083 ]],\n",
      "\n",
      "       [[-0.39264178]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 0.8421083 ]],\n",
      "\n",
      "       [[-0.39264178]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 0.8421083 ]],\n",
      "\n",
      "       [[-0.39264178]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8b043668>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8b043b00>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-True-1-34-15-1-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 0.6649719 ],\n",
      "        [ 0.8931729 ],\n",
      "        [-2.0855196 ],\n",
      "        [-2.0635462 ],\n",
      "        [ 0.88037103],\n",
      "   ...4499067 ],\n",
      "        [ 0.26467362],\n",
      "        [-0.25956842],\n",
      "        [-1.2320185 ],\n",
      "        [-0.29510295]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8af5b6d8>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 0.6649719 ],\n",
      "        [ 0.8931729 ],\n",
      "        [-2.0855196 ],\n",
      "        [-2.0635462 ],\n",
      "        [ 0.88037103],\n",
      "   ...4499067 ],\n",
      "        [ 0.26467362],\n",
      "        [-0.25956842],\n",
      "        [-1.2320185 ],\n",
      "        [-0.29510295]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 0.6649719 ],\n",
      "        [ 0.8931729 ],\n",
      "        [-2.0855196 ],\n",
      "        [-2.0635462 ],\n",
      "        [ 0.88037103],\n",
      "   ...4499067 ],\n",
      "        [ 0.26467362],\n",
      "        [-0.25956842],\n",
      "        [-1.2320185 ],\n",
      "        [-0.29510295]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 0.6649719 ],\n",
      "        [ 0.8931729 ],\n",
      "        [-2.0855196 ],\n",
      "        [-2.0635462 ],\n",
      "        [ 0.88037103],\n",
      "   ...4499067 ],\n",
      "        [ 0.26467362],\n",
      "        [-0.25956842],\n",
      "        [-1.2320185 ],\n",
      "        [-0.29510295]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8af5b278>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8af5b198>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-True-1-34-15-1-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 0.08712535],\n",
      "        [ 0.09840908],\n",
      "        [ 1.4579859 ],\n",
      "        [ 1.9671534 ],\n",
      "        [-0.09177416],\n",
      "   ...31910563],\n",
      "        [-0.86133343],\n",
      "        [-1.5724492 ],\n",
      "        [-1.2166636 ],\n",
      "        [-0.54710263]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d4b6f28>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 0.08712535],\n",
      "        [ 0.09840908],\n",
      "        [ 1.4579859 ],\n",
      "        [ 1.9671534 ],\n",
      "        [-0.09177416],\n",
      "   ...31910563],\n",
      "        [-0.86133343],\n",
      "        [-1.5724492 ],\n",
      "        [-1.2166636 ],\n",
      "        [-0.54710263]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 0.08712535],\n",
      "        [ 0.09840908],\n",
      "        [ 1.4579859 ],\n",
      "        [ 1.9671534 ],\n",
      "        [-0.09177416],\n",
      "   ...31910563],\n",
      "        [-0.86133343],\n",
      "        [-1.5724492 ],\n",
      "        [-1.2166636 ],\n",
      "        [-0.54710263]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 0.08712535],\n",
      "        [ 0.09840908],\n",
      "        [ 1.4579859 ],\n",
      "        [ 1.9671534 ],\n",
      "        [-0.09177416],\n",
      "   ...31910563],\n",
      "        [-0.86133343],\n",
      "        [-1.5724492 ],\n",
      "        [-1.2166636 ],\n",
      "        [-0.54710263]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d4b6160>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d4b6278>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-True-1-34-15-2-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.6801613 ],\n",
      "        [ 0.18478999],\n",
      "        [ 0.9050329 ],\n",
      "        [ 0.4161643 ],\n",
      "        [-1.0764536 ],\n",
      "   ...98090917],\n",
      "        [-0.40418026],\n",
      "        [ 1.5725149 ],\n",
      "        [ 1.7388452 ],\n",
      "        [-0.2677518 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8b17a7f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.6801613 ],\n",
      "        [ 0.18478999],\n",
      "        [ 0.9050329 ],\n",
      "        [ 0.4161643 ],\n",
      "        [-1.0764536 ],\n",
      "   ...98090917],\n",
      "        [-0.40418026],\n",
      "        [ 1.5725149 ],\n",
      "        [ 1.7388452 ],\n",
      "        [-0.2677518 ]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.6801613 ],\n",
      "        [ 0.18478999],\n",
      "        [ 0.9050329 ],\n",
      "        [ 0.4161643 ],\n",
      "        [-1.0764536 ],\n",
      "   ...98090917],\n",
      "        [-0.40418026],\n",
      "        [ 1.5725149 ],\n",
      "        [ 1.7388452 ],\n",
      "        [-0.2677518 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.6801613 ],\n",
      "        [ 0.18478999],\n",
      "        [ 0.9050329 ],\n",
      "        [ 0.4161643 ],\n",
      "        [-1.0764536 ],\n",
      "   ...98090917],\n",
      "        [-0.40418026],\n",
      "        [ 1.5725149 ],\n",
      "        [ 1.7388452 ],\n",
      "        [-0.2677518 ]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8b17ac18>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8b17a860>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-True-1-34-15-2-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 1.9316022 ],\n",
      "        [-1.3046473 ],\n",
      "        [ 0.3663776 ],\n",
      "        [ 0.04359724],\n",
      "        [ 0.42298144],\n",
      "   ...41126606],\n",
      "        [-0.05812784],\n",
      "        [-0.725966  ],\n",
      "        [ 2.6583893 ],\n",
      "        [ 1.2699621 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d40c6a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 1.9316022 ],\n",
      "        [-1.3046473 ],\n",
      "        [ 0.3663776 ],\n",
      "        [ 0.04359724],\n",
      "        [ 0.42298144],\n",
      "   ...41126606],\n",
      "        [-0.05812784],\n",
      "        [-0.725966  ],\n",
      "        [ 2.6583893 ],\n",
      "        [ 1.2699621 ]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 1.9316022 ],\n",
      "        [-1.3046473 ],\n",
      "        [ 0.3663776 ],\n",
      "        [ 0.04359724],\n",
      "        [ 0.42298144],\n",
      "   ...41126606],\n",
      "        [-0.05812784],\n",
      "        [-0.725966  ],\n",
      "        [ 2.6583893 ],\n",
      "        [ 1.2699621 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 1.9316022 ],\n",
      "        [-1.3046473 ],\n",
      "        [ 0.3663776 ],\n",
      "        [ 0.04359724],\n",
      "        [ 0.42298144],\n",
      "   ...41126606],\n",
      "        [-0.05812784],\n",
      "        [-0.725966  ],\n",
      "        [ 2.6583893 ],\n",
      "        [ 1.2699621 ]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d40cdd8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d40c2e8>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-True-12-1-1-1-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.34536198, -0.7098419 , -1.0065168 , -0.15804777,\n",
      "         -0.25949013,  0.3546071 ,  0.42582244, -0.1081837 ,\n",
      "         -0.26436535, -0.02509702, -1.9914559 ,  1.826131  ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8b2454e0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.34536198, -0.7098419 , -1.0065168 , -0.15804777,\n",
      "         -0.25949013,  0.3546071 ,  0.42582244, -0.1081837 ,\n",
      "         -0.26436535, -0.02509702, -1.9914559 ,  1.826131  ]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.34536198, -0.7098419 , -1.0065168 , -0.15804777,\n",
      "         -0.25949013,  0.3546071 ,  0.42582244, -0.1081837 ,\n",
      "         -0.26436535, -0.02509702, -1.9914559 ,  1.826131  ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.34536198, -0.7098419 , -1.0065168 , -0.15804777,\n",
      "         -0.25949013,  0.3546071 ,  0.42582244, -0.1081837 ,\n",
      "         -0.26436535, -0.02509702, -1.9914559 ,  1.826131  ]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8b245c50>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8b245198>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-True-12-1-1-1-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.9542361 , -0.35828844, -0.87365365, -0.44289154,\n",
      "          1.0385538 ,  1.7639998 , -1.2051897 , -0.06317854,\n",
      "         -0.6145886 , -0.92178226,  0.83358806,  0.701935  ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8b1caf28>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.9542361 , -0.35828844, -0.87365365, -0.44289154,\n",
      "          1.0385538 ,  1.7639998 , -1.2051897 , -0.06317854,\n",
      "         -0.6145886 , -0.92178226,  0.83358806,  0.701935  ]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.9542361 , -0.35828844, -0.87365365, -0.44289154,\n",
      "          1.0385538 ,  1.7639998 , -1.2051897 , -0.06317854,\n",
      "         -0.6145886 , -0.92178226,  0.83358806,  0.701935  ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.9542361 , -0.35828844, -0.87365365, -0.44289154,\n",
      "          1.0385538 ,  1.7639998 , -1.2051897 , -0.06317854,\n",
      "         -0.6145886 , -0.92178226,  0.83358806,  0.701935  ]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8b1cab38>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8b1ca438>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-True-12-1-1-2-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.36101988, -0.5966917 ,  0.61911756,  1.9135478 ,\n",
      "          1.0081903 ,  1.5233302 , -0.09230531,  0.486587...9942176, -0.7588537 ,  1.1010222 ,\n",
      "         -0.74644   , -1.6223305 , -0.81851757,  1.5277772 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d3cce80>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.36101988, -0.5966917 ,  0.61911756,  1.9135478 ,\n",
      "          1.0081903 ,  1.5233302 , -0.09230531,  0.486587...9942176, -0.7588537 ,  1.1010222 ,\n",
      "         -0.74644   , -1.6223305 , -0.81851757,  1.5277772 ]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.36101988, -0.5966917 ,  0.61911756,  1.9135478 ,\n",
      "          1.0081903 ,  1.5233302 , -0.09230531,  0.486587...9942176, -0.7588537 ,  1.1010222 ,\n",
      "         -0.74644   , -1.6223305 , -0.81851757,  1.5277772 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.36101988, -0.5966917 ,  0.61911756,  1.9135478 ,\n",
      "          1.0081903 ,  1.5233302 , -0.09230531,  0.486587...9942176, -0.7588537 ,  1.1010222 ,\n",
      "         -0.74644   , -1.6223305 , -0.81851757,  1.5277772 ]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d3ccd68>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d3cc940>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-True-12-1-1-2-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-1.124571  ,  1.2541127 ,  0.62044543, -2.0037904 ,\n",
      "          2.567058  , -0.92307705,  1.0967841 ,  1.830683...743701 , -0.20276304,  0.556435  ,\n",
      "          1.024742  , -1.6645317 , -0.6009241 ,  0.6577079 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d31f978>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-1.124571  ,  1.2541127 ,  0.62044543, -2.0037904 ,\n",
      "          2.567058  , -0.92307705,  1.0967841 ,  1.830683...743701 , -0.20276304,  0.556435  ,\n",
      "          1.024742  , -1.6645317 , -0.6009241 ,  0.6577079 ]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-1.124571  ,  1.2541127 ,  0.62044543, -2.0037904 ,\n",
      "          2.567058  , -0.92307705,  1.0967841 ,  1.830683...743701 , -0.20276304,  0.556435  ,\n",
      "          1.024742  , -1.6645317 , -0.6009241 ,  0.6577079 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-1.124571  ,  1.2541127 ,  0.62044543, -2.0037904 ,\n",
      "          2.567058  , -0.92307705,  1.0967841 ,  1.830683...743701 , -0.20276304,  0.556435  ,\n",
      "          1.024742  , -1.6645317 , -0.6009241 ,  0.6577079 ]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d31fa20>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d31f2b0>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-True-12-1-15-1-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 1.6307313 ,  0.9477477 , -1.1243066 ,  0.5068073 ,\n",
      "         -1.8343153 ,  1.2745912 , -1.4787502 , -0.777219...5584987, -1.8253518 , -1.048855  ,\n",
      "          0.36126828, -0.82006705,  0.5223211 ,  0.55366033]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8b17a4e0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 1.6307313 ,  0.9477477 , -1.1243066 ,  0.5068073 ,\n",
      "         -1.8343153 ,  1.2745912 , -1.4787502 , -0.777219...5584987, -1.8253518 , -1.048855  ,\n",
      "          0.36126828, -0.82006705,  0.5223211 ,  0.55366033]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 1.6307313 ,  0.9477477 , -1.1243066 ,  0.5068073 ,\n",
      "         -1.8343153 ,  1.2745912 , -1.4787502 , -0.777219...5584987, -1.8253518 , -1.048855  ,\n",
      "          0.36126828, -0.82006705,  0.5223211 ,  0.55366033]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 1.6307313 ,  0.9477477 , -1.1243066 ,  0.5068073 ,\n",
      "         -1.8343153 ,  1.2745912 , -1.4787502 , -0.777219...5584987, -1.8253518 , -1.048855  ,\n",
      "          0.36126828, -0.82006705,  0.5223211 ,  0.55366033]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8b17aa20>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8b17a438>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-True-12-1-15-1-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 1.49087524e+00,  5.92329800e-01, -1.60939977e-01,\n",
      "          1.19594836e+00, -6.55453861e-01, -2.65341967e-01..., -2.52680206e+00, -8.75702143e-01,\n",
      "         -1.15021265e+00,  2.10110950e+00,  8.82749200e-01]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8b24f390>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 1.49087524e+00,  5.92329800e-01, -1.60939977e-01,\n",
      "          1.19594836e+00, -6.55453861e-01, -2.65341967e-01..., -2.52680206e+00, -8.75702143e-01,\n",
      "         -1.15021265e+00,  2.10110950e+00,  8.82749200e-01]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 1.49087524e+00,  5.92329800e-01, -1.60939977e-01,\n",
      "          1.19594836e+00, -6.55453861e-01, -2.65341967e-01..., -2.52680206e+00, -8.75702143e-01,\n",
      "         -1.15021265e+00,  2.10110950e+00,  8.82749200e-01]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 1.49087524e+00,  5.92329800e-01, -1.60939977e-01,\n",
      "          1.19594836e+00, -6.55453861e-01, -2.65341967e-01..., -2.52680206e+00, -8.75702143e-01,\n",
      "         -1.15021265e+00,  2.10110950e+00,  8.82749200e-01]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8b24f3c8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8b24f128>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-True-12-1-15-2-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-2.67867416e-01,  1.15274906e+00,  8.08792412e-02,\n",
      "         -9.45308983e-01, -5.52106559e-01, -2.98287541e-01..., -7.85343349e-01,  1.33628935e-01,\n",
      "         -6.54188871e-01, -6.72798812e-01, -1.00577748e+00]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d3e8dd8>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-2.67867416e-01,  1.15274906e+00,  8.08792412e-02,\n",
      "         -9.45308983e-01, -5.52106559e-01, -2.98287541e-01..., -7.85343349e-01,  1.33628935e-01,\n",
      "         -6.54188871e-01, -6.72798812e-01, -1.00577748e+00]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-2.67867416e-01,  1.15274906e+00,  8.08792412e-02,\n",
      "         -9.45308983e-01, -5.52106559e-01, -2.98287541e-01..., -7.85343349e-01,  1.33628935e-01,\n",
      "         -6.54188871e-01, -6.72798812e-01, -1.00577748e+00]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-2.67867416e-01,  1.15274906e+00,  8.08792412e-02,\n",
      "         -9.45308983e-01, -5.52106559e-01, -2.98287541e-01..., -7.85343349e-01,  1.33628935e-01,\n",
      "         -6.54188871e-01, -6.72798812e-01, -1.00577748e+00]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d3e8c88>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d3e8eb8>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-True-12-1-15-2-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-2.66995400e-01,  2.50096321e-01, -1.01369480e-03,\n",
      "          7.26220429e-01, -1.57844007e+00, -8.75977576e-01...,  2.74478972e-01,  1.57081866e+00,\n",
      "          3.12104493e-01,  1.29510176e+00, -7.85299465e-02]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8b17de10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-2.66995400e-01,  2.50096321e-01, -1.01369480e-03,\n",
      "          7.26220429e-01, -1.57844007e+00, -8.75977576e-01...,  2.74478972e-01,  1.57081866e+00,\n",
      "          3.12104493e-01,  1.29510176e+00, -7.85299465e-02]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-2.66995400e-01,  2.50096321e-01, -1.01369480e-03,\n",
      "          7.26220429e-01, -1.57844007e+00, -8.75977576e-01...,  2.74478972e-01,  1.57081866e+00,\n",
      "          3.12104493e-01,  1.29510176e+00, -7.85299465e-02]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-2.66995400e-01,  2.50096321e-01, -1.01369480e-03,\n",
      "          7.26220429e-01, -1.57844007e+00, -8.75977576e-01...,  2.74478972e-01,  1.57081866e+00,\n",
      "          3.12104493e-01,  1.29510176e+00, -7.85299465e-02]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8b17d940>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8b17d898>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-True-12-34-1-1-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 0.6968625 , -0.8046939 , -0.8960416 ,  0.5933122 ,\n",
      "         -0.0449112 , -2.1908593 ,  1.1246326 ,  0.31854257,\n",
      "          0.7757645 ,  0.12720434, -0.04834571, -1.1986461 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8ae43b00>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 0.6968625 , -0.8046939 , -0.8960416 ,  0.5933122 ,\n",
      "         -0.0449112 , -2.1908593 ,  1.1246326 ,  0.31854257,\n",
      "          0.7757645 ,  0.12720434, -0.04834571, -1.1986461 ]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 0.6968625 , -0.8046939 , -0.8960416 ,  0.5933122 ,\n",
      "         -0.0449112 , -2.1908593 ,  1.1246326 ,  0.31854257,\n",
      "          0.7757645 ,  0.12720434, -0.04834571, -1.1986461 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 0.6968625 , -0.8046939 , -0.8960416 ,  0.5933122 ,\n",
      "         -0.0449112 , -2.1908593 ,  1.1246326 ,  0.31854257,\n",
      "          0.7757645 ,  0.12720434, -0.04834571, -1.1986461 ]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8ae43710>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8ae43eb8>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-True-12-34-1-1-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 0.8025161 ,  0.8774632 , -1.9300404 , -0.48578742,\n",
      "          0.2984449 , -0.05922911, -0.09152806, -0.04586298,\n",
      "         -1.6389444 , -0.99134374,  0.59274423,  1.7252145 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d6b7898>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 0.8025161 ,  0.8774632 , -1.9300404 , -0.48578742,\n",
      "          0.2984449 , -0.05922911, -0.09152806, -0.04586298,\n",
      "         -1.6389444 , -0.99134374,  0.59274423,  1.7252145 ]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 0.8025161 ,  0.8774632 , -1.9300404 , -0.48578742,\n",
      "          0.2984449 , -0.05922911, -0.09152806, -0.04586298,\n",
      "         -1.6389444 , -0.99134374,  0.59274423,  1.7252145 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 0.8025161 ,  0.8774632 , -1.9300404 , -0.48578742,\n",
      "          0.2984449 , -0.05922911, -0.09152806, -0.04586298,\n",
      "         -1.6389444 , -0.99134374,  0.59274423,  1.7252145 ]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d6b7518>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d6b7b70>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-True-12-34-1-2-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-1.1670702 , -0.43506444, -0.2893483 ,  1.0211321 ,\n",
      "         -1.7746226 ,  0.2718975 , -1.6688026 ,  0.190376...54977  ,  1.0540757 ,  1.2071985 ,\n",
      "          0.2870996 ,  0.8272443 , -1.682952  ,  1.3161759 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d6b97b8>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-1.1670702 , -0.43506444, -0.2893483 ,  1.0211321 ,\n",
      "         -1.7746226 ,  0.2718975 , -1.6688026 ,  0.190376...54977  ,  1.0540757 ,  1.2071985 ,\n",
      "          0.2870996 ,  0.8272443 , -1.682952  ,  1.3161759 ]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-1.1670702 , -0.43506444, -0.2893483 ,  1.0211321 ,\n",
      "         -1.7746226 ,  0.2718975 , -1.6688026 ,  0.190376...54977  ,  1.0540757 ,  1.2071985 ,\n",
      "          0.2870996 ,  0.8272443 , -1.682952  ,  1.3161759 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-1.1670702 , -0.43506444, -0.2893483 ,  1.0211321 ,\n",
      "         -1.7746226 ,  0.2718975 , -1.6688026 ,  0.190376...54977  ,  1.0540757 ,  1.2071985 ,\n",
      "          0.2870996 ,  0.8272443 , -1.682952  ,  1.3161759 ]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d6b95c0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d6b96d8>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-True-12-34-1-2-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.6164715 ,  0.71997046,  1.4943802 , -1.0902181 ,\n",
      "          0.3775785 , -2.345937  ,  0.4420846 ,  0.605730...4017377, -1.2195109 , -1.5124743 ,\n",
      "          1.1101441 , -0.12459257, -0.05888505,  0.56911993]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8af5b358>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.6164715 ,  0.71997046,  1.4943802 , -1.0902181 ,\n",
      "          0.3775785 , -2.345937  ,  0.4420846 ,  0.605730...4017377, -1.2195109 , -1.5124743 ,\n",
      "          1.1101441 , -0.12459257, -0.05888505,  0.56911993]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.6164715 ,  0.71997046,  1.4943802 , -1.0902181 ,\n",
      "          0.3775785 , -2.345937  ,  0.4420846 ,  0.605730...4017377, -1.2195109 , -1.5124743 ,\n",
      "          1.1101441 , -0.12459257, -0.05888505,  0.56911993]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.6164715 ,  0.71997046,  1.4943802 , -1.0902181 ,\n",
      "          0.3775785 , -2.345937  ,  0.4420846 ,  0.605730...4017377, -1.2195109 , -1.5124743 ,\n",
      "          1.1101441 , -0.12459257, -0.05888505,  0.56911993]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8af5bc88>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8af5bda0>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-True-12-34-15-1-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 1.5387565e-01,  6.7968035e-01,  8.2727003e-01, -4.8117256e-01,\n",
      "         -6.2819898e-01, -1.0134008e+00,  7.5...2e-01,  1.6316050e-01,\n",
      "          7.9389149e-01,  1.6846275e+00,  5.5895972e-01, -1.4366676e+00]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8b0d9be0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 1.5387565e-01,  6.7968035e-01,  8.2727003e-01, -4.8117256e-01,\n",
      "         -6.2819898e-01, -1.0134008e+00,  7.5...2e-01,  1.6316050e-01,\n",
      "          7.9389149e-01,  1.6846275e+00,  5.5895972e-01, -1.4366676e+00]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 1.5387565e-01,  6.7968035e-01,  8.2727003e-01, -4.8117256e-01,\n",
      "         -6.2819898e-01, -1.0134008e+00,  7.5...2e-01,  1.6316050e-01,\n",
      "          7.9389149e-01,  1.6846275e+00,  5.5895972e-01, -1.4366676e+00]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 1.5387565e-01,  6.7968035e-01,  8.2727003e-01, -4.8117256e-01,\n",
      "         -6.2819898e-01, -1.0134008e+00,  7.5...2e-01,  1.6316050e-01,\n",
      "          7.9389149e-01,  1.6846275e+00,  5.5895972e-01, -1.4366676e+00]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8b0d97f0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8b0d95f8>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-True-12-34-15-1-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 0.13440827, -0.823138  ,  0.54554886, -2.2684326 ,\n",
      "         -0.12028381, -0.22065006,  0.5451129 , -0.306208...57348  , -0.48708922,  0.34304893,\n",
      "         -0.8143099 ,  0.03041148,  0.5109522 , -0.7516792 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8ac3c518>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 0.13440827, -0.823138  ,  0.54554886, -2.2684326 ,\n",
      "         -0.12028381, -0.22065006,  0.5451129 , -0.306208...57348  , -0.48708922,  0.34304893,\n",
      "         -0.8143099 ,  0.03041148,  0.5109522 , -0.7516792 ]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 0.13440827, -0.823138  ,  0.54554886, -2.2684326 ,\n",
      "         -0.12028381, -0.22065006,  0.5451129 , -0.306208...57348  , -0.48708922,  0.34304893,\n",
      "         -0.8143099 ,  0.03041148,  0.5109522 , -0.7516792 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 0.13440827, -0.823138  ,  0.54554886, -2.2684326 ,\n",
      "         -0.12028381, -0.22065006,  0.5451129 , -0.306208...57348  , -0.48708922,  0.34304893,\n",
      "         -0.8143099 ,  0.03041148,  0.5109522 , -0.7516792 ]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8ac3cba8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8ac3cc50>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-True-12-34-15-2-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 3.1684648e-02,  7.7337861e-01,  5.3461444e-01,  4.9806458e-01,\n",
      "          1.2948489e+00, -3.2122758e-01,  1.2...2e+00,  7.4448282e-01,\n",
      "          2.0502040e+00,  6.7024857e-01,  4.9528681e-02,  1.5952048e-01]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d6b9e48>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 3.1684648e-02,  7.7337861e-01,  5.3461444e-01,  4.9806458e-01,\n",
      "          1.2948489e+00, -3.2122758e-01,  1.2...2e+00,  7.4448282e-01,\n",
      "          2.0502040e+00,  6.7024857e-01,  4.9528681e-02,  1.5952048e-01]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 3.1684648e-02,  7.7337861e-01,  5.3461444e-01,  4.9806458e-01,\n",
      "          1.2948489e+00, -3.2122758e-01,  1.2...2e+00,  7.4448282e-01,\n",
      "          2.0502040e+00,  6.7024857e-01,  4.9528681e-02,  1.5952048e-01]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 3.1684648e-02,  7.7337861e-01,  5.3461444e-01,  4.9806458e-01,\n",
      "          1.2948489e+00, -3.2122758e-01,  1.2...2e+00,  7.4448282e-01,\n",
      "          2.0502040e+00,  6.7024857e-01,  4.9528681e-02,  1.5952048e-01]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d6b9470>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d6b9908>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-True-12-34-15-2-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 6.52350962e-01, -1.39522264e-02, -1.10956001e+00,\n",
      "         -1.87072515e+00, -2.87549555e-01, -2.55765647e-01..., -9.35390294e-01,  3.04071069e-01,\n",
      "         -1.16085696e+00, -1.97852898e+00, -1.02311790e+00]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d6b7cc0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 6.52350962e-01, -1.39522264e-02, -1.10956001e+00,\n",
      "         -1.87072515e+00, -2.87549555e-01, -2.55765647e-01..., -9.35390294e-01,  3.04071069e-01,\n",
      "         -1.16085696e+00, -1.97852898e+00, -1.02311790e+00]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 6.52350962e-01, -1.39522264e-02, -1.10956001e+00,\n",
      "         -1.87072515e+00, -2.87549555e-01, -2.55765647e-01..., -9.35390294e-01,  3.04071069e-01,\n",
      "         -1.16085696e+00, -1.97852898e+00, -1.02311790e+00]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 6.52350962e-01, -1.39522264e-02, -1.10956001e+00,\n",
      "         -1.87072515e+00, -2.87549555e-01, -2.55765647e-01..., -9.35390294e-01,  3.04071069e-01,\n",
      "         -1.16085696e+00, -1.97852898e+00, -1.02311790e+00]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d6b7908>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d6b7710>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-False-1-1-1-1-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-1.2967196]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8ad32dd8>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-1.2967196]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-1.2967196]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-1.2967196]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8ad32ac8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8ad329e8>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-False-1-1-1-1-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.48926884]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d50beb8>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.48926884]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.48926884]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.48926884]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d50b2e8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d50b278>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-False-1-1-1-2-1] ________\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.3569762]],\n",
      "\n",
      "       [[-2.4829571]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d31fa58>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.3569762]],\n",
      "\n",
      "       [[-2.4829571]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.3569762]],\n",
      "\n",
      "       [[-2.4829571]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.3569762]],\n",
      "\n",
      "       [[-2.4829571]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d31fac8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d31f080>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-False-1-1-1-2-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.487211 ]],\n",
      "\n",
      "       [[ 0.8055875]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d50b7f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.487211 ]],\n",
      "\n",
      "       [[ 0.8055875]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.487211 ]],\n",
      "\n",
      "       [[ 0.8055875]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.487211 ]],\n",
      "\n",
      "       [[ 0.8055875]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d50b470>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d50b048>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-False-1-1-15-1-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-1.4012849 ],\n",
      "        [-0.7106078 ],\n",
      "        [-0.44874716],\n",
      "        [-0.55248994],\n",
      "        [ 0.43982306],\n",
      "   ...0159125 ],\n",
      "        [-0.71429604],\n",
      "        [ 0.9195279 ],\n",
      "        [-1.5269486 ],\n",
      "        [-0.9928179 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d6b9a90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-1.4012849 ],\n",
      "        [-0.7106078 ],\n",
      "        [-0.44874716],\n",
      "        [-0.55248994],\n",
      "        [ 0.43982306],\n",
      "   ...0159125 ],\n",
      "        [-0.71429604],\n",
      "        [ 0.9195279 ],\n",
      "        [-1.5269486 ],\n",
      "        [-0.9928179 ]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-1.4012849 ],\n",
      "        [-0.7106078 ],\n",
      "        [-0.44874716],\n",
      "        [-0.55248994],\n",
      "        [ 0.43982306],\n",
      "   ...0159125 ],\n",
      "        [-0.71429604],\n",
      "        [ 0.9195279 ],\n",
      "        [-1.5269486 ],\n",
      "        [-0.9928179 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-1.4012849 ],\n",
      "        [-0.7106078 ],\n",
      "        [-0.44874716],\n",
      "        [-0.55248994],\n",
      "        [ 0.43982306],\n",
      "   ...0159125 ],\n",
      "        [-0.71429604],\n",
      "        [ 0.9195279 ],\n",
      "        [-1.5269486 ],\n",
      "        [-0.9928179 ]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d6b9be0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d6b9438>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-False-1-1-15-1-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.08505517],\n",
      "        [ 0.7226807 ],\n",
      "        [-0.06353617],\n",
      "        [ 0.41219783],\n",
      "        [-1.3199695 ],\n",
      "   ...10732098],\n",
      "        [-0.32146773],\n",
      "        [ 0.7100453 ],\n",
      "        [-0.5921178 ],\n",
      "        [-0.8760154 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8acb3940>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.08505517],\n",
      "        [ 0.7226807 ],\n",
      "        [-0.06353617],\n",
      "        [ 0.41219783],\n",
      "        [-1.3199695 ],\n",
      "   ...10732098],\n",
      "        [-0.32146773],\n",
      "        [ 0.7100453 ],\n",
      "        [-0.5921178 ],\n",
      "        [-0.8760154 ]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.08505517],\n",
      "        [ 0.7226807 ],\n",
      "        [-0.06353617],\n",
      "        [ 0.41219783],\n",
      "        [-1.3199695 ],\n",
      "   ...10732098],\n",
      "        [-0.32146773],\n",
      "        [ 0.7100453 ],\n",
      "        [-0.5921178 ],\n",
      "        [-0.8760154 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.08505517],\n",
      "        [ 0.7226807 ],\n",
      "        [-0.06353617],\n",
      "        [ 0.41219783],\n",
      "        [-1.3199695 ],\n",
      "   ...10732098],\n",
      "        [-0.32146773],\n",
      "        [ 0.7100453 ],\n",
      "        [-0.5921178 ],\n",
      "        [-0.8760154 ]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8acb3908>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8acb37b8>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-False-1-1-15-2-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 1.2529162 ],\n",
      "        [-1.2936548 ],\n",
      "        [ 0.52514   ],\n",
      "        [-2.230198  ],\n",
      "        [ 0.44727874],\n",
      "   ...566993  ],\n",
      "        [-0.6556638 ],\n",
      "        [-0.88059735],\n",
      "        [ 0.34910366],\n",
      "        [-1.6142222 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d6b9978>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 1.2529162 ],\n",
      "        [-1.2936548 ],\n",
      "        [ 0.52514   ],\n",
      "        [-2.230198  ],\n",
      "        [ 0.44727874],\n",
      "   ...566993  ],\n",
      "        [-0.6556638 ],\n",
      "        [-0.88059735],\n",
      "        [ 0.34910366],\n",
      "        [-1.6142222 ]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 1.2529162 ],\n",
      "        [-1.2936548 ],\n",
      "        [ 0.52514   ],\n",
      "        [-2.230198  ],\n",
      "        [ 0.44727874],\n",
      "   ...566993  ],\n",
      "        [-0.6556638 ],\n",
      "        [-0.88059735],\n",
      "        [ 0.34910366],\n",
      "        [-1.6142222 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 1.2529162 ],\n",
      "        [-1.2936548 ],\n",
      "        [ 0.52514   ],\n",
      "        [-2.230198  ],\n",
      "        [ 0.44727874],\n",
      "   ...566993  ],\n",
      "        [-0.6556638 ],\n",
      "        [-0.88059735],\n",
      "        [ 0.34910366],\n",
      "        [-1.6142222 ]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d6b97f0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d6b9828>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-False-1-1-15-2-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.14731367],\n",
      "        [-0.5717174 ],\n",
      "        [-0.9330612 ],\n",
      "        [-0.5330706 ],\n",
      "        [ 1.8157257 ],\n",
      "   ...98943895],\n",
      "        [ 1.4160885 ],\n",
      "        [ 0.03853826],\n",
      "        [ 0.3223383 ],\n",
      "        [-0.44132602]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d6ac128>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.14731367],\n",
      "        [-0.5717174 ],\n",
      "        [-0.9330612 ],\n",
      "        [-0.5330706 ],\n",
      "        [ 1.8157257 ],\n",
      "   ...98943895],\n",
      "        [ 1.4160885 ],\n",
      "        [ 0.03853826],\n",
      "        [ 0.3223383 ],\n",
      "        [-0.44132602]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.14731367],\n",
      "        [-0.5717174 ],\n",
      "        [-0.9330612 ],\n",
      "        [-0.5330706 ],\n",
      "        [ 1.8157257 ],\n",
      "   ...98943895],\n",
      "        [ 1.4160885 ],\n",
      "        [ 0.03853826],\n",
      "        [ 0.3223383 ],\n",
      "        [-0.44132602]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.14731367],\n",
      "        [-0.5717174 ],\n",
      "        [-0.9330612 ],\n",
      "        [-0.5330706 ],\n",
      "        [ 1.8157257 ],\n",
      "   ...98943895],\n",
      "        [ 1.4160885 ],\n",
      "        [ 0.03853826],\n",
      "        [ 0.3223383 ],\n",
      "        [-0.44132602]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d6ac3c8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d6acf28>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-False-1-34-1-1-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-1.1784312]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8b16a4a8>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-1.1784312]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-1.1784312]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-1.1784312]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8b16a7b8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8b16af60>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-False-1-34-1-1-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-1.5874013]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d5a1048>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-1.5874013]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-1.5874013]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-1.5874013]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d5a1550>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d5a1630>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-False-1-34-1-2-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-1.1702715 ]],\n",
      "\n",
      "       [[ 0.00513608]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d407898>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-1.1702715 ]],\n",
      "\n",
      "       [[ 0.00513608]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-1.1702715 ]],\n",
      "\n",
      "       [[ 0.00513608]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-1.1702715 ]],\n",
      "\n",
      "       [[ 0.00513608]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d407588>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d407630>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-False-1-34-1-2-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.04091227]],\n",
      "\n",
      "       [[ 0.07962015]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8ac46588>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.04091227]],\n",
      "\n",
      "       [[ 0.07962015]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.04091227]],\n",
      "\n",
      "       [[ 0.07962015]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.04091227]],\n",
      "\n",
      "       [[ 0.07962015]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8ac46b70>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8ac46f98>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-False-1-34-15-1-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 1.9120516 ],\n",
      "        [-0.8157427 ],\n",
      "        [ 0.27329087],\n",
      "        [-0.07928905],\n",
      "        [-0.34651178],\n",
      "   ...4871925 ],\n",
      "        [-0.29283243],\n",
      "        [ 0.6976769 ],\n",
      "        [-0.57035494],\n",
      "        [ 0.67118037]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d407b38>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 1.9120516 ],\n",
      "        [-0.8157427 ],\n",
      "        [ 0.27329087],\n",
      "        [-0.07928905],\n",
      "        [-0.34651178],\n",
      "   ...4871925 ],\n",
      "        [-0.29283243],\n",
      "        [ 0.6976769 ],\n",
      "        [-0.57035494],\n",
      "        [ 0.67118037]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 1.9120516 ],\n",
      "        [-0.8157427 ],\n",
      "        [ 0.27329087],\n",
      "        [-0.07928905],\n",
      "        [-0.34651178],\n",
      "   ...4871925 ],\n",
      "        [-0.29283243],\n",
      "        [ 0.6976769 ],\n",
      "        [-0.57035494],\n",
      "        [ 0.67118037]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 1.9120516 ],\n",
      "        [-0.8157427 ],\n",
      "        [ 0.27329087],\n",
      "        [-0.07928905],\n",
      "        [-0.34651178],\n",
      "   ...4871925 ],\n",
      "        [-0.29283243],\n",
      "        [ 0.6976769 ],\n",
      "        [-0.57035494],\n",
      "        [ 0.67118037]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d407c88>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d407240>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-False-1-34-15-1-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 0.5559772 ],\n",
      "        [ 0.2641369 ],\n",
      "        [-0.90957916],\n",
      "        [-0.17898846],\n",
      "        [-0.30823967],\n",
      "   ...221834  ],\n",
      "        [-0.24431008],\n",
      "        [ 0.43343514],\n",
      "        [ 0.22049612],\n",
      "        [ 0.6244663 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8ac3c6a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 0.5559772 ],\n",
      "        [ 0.2641369 ],\n",
      "        [-0.90957916],\n",
      "        [-0.17898846],\n",
      "        [-0.30823967],\n",
      "   ...221834  ],\n",
      "        [-0.24431008],\n",
      "        [ 0.43343514],\n",
      "        [ 0.22049612],\n",
      "        [ 0.6244663 ]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 0.5559772 ],\n",
      "        [ 0.2641369 ],\n",
      "        [-0.90957916],\n",
      "        [-0.17898846],\n",
      "        [-0.30823967],\n",
      "   ...221834  ],\n",
      "        [-0.24431008],\n",
      "        [ 0.43343514],\n",
      "        [ 0.22049612],\n",
      "        [ 0.6244663 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 0.5559772 ],\n",
      "        [ 0.2641369 ],\n",
      "        [-0.90957916],\n",
      "        [-0.17898846],\n",
      "        [-0.30823967],\n",
      "   ...221834  ],\n",
      "        [-0.24431008],\n",
      "        [ 0.43343514],\n",
      "        [ 0.22049612],\n",
      "        [ 0.6244663 ]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8ac3c048>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8ac3cc50>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-False-1-34-15-2-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-1.8230641e+00],\n",
      "        [-1.5100093e+00],\n",
      "        [ 2.7450981e-02],\n",
      "        [-4.2464247e-01],\n",
      "        [ 1.16...       [-4.5869589e-01],\n",
      "        [-1.4909040e+00],\n",
      "        [-1.9857830e+00],\n",
      "        [ 1.0005305e-01]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d471b38>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-1.8230641e+00],\n",
      "        [-1.5100093e+00],\n",
      "        [ 2.7450981e-02],\n",
      "        [-4.2464247e-01],\n",
      "        [ 1.16...       [-4.5869589e-01],\n",
      "        [-1.4909040e+00],\n",
      "        [-1.9857830e+00],\n",
      "        [ 1.0005305e-01]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-1.8230641e+00],\n",
      "        [-1.5100093e+00],\n",
      "        [ 2.7450981e-02],\n",
      "        [-4.2464247e-01],\n",
      "        [ 1.16...       [-4.5869589e-01],\n",
      "        [-1.4909040e+00],\n",
      "        [-1.9857830e+00],\n",
      "        [ 1.0005305e-01]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-1.8230641e+00],\n",
      "        [-1.5100093e+00],\n",
      "        [ 2.7450981e-02],\n",
      "        [-4.2464247e-01],\n",
      "        [ 1.16...       [-4.5869589e-01],\n",
      "        [-1.4909040e+00],\n",
      "        [-1.9857830e+00],\n",
      "        [ 1.0005305e-01]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d471588>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d471a90>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-False-1-34-15-2-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-1.4574245 ],\n",
      "        [-0.2013997 ],\n",
      "        [-0.19803645],\n",
      "        [ 0.28375617],\n",
      "        [ 0.7522538 ],\n",
      "   ...7067582 ],\n",
      "        [-0.5666698 ],\n",
      "        [ 0.54220015],\n",
      "        [ 0.9579207 ],\n",
      "        [ 0.4349071 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d7d79b0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-1.4574245 ],\n",
      "        [-0.2013997 ],\n",
      "        [-0.19803645],\n",
      "        [ 0.28375617],\n",
      "        [ 0.7522538 ],\n",
      "   ...7067582 ],\n",
      "        [-0.5666698 ],\n",
      "        [ 0.54220015],\n",
      "        [ 0.9579207 ],\n",
      "        [ 0.4349071 ]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-1.4574245 ],\n",
      "        [-0.2013997 ],\n",
      "        [-0.19803645],\n",
      "        [ 0.28375617],\n",
      "        [ 0.7522538 ],\n",
      "   ...7067582 ],\n",
      "        [-0.5666698 ],\n",
      "        [ 0.54220015],\n",
      "        [ 0.9579207 ],\n",
      "        [ 0.4349071 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-1.4574245 ],\n",
      "        [-0.2013997 ],\n",
      "        [-0.19803645],\n",
      "        [ 0.28375617],\n",
      "        [ 0.7522538 ],\n",
      "   ...7067582 ],\n",
      "        [-0.5666698 ],\n",
      "        [ 0.54220015],\n",
      "        [ 0.9579207 ],\n",
      "        [ 0.4349071 ]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d7d7cf8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d7d7a90>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-False-12-1-1-1-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.75205564, -0.400231  , -1.2031878 ,  1.3066326 ,\n",
      "          0.02379085, -0.3247765 , -0.44532517, -0.31247577,\n",
      "          1.3209194 ,  0.17914806,  0.19658919, -0.30138347]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d359eb8>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.75205564, -0.400231  , -1.2031878 ,  1.3066326 ,\n",
      "          0.02379085, -0.3247765 , -0.44532517, -0.31247577,\n",
      "          1.3209194 ,  0.17914806,  0.19658919, -0.30138347]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.75205564, -0.400231  , -1.2031878 ,  1.3066326 ,\n",
      "          0.02379085, -0.3247765 , -0.44532517, -0.31247577,\n",
      "          1.3209194 ,  0.17914806,  0.19658919, -0.30138347]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.75205564, -0.400231  , -1.2031878 ,  1.3066326 ,\n",
      "          0.02379085, -0.3247765 , -0.44532517, -0.31247577,\n",
      "          1.3209194 ,  0.17914806,  0.19658919, -0.30138347]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d359320>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d359cc0>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-False-12-1-1-1-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.39061213,  0.11355296,  0.7585506 , -0.00347438,\n",
      "          0.88874555,  1.996159  ,  0.41603863,  0.2123184 ,\n",
      "          0.5021734 ,  0.49470073,  0.8027639 , -0.59241134]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d45a6a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.39061213,  0.11355296,  0.7585506 , -0.00347438,\n",
      "          0.88874555,  1.996159  ,  0.41603863,  0.2123184 ,\n",
      "          0.5021734 ,  0.49470073,  0.8027639 , -0.59241134]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.39061213,  0.11355296,  0.7585506 , -0.00347438,\n",
      "          0.88874555,  1.996159  ,  0.41603863,  0.2123184 ,\n",
      "          0.5021734 ,  0.49470073,  0.8027639 , -0.59241134]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.39061213,  0.11355296,  0.7585506 , -0.00347438,\n",
      "          0.88874555,  1.996159  ,  0.41603863,  0.2123184 ,\n",
      "          0.5021734 ,  0.49470073,  0.8027639 , -0.59241134]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d45a208>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d45a080>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_______ test_language_model_implementation[cuda-lstm-1-False-12-1-1-2-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 0.61198837, -1.1809194 , -0.37146524,  1.5078272 ,\n",
      "         -0.22554408,  1.4468275 , -0.14404434, -0.930171...6756366,  1.1456013 ,  1.6424134 ,\n",
      "          0.47258177, -0.5554354 ,  0.33145922,  0.08320847]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8ae3c908>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 0.61198837, -1.1809194 , -0.37146524,  1.5078272 ,\n",
      "         -0.22554408,  1.4468275 , -0.14404434, -0.930171...6756366,  1.1456013 ,  1.6424134 ,\n",
      "          0.47258177, -0.5554354 ,  0.33145922,  0.08320847]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 0.61198837, -1.1809194 , -0.37146524,  1.5078272 ,\n",
      "         -0.22554408,  1.4468275 , -0.14404434, -0.930171...6756366,  1.1456013 ,  1.6424134 ,\n",
      "          0.47258177, -0.5554354 ,  0.33145922,  0.08320847]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 0.61198837, -1.1809194 , -0.37146524,  1.5078272 ,\n",
      "         -0.22554408,  1.4468275 , -0.14404434, -0.930171...6756366,  1.1456013 ,  1.6424134 ,\n",
      "          0.47258177, -0.5554354 ,  0.33145922,  0.08320847]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8ae3c1d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8ae3cc88>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-False-12-1-1-2-13] _______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.13748014,  1.8138973 ,  0.11445946, -0.6487165 ,\n",
      "         -0.48486188, -1.1274132 , -0.34208214, -2.138514...2255126, -0.56333345,  0.7195719 ,\n",
      "          1.9258281 ,  0.50510913,  1.7726442 , -1.3750144 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8b043208>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.13748014,  1.8138973 ,  0.11445946, -0.6487165 ,\n",
      "         -0.48486188, -1.1274132 , -0.34208214, -2.138514...2255126, -0.56333345,  0.7195719 ,\n",
      "          1.9258281 ,  0.50510913,  1.7726442 , -1.3750144 ]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.13748014,  1.8138973 ,  0.11445946, -0.6487165 ,\n",
      "         -0.48486188, -1.1274132 , -0.34208214, -2.138514...2255126, -0.56333345,  0.7195719 ,\n",
      "          1.9258281 ,  0.50510913,  1.7726442 , -1.3750144 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.13748014,  1.8138973 ,  0.11445946, -0.6487165 ,\n",
      "         -0.48486188, -1.1274132 , -0.34208214, -2.138514...2255126, -0.56333345,  0.7195719 ,\n",
      "          1.9258281 ,  0.50510913,  1.7726442 , -1.3750144 ]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8b0436d8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8b043c88>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-False-12-1-15-1-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.60961676,  0.60835946, -1.0193657 ,  0.835421  ,\n",
      "         -0.70762855,  1.8981487 , -1.3349706 , -2.034881...6082996,  0.6776007 , -1.0991824 ,\n",
      "         -1.2517905 ,  0.06857385,  0.07265905,  1.7740982 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d5a13c8>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.60961676,  0.60835946, -1.0193657 ,  0.835421  ,\n",
      "         -0.70762855,  1.8981487 , -1.3349706 , -2.034881...6082996,  0.6776007 , -1.0991824 ,\n",
      "         -1.2517905 ,  0.06857385,  0.07265905,  1.7740982 ]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.60961676,  0.60835946, -1.0193657 ,  0.835421  ,\n",
      "         -0.70762855,  1.8981487 , -1.3349706 , -2.034881...6082996,  0.6776007 , -1.0991824 ,\n",
      "         -1.2517905 ,  0.06857385,  0.07265905,  1.7740982 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.60961676,  0.60835946, -1.0193657 ,  0.835421  ,\n",
      "         -0.70762855,  1.8981487 , -1.3349706 , -2.034881...6082996,  0.6776007 , -1.0991824 ,\n",
      "         -1.2517905 ,  0.06857385,  0.07265905,  1.7740982 ]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d5a1f60>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d5a1e80>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-False-12-1-15-1-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 1.4807976 , -0.23713666,  1.8412116 ,  1.1231195 ,\n",
      "         -0.8312116 , -0.09525404,  1.7628142 ,  0.328888...57416  , -0.1911189 ,  2.4391034 ,\n",
      "          0.14664131, -0.19146135, -0.40154973,  0.55880994]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d5d9828>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 1.4807976 , -0.23713666,  1.8412116 ,  1.1231195 ,\n",
      "         -0.8312116 , -0.09525404,  1.7628142 ,  0.328888...57416  , -0.1911189 ,  2.4391034 ,\n",
      "          0.14664131, -0.19146135, -0.40154973,  0.55880994]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 1.4807976 , -0.23713666,  1.8412116 ,  1.1231195 ,\n",
      "         -0.8312116 , -0.09525404,  1.7628142 ,  0.328888...57416  , -0.1911189 ,  2.4391034 ,\n",
      "          0.14664131, -0.19146135, -0.40154973,  0.55880994]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 1.4807976 , -0.23713666,  1.8412116 ,  1.1231195 ,\n",
      "         -0.8312116 , -0.09525404,  1.7628142 ,  0.328888...57416  , -0.1911189 ,  2.4391034 ,\n",
      "          0.14664131, -0.19146135, -0.40154973,  0.55880994]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d5d9e10>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d5d94e0>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-False-12-1-15-2-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-5.94476283e-01, -4.29129183e-01,  1.17034686e+00,\n",
      "         -9.58722830e-01, -5.10408103e-01,  5.76270163e-01...,  1.67789960e+00, -8.63769889e-01,\n",
      "          3.07307541e-01, -1.41732013e+00, -6.09797776e-01]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d6ac3c8>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-5.94476283e-01, -4.29129183e-01,  1.17034686e+00,\n",
      "         -9.58722830e-01, -5.10408103e-01,  5.76270163e-01...,  1.67789960e+00, -8.63769889e-01,\n",
      "          3.07307541e-01, -1.41732013e+00, -6.09797776e-01]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-5.94476283e-01, -4.29129183e-01,  1.17034686e+00,\n",
      "         -9.58722830e-01, -5.10408103e-01,  5.76270163e-01...,  1.67789960e+00, -8.63769889e-01,\n",
      "          3.07307541e-01, -1.41732013e+00, -6.09797776e-01]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-5.94476283e-01, -4.29129183e-01,  1.17034686e+00,\n",
      "         -9.58722830e-01, -5.10408103e-01,  5.76270163e-01...,  1.67789960e+00, -8.63769889e-01,\n",
      "          3.07307541e-01, -1.41732013e+00, -6.09797776e-01]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d6ac780>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d6ac400>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-False-12-1-15-2-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-8.72423053e-01,  3.51663798e-01,  1.10680556e+00,\n",
      "         -1.46658719e-01,  2.47809261e-01, -3.26968431e-01...,  3.77119452e-01,  6.41514838e-01,\n",
      "         -1.16623676e+00, -6.91467941e-01, -6.51336163e-02]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8b16a978>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-8.72423053e-01,  3.51663798e-01,  1.10680556e+00,\n",
      "         -1.46658719e-01,  2.47809261e-01, -3.26968431e-01...,  3.77119452e-01,  6.41514838e-01,\n",
      "         -1.16623676e+00, -6.91467941e-01, -6.51336163e-02]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-8.72423053e-01,  3.51663798e-01,  1.10680556e+00,\n",
      "         -1.46658719e-01,  2.47809261e-01, -3.26968431e-01...,  3.77119452e-01,  6.41514838e-01,\n",
      "         -1.16623676e+00, -6.91467941e-01, -6.51336163e-02]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-8.72423053e-01,  3.51663798e-01,  1.10680556e+00,\n",
      "         -1.46658719e-01,  2.47809261e-01, -3.26968431e-01...,  3.77119452e-01,  6.41514838e-01,\n",
      "         -1.16623676e+00, -6.91467941e-01, -6.51336163e-02]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8b16aba8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8b16a198>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-False-12-34-1-1-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.8390008 , -0.6728069 ,  1.029781  , -0.24591874,\n",
      "          1.0805453 ,  0.51234674,  2.3017735 ,  1.8392552 ,\n",
      "          0.14454699,  0.17239138, -0.7808016 ,  1.9784786 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8ae50ef0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.8390008 , -0.6728069 ,  1.029781  , -0.24591874,\n",
      "          1.0805453 ,  0.51234674,  2.3017735 ,  1.8392552 ,\n",
      "          0.14454699,  0.17239138, -0.7808016 ,  1.9784786 ]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.8390008 , -0.6728069 ,  1.029781  , -0.24591874,\n",
      "          1.0805453 ,  0.51234674,  2.3017735 ,  1.8392552 ,\n",
      "          0.14454699,  0.17239138, -0.7808016 ,  1.9784786 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.8390008 , -0.6728069 ,  1.029781  , -0.24591874,\n",
      "          1.0805453 ,  0.51234674,  2.3017735 ,  1.8392552 ,\n",
      "          0.14454699,  0.17239138, -0.7808016 ,  1.9784786 ]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8ae50978>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8ae50a90>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-False-12-34-1-1-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.3090797 ,  0.03541571,  0.10334412,  1.5712212 ,\n",
      "         -1.2286425 ,  0.5836974 , -0.51421237,  0.35992754,\n",
      "         -1.9804106 , -0.9917184 , -0.20687194,  0.03263685]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d48eda0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.3090797 ,  0.03541571,  0.10334412,  1.5712212 ,\n",
      "         -1.2286425 ,  0.5836974 , -0.51421237,  0.35992754,\n",
      "         -1.9804106 , -0.9917184 , -0.20687194,  0.03263685]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.3090797 ,  0.03541571,  0.10334412,  1.5712212 ,\n",
      "         -1.2286425 ,  0.5836974 , -0.51421237,  0.35992754,\n",
      "         -1.9804106 , -0.9917184 , -0.20687194,  0.03263685]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.3090797 ,  0.03541571,  0.10334412,  1.5712212 ,\n",
      "         -1.2286425 ,  0.5836974 , -0.51421237,  0.35992754,\n",
      "         -1.9804106 , -0.9917184 , -0.20687194,  0.03263685]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d48e7f0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d48e358>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-False-12-34-1-2-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-1.3286996e-01,  1.9755453e+00, -3.6398891e-01,  3.3569387e-01,\n",
      "          1.8005176e-01, -3.8925633e-01, -8.6...0e-01, -4.7426968e-04,\n",
      "          4.1127580e-01, -3.9397755e-01, -5.2240872e-01,  1.2276914e+00]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d6ac6d8>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-1.3286996e-01,  1.9755453e+00, -3.6398891e-01,  3.3569387e-01,\n",
      "          1.8005176e-01, -3.8925633e-01, -8.6...0e-01, -4.7426968e-04,\n",
      "          4.1127580e-01, -3.9397755e-01, -5.2240872e-01,  1.2276914e+00]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-1.3286996e-01,  1.9755453e+00, -3.6398891e-01,  3.3569387e-01,\n",
      "          1.8005176e-01, -3.8925633e-01, -8.6...0e-01, -4.7426968e-04,\n",
      "          4.1127580e-01, -3.9397755e-01, -5.2240872e-01,  1.2276914e+00]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-1.3286996e-01,  1.9755453e+00, -3.6398891e-01,  3.3569387e-01,\n",
      "          1.8005176e-01, -3.8925633e-01, -8.6...0e-01, -4.7426968e-04,\n",
      "          4.1127580e-01, -3.9397755e-01, -5.2240872e-01,  1.2276914e+00]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d6ac4e0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d6ac6a0>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-False-12-34-1-2-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 1.1349584 ,  0.7896287 , -0.25979453, -1.5934777 ,\n",
      "         -0.389567  , -0.8571305 ,  0.06557515,  0.215276...8161164,  0.3955052 ,  0.11840755,\n",
      "          2.1784267 ,  0.1749584 , -0.47919703, -0.286161  ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8ad6cf98>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 1.1349584 ,  0.7896287 , -0.25979453, -1.5934777 ,\n",
      "         -0.389567  , -0.8571305 ,  0.06557515,  0.215276...8161164,  0.3955052 ,  0.11840755,\n",
      "          2.1784267 ,  0.1749584 , -0.47919703, -0.286161  ]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 1.1349584 ,  0.7896287 , -0.25979453, -1.5934777 ,\n",
      "         -0.389567  , -0.8571305 ,  0.06557515,  0.215276...8161164,  0.3955052 ,  0.11840755,\n",
      "          2.1784267 ,  0.1749584 , -0.47919703, -0.286161  ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 1.1349584 ,  0.7896287 , -0.25979453, -1.5934777 ,\n",
      "         -0.389567  , -0.8571305 ,  0.06557515,  0.215276...8161164,  0.3955052 ,  0.11840755,\n",
      "          2.1784267 ,  0.1749584 , -0.47919703, -0.286161  ]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8ad6c1d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8ad6ce80>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-False-12-34-15-1-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 2.02517301e-01,  1.11200881e+00, -7.85817385e-01,\n",
      "          1.30611464e-01,  3.42622578e-01,  9.21808183e-02...,  1.11004853e+00, -6.36541903e-01,\n",
      "         -5.57707138e-02,  8.39220047e-01, -8.62081528e-01]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8acf2fd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 2.02517301e-01,  1.11200881e+00, -7.85817385e-01,\n",
      "          1.30611464e-01,  3.42622578e-01,  9.21808183e-02...,  1.11004853e+00, -6.36541903e-01,\n",
      "         -5.57707138e-02,  8.39220047e-01, -8.62081528e-01]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 2.02517301e-01,  1.11200881e+00, -7.85817385e-01,\n",
      "          1.30611464e-01,  3.42622578e-01,  9.21808183e-02...,  1.11004853e+00, -6.36541903e-01,\n",
      "         -5.57707138e-02,  8.39220047e-01, -8.62081528e-01]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 2.02517301e-01,  1.11200881e+00, -7.85817385e-01,\n",
      "          1.30611464e-01,  3.42622578e-01,  9.21808183e-02...,  1.11004853e+00, -6.36541903e-01,\n",
      "         -5.57707138e-02,  8.39220047e-01, -8.62081528e-01]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8acf2c50>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8acf2ac8>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1-False-12-34-15-1-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.7253668 , -0.59755886,  0.7919878 , -1.8797792 ,\n",
      "         -0.30888888, -0.2321272 , -0.8678941 ,  0.207935...172733 , -1.9859698 ,  0.6604798 ,\n",
      "          1.8509002 , -1.8146076 ,  1.6149914 , -0.77425706]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d58c4a8>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.7253668 , -0.59755886,  0.7919878 , -1.8797792 ,\n",
      "         -0.30888888, -0.2321272 , -0.8678941 ,  0.207935...172733 , -1.9859698 ,  0.6604798 ,\n",
      "          1.8509002 , -1.8146076 ,  1.6149914 , -0.77425706]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.7253668 , -0.59755886,  0.7919878 , -1.8797792 ,\n",
      "         -0.30888888, -0.2321272 , -0.8678941 ,  0.207935...172733 , -1.9859698 ,  0.6604798 ,\n",
      "          1.8509002 , -1.8146076 ,  1.6149914 , -0.77425706]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.7253668 , -0.59755886,  0.7919878 , -1.8797792 ,\n",
      "         -0.30888888, -0.2321272 , -0.8678941 ,  0.207935...172733 , -1.9859698 ,  0.6604798 ,\n",
      "          1.8509002 , -1.8146076 ,  1.6149914 , -0.77425706]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d58c128>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d48e518>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1-False-12-34-15-2-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 4.57609028e-01,  2.08502561e-01, -6.79295003e-01,\n",
      "         -5.90703070e-01, -1.62111712e+00, -3.74645889e-01..., -7.93829918e-01,  7.65202343e-01,\n",
      "          1.76591063e+00, -8.48398358e-02,  1.97056806e+00]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d508358>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 4.57609028e-01,  2.08502561e-01, -6.79295003e-01,\n",
      "         -5.90703070e-01, -1.62111712e+00, -3.74645889e-01..., -7.93829918e-01,  7.65202343e-01,\n",
      "          1.76591063e+00, -8.48398358e-02,  1.97056806e+00]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 4.57609028e-01,  2.08502561e-01, -6.79295003e-01,\n",
      "         -5.90703070e-01, -1.62111712e+00, -3.74645889e-01..., -7.93829918e-01,  7.65202343e-01,\n",
      "          1.76591063e+00, -8.48398358e-02,  1.97056806e+00]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 4.57609028e-01,  2.08502561e-01, -6.79295003e-01,\n",
      "         -5.90703070e-01, -1.62111712e+00, -3.74645889e-01..., -7.93829918e-01,  7.65202343e-01,\n",
      "          1.76591063e+00, -8.48398358e-02,  1.97056806e+00]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d5082b0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d508400>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1-False-12-34-15-2-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0...0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-6.44938827e-01, -1.24899828e+00,  7.20994830e-01,\n",
      "          1.34495509e+00,  4.76344347e-01,  1.60475280e-02...,  2.28240275e+00,  1.63937783e+00,\n",
      "          6.60888433e-01, -9.32319164e-02,  7.99103737e-01]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d6b9978>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-6.44938827e-01, -1.24899828e+00,  7.20994830e-01,\n",
      "          1.34495509e+00,  4.76344347e-01,  1.60475280e-02...,  2.28240275e+00,  1.63937783e+00,\n",
      "          6.60888433e-01, -9.32319164e-02,  7.99103737e-01]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-6.44938827e-01, -1.24899828e+00,  7.20994830e-01,\n",
      "          1.34495509e+00,  4.76344347e-01,  1.60475280e-02...,  2.28240275e+00,  1.63937783e+00,\n",
      "          6.60888433e-01, -9.32319164e-02,  7.99103737e-01]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-6.44938827e-01, -1.24899828e+00,  7.20994830e-01,\n",
      "          1.34495509e+00,  4.76344347e-01,  1.60475280e-02...,  2.28240275e+00,  1.63937783e+00,\n",
      "          6.60888433e-01, -9.32319164e-02,  7.99103737e-01]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d6b9080>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d6b9550>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1000-True-1-1-1-1-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[643.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[2.2245166]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d326e48>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[2.2245166]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[2.2245166]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[2.2245166]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d326f28>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d326c88>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1000-True-1-1-1-1-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[487.],\n",
      "       [790.],\n",
      "       [168.],\n",
      "       [ 71.],\n",
      "       [629.],\n",
      "       [321.],\n",
      "       [783.],\n",
      "       [702.],\n",
      "       [971.],\n",
      "       [444.],\n",
      "       [582.],\n",
      "       [232.],\n",
      "       [894.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.64356816]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d3d2978>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.64356816]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.64356816]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.64356816]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d3d24e0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d3d2a20>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1000-True-1-1-1-2-1] _______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[410.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 0.52335286]],\n",
      "\n",
      "       [[-1.6666439 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d48bb38>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 0.52335286]],\n",
      "\n",
      "       [[-1.6666439 ]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 0.52335286]],\n",
      "\n",
      "       [[-1.6666439 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 0.52335286]],\n",
      "\n",
      "       [[-1.6666439 ]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d48b320>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d48bac8>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1000-True-1-1-1-2-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[602.],\n",
      "       [223.],\n",
      "       [133.],\n",
      "       [320.],\n",
      "       [266.],\n",
      "       [500.],\n",
      "       [389.],\n",
      "       [545.],\n",
      "       [738.],\n",
      "       [ 66.],\n",
      "       [ 85.],\n",
      "       [292.],\n",
      "       [688.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[2.0334625 ]],\n",
      "\n",
      "       [[0.18210669]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8ae3c400>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[2.0334625 ]],\n",
      "\n",
      "       [[0.18210669]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[2.0334625 ]],\n",
      "\n",
      "       [[0.18210669]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[2.0334625 ]],\n",
      "\n",
      "       [[0.18210669]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8ae3cf28>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8ae3cda0>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1000-True-1-1-15-1-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[909., 855., 809.,  28., 409., 772., 108., 834., 592., 937., 388.,\n",
      "        257., 290., 342., 187.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 0.660103  ],\n",
      "        [ 0.31271458],\n",
      "        [-0.76388466],\n",
      "        [-0.36658964],\n",
      "        [ 1.5006272 ],\n",
      "   ...8980462 ],\n",
      "        [-1.8213406 ],\n",
      "        [ 1.3122422 ],\n",
      "        [ 1.1319299 ],\n",
      "        [ 1.4121308 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8ae43128>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 0.660103  ],\n",
      "        [ 0.31271458],\n",
      "        [-0.76388466],\n",
      "        [-0.36658964],\n",
      "        [ 1.5006272 ],\n",
      "   ...8980462 ],\n",
      "        [-1.8213406 ],\n",
      "        [ 1.3122422 ],\n",
      "        [ 1.1319299 ],\n",
      "        [ 1.4121308 ]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 0.660103  ],\n",
      "        [ 0.31271458],\n",
      "        [-0.76388466],\n",
      "        [-0.36658964],\n",
      "        [ 1.5006272 ],\n",
      "   ...8980462 ],\n",
      "        [-1.8213406 ],\n",
      "        [ 1.3122422 ],\n",
      "        [ 1.1319299 ],\n",
      "        [ 1.4121308 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 0.660103  ],\n",
      "        [ 0.31271458],\n",
      "        [-0.76388466],\n",
      "        [-0.36658964],\n",
      "        [ 1.5006272 ],\n",
      "   ...8980462 ],\n",
      "        [-1.8213406 ],\n",
      "        [ 1.3122422 ],\n",
      "        [ 1.1319299 ],\n",
      "        [ 1.4121308 ]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8ae438d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8ae43da0>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-True-1-1-15-1-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[237., 833., 604., 641., 170., 534., 307., 701., 226., 929., 125.,\n",
      "        122., 265., 871., 541.],\n",
      "       [787...    [391., 656., 942., 102., 777., 804., 882., 609., 200., 485., 396.,\n",
      "        370., 944.,  17., 406.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.06568567],\n",
      "        [ 1.3731501 ],\n",
      "        [-0.4222916 ],\n",
      "        [-0.73984116],\n",
      "        [-1.1673123 ],\n",
      "   ...71916807],\n",
      "        [ 0.12141355],\n",
      "        [ 0.571775  ],\n",
      "        [-0.65675163],\n",
      "        [-0.98280257]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8ad6c2b0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.06568567],\n",
      "        [ 1.3731501 ],\n",
      "        [-0.4222916 ],\n",
      "        [-0.73984116],\n",
      "        [-1.1673123 ],\n",
      "   ...71916807],\n",
      "        [ 0.12141355],\n",
      "        [ 0.571775  ],\n",
      "        [-0.65675163],\n",
      "        [-0.98280257]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.06568567],\n",
      "        [ 1.3731501 ],\n",
      "        [-0.4222916 ],\n",
      "        [-0.73984116],\n",
      "        [-1.1673123 ],\n",
      "   ...71916807],\n",
      "        [ 0.12141355],\n",
      "        [ 0.571775  ],\n",
      "        [-0.65675163],\n",
      "        [-0.98280257]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.06568567],\n",
      "        [ 1.3731501 ],\n",
      "        [-0.4222916 ],\n",
      "        [-0.73984116],\n",
      "        [-1.1673123 ],\n",
      "   ...71916807],\n",
      "        [ 0.12141355],\n",
      "        [ 0.571775  ],\n",
      "        [-0.65675163],\n",
      "        [-0.98280257]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8ad6cd30>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8ad6cc50>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1000-True-1-1-15-2-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[278., 630., 485., 953., 619., 647., 108., 820., 750., 175., 496.,\n",
      "         79., 412., 529., 568.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 0.59913146],\n",
      "        [-0.35906857],\n",
      "        [-1.2694135 ],\n",
      "        [-0.71298254],\n",
      "        [ 0.18084718],\n",
      "   ...41729373],\n",
      "        [ 0.41769433],\n",
      "        [-1.2767942 ],\n",
      "        [-0.2859295 ],\n",
      "        [-0.24489143]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d4560b8>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 0.59913146],\n",
      "        [-0.35906857],\n",
      "        [-1.2694135 ],\n",
      "        [-0.71298254],\n",
      "        [ 0.18084718],\n",
      "   ...41729373],\n",
      "        [ 0.41769433],\n",
      "        [-1.2767942 ],\n",
      "        [-0.2859295 ],\n",
      "        [-0.24489143]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 0.59913146],\n",
      "        [-0.35906857],\n",
      "        [-1.2694135 ],\n",
      "        [-0.71298254],\n",
      "        [ 0.18084718],\n",
      "   ...41729373],\n",
      "        [ 0.41769433],\n",
      "        [-1.2767942 ],\n",
      "        [-0.2859295 ],\n",
      "        [-0.24489143]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 0.59913146],\n",
      "        [-0.35906857],\n",
      "        [-1.2694135 ],\n",
      "        [-0.71298254],\n",
      "        [ 0.18084718],\n",
      "   ...41729373],\n",
      "        [ 0.41769433],\n",
      "        [-1.2767942 ],\n",
      "        [-0.2859295 ],\n",
      "        [-0.24489143]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d4569b0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d456080>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-True-1-1-15-2-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[584., 716., 510., 628., 132., 861.,  75.,  82., 701., 435., 467.,\n",
      "        500., 950., 288., 166.],\n",
      "       [696...    [180., 214., 355., 748., 472., 574., 251., 354., 708.,  66., 253.,\n",
      "        770., 645., 250., 725.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 0.43737394],\n",
      "        [-1.9908825 ],\n",
      "        [ 0.89649385],\n",
      "        [-0.31349173],\n",
      "        [ 0.4857148 ],\n",
      "   ...5891742 ],\n",
      "        [ 0.05874595],\n",
      "        [-0.40357208],\n",
      "        [ 0.02433455],\n",
      "        [ 0.813401  ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d370da0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 0.43737394],\n",
      "        [-1.9908825 ],\n",
      "        [ 0.89649385],\n",
      "        [-0.31349173],\n",
      "        [ 0.4857148 ],\n",
      "   ...5891742 ],\n",
      "        [ 0.05874595],\n",
      "        [-0.40357208],\n",
      "        [ 0.02433455],\n",
      "        [ 0.813401  ]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 0.43737394],\n",
      "        [-1.9908825 ],\n",
      "        [ 0.89649385],\n",
      "        [-0.31349173],\n",
      "        [ 0.4857148 ],\n",
      "   ...5891742 ],\n",
      "        [ 0.05874595],\n",
      "        [-0.40357208],\n",
      "        [ 0.02433455],\n",
      "        [ 0.813401  ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 0.43737394],\n",
      "        [-1.9908825 ],\n",
      "        [ 0.89649385],\n",
      "        [-0.31349173],\n",
      "        [ 0.4857148 ],\n",
      "   ...5891742 ],\n",
      "        [ 0.05874595],\n",
      "        [-0.40357208],\n",
      "        [ 0.02433455],\n",
      "        [ 0.813401  ]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d370ac8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d370160>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1000-True-1-34-1-1-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[79.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-1.863637]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d629d68>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-1.863637]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-1.863637]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-1.863637]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d629be0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d629a20>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-True-1-34-1-1-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[620.],\n",
      "       [782.],\n",
      "       [551.],\n",
      "       [713.],\n",
      "       [377.],\n",
      "       [939.],\n",
      "       [763.],\n",
      "       [285.],\n",
      "       [371.],\n",
      "       [978.],\n",
      "       [339.],\n",
      "       [662.],\n",
      "       [300.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-1.3719796]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d4eba20>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-1.3719796]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-1.3719796]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-1.3719796]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d4ebfd0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d4eb6a0>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1000-True-1-34-1-2-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[737.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 1.614881  ]],\n",
      "\n",
      "       [[-0.04536847]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8ae15f28>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 1.614881  ]],\n",
      "\n",
      "       [[-0.04536847]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 1.614881  ]],\n",
      "\n",
      "       [[-0.04536847]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 1.614881  ]],\n",
      "\n",
      "       [[-0.04536847]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8ae153c8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8ae15e48>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-True-1-34-1-2-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[ 29.],\n",
      "       [ 87.],\n",
      "       [361.],\n",
      "       [774.],\n",
      "       [550.],\n",
      "       [257.],\n",
      "       [313.],\n",
      "       [642.],\n",
      "       [490.],\n",
      "       [994.],\n",
      "       [388.],\n",
      "       [132.],\n",
      "       [ 86.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[0.38544208]],\n",
      "\n",
      "       [[0.0794538 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8af723c8>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[0.38544208]],\n",
      "\n",
      "       [[0.0794538 ]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[0.38544208]],\n",
      "\n",
      "       [[0.0794538 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[0.38544208]],\n",
      "\n",
      "       [[0.0794538 ]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8af72e10>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8af724a8>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-True-1-34-15-1-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[137., 556., 373.,  96., 600., 711., 280., 579., 641.,   7.,  64.,\n",
      "         83., 895., 568., 599.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-1.826215  ],\n",
      "        [-1.1420728 ],\n",
      "        [-1.9593116 ],\n",
      "        [-0.79954857],\n",
      "        [ 0.07347175],\n",
      "   ...3406817 ],\n",
      "        [ 1.5253737 ],\n",
      "        [ 0.511894  ],\n",
      "        [-2.261007  ],\n",
      "        [-0.45757335]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d6b6518>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-1.826215  ],\n",
      "        [-1.1420728 ],\n",
      "        [-1.9593116 ],\n",
      "        [-0.79954857],\n",
      "        [ 0.07347175],\n",
      "   ...3406817 ],\n",
      "        [ 1.5253737 ],\n",
      "        [ 0.511894  ],\n",
      "        [-2.261007  ],\n",
      "        [-0.45757335]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-1.826215  ],\n",
      "        [-1.1420728 ],\n",
      "        [-1.9593116 ],\n",
      "        [-0.79954857],\n",
      "        [ 0.07347175],\n",
      "   ...3406817 ],\n",
      "        [ 1.5253737 ],\n",
      "        [ 0.511894  ],\n",
      "        [-2.261007  ],\n",
      "        [-0.45757335]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-1.826215  ],\n",
      "        [-1.1420728 ],\n",
      "        [-1.9593116 ],\n",
      "        [-0.79954857],\n",
      "        [ 0.07347175],\n",
      "   ...3406817 ],\n",
      "        [ 1.5253737 ],\n",
      "        [ 0.511894  ],\n",
      "        [-2.261007  ],\n",
      "        [-0.45757335]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d6b6710>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d6b62b0>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-True-1-34-15-1-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[546., 861., 549., 326., 300., 848., 224., 135., 379., 339., 462.,\n",
      "        502., 153., 311., 971.],\n",
      "       [695...    [487., 816., 458., 743., 692., 190.,  13., 386., 872., 603., 103.,\n",
      "        888., 629.,  84., 288.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 0.72634244],\n",
      "        [-0.13514316],\n",
      "        [-2.179011  ],\n",
      "        [ 0.1809642 ],\n",
      "        [-0.899966  ],\n",
      "   ...47566393],\n",
      "        [ 1.3585775 ],\n",
      "        [-0.2309782 ],\n",
      "        [-0.11626644],\n",
      "        [ 0.13496543]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8ae5a5c0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 0.72634244],\n",
      "        [-0.13514316],\n",
      "        [-2.179011  ],\n",
      "        [ 0.1809642 ],\n",
      "        [-0.899966  ],\n",
      "   ...47566393],\n",
      "        [ 1.3585775 ],\n",
      "        [-0.2309782 ],\n",
      "        [-0.11626644],\n",
      "        [ 0.13496543]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 0.72634244],\n",
      "        [-0.13514316],\n",
      "        [-2.179011  ],\n",
      "        [ 0.1809642 ],\n",
      "        [-0.899966  ],\n",
      "   ...47566393],\n",
      "        [ 1.3585775 ],\n",
      "        [-0.2309782 ],\n",
      "        [-0.11626644],\n",
      "        [ 0.13496543]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 0.72634244],\n",
      "        [-0.13514316],\n",
      "        [-2.179011  ],\n",
      "        [ 0.1809642 ],\n",
      "        [-0.899966  ],\n",
      "   ...47566393],\n",
      "        [ 1.3585775 ],\n",
      "        [-0.2309782 ],\n",
      "        [-0.11626644],\n",
      "        [ 0.13496543]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8ae5a2b0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8ae5a080>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-True-1-34-15-2-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[320., 516., 815., 725., 239.,   1., 184., 948.,  74., 831., 273.,\n",
      "        543., 203., 575., 925.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 0.6992946 ],\n",
      "        [ 1.2664204 ],\n",
      "        [-1.4653152 ],\n",
      "        [ 1.0068598 ],\n",
      "        [ 0.36671335],\n",
      "   ...66416466],\n",
      "        [-1.0358843 ],\n",
      "        [ 1.6342863 ],\n",
      "        [-0.16751502],\n",
      "        [-0.36382276]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d326ac8>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 0.6992946 ],\n",
      "        [ 1.2664204 ],\n",
      "        [-1.4653152 ],\n",
      "        [ 1.0068598 ],\n",
      "        [ 0.36671335],\n",
      "   ...66416466],\n",
      "        [-1.0358843 ],\n",
      "        [ 1.6342863 ],\n",
      "        [-0.16751502],\n",
      "        [-0.36382276]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 0.6992946 ],\n",
      "        [ 1.2664204 ],\n",
      "        [-1.4653152 ],\n",
      "        [ 1.0068598 ],\n",
      "        [ 0.36671335],\n",
      "   ...66416466],\n",
      "        [-1.0358843 ],\n",
      "        [ 1.6342863 ],\n",
      "        [-0.16751502],\n",
      "        [-0.36382276]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 0.6992946 ],\n",
      "        [ 1.2664204 ],\n",
      "        [-1.4653152 ],\n",
      "        [ 1.0068598 ],\n",
      "        [ 0.36671335],\n",
      "   ...66416466],\n",
      "        [-1.0358843 ],\n",
      "        [ 1.6342863 ],\n",
      "        [-0.16751502],\n",
      "        [-0.36382276]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d3269e8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d326630>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-True-1-34-15-2-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[831., 936., 556., 597., 589., 886., 584., 318., 132., 298., 895.,\n",
      "        781., 761.,  85.,  53.],\n",
      "       [ 60...    [ 74., 626., 174., 762., 660., 917., 895., 743., 144., 896., 323.,\n",
      "         74.,  82., 190., 651.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-1.0497457 ],\n",
      "        [ 1.3685943 ],\n",
      "        [ 0.43657884],\n",
      "        [-0.010417  ],\n",
      "        [ 1.06138   ],\n",
      "   ...78456956],\n",
      "        [-0.820344  ],\n",
      "        [-1.5403913 ],\n",
      "        [ 0.80097187],\n",
      "        [ 0.8133989 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8b10df60>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-1.0497457 ],\n",
      "        [ 1.3685943 ],\n",
      "        [ 0.43657884],\n",
      "        [-0.010417  ],\n",
      "        [ 1.06138   ],\n",
      "   ...78456956],\n",
      "        [-0.820344  ],\n",
      "        [-1.5403913 ],\n",
      "        [ 0.80097187],\n",
      "        [ 0.8133989 ]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-1.0497457 ],\n",
      "        [ 1.3685943 ],\n",
      "        [ 0.43657884],\n",
      "        [-0.010417  ],\n",
      "        [ 1.06138   ],\n",
      "   ...78456956],\n",
      "        [-0.820344  ],\n",
      "        [-1.5403913 ],\n",
      "        [ 0.80097187],\n",
      "        [ 0.8133989 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-1.0497457 ],\n",
      "        [ 1.3685943 ],\n",
      "        [ 0.43657884],\n",
      "        [-0.010417  ],\n",
      "        [ 1.06138   ],\n",
      "   ...78456956],\n",
      "        [-0.820344  ],\n",
      "        [-1.5403913 ],\n",
      "        [ 0.80097187],\n",
      "        [ 0.8133989 ]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8b10d3c8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8b10d9b0>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1000-True-12-1-1-1-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[33.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.7138061 , -0.24086373, -0.7090214 , -0.67658436,\n",
      "         -1.0848746 ,  1.6582923 ,  0.06583206,  0.6854416 ,\n",
      "          0.89392   ,  1.3898661 , -1.4250346 , -0.20584404]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d5e2208>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.7138061 , -0.24086373, -0.7090214 , -0.67658436,\n",
      "         -1.0848746 ,  1.6582923 ,  0.06583206,  0.6854416 ,\n",
      "          0.89392   ,  1.3898661 , -1.4250346 , -0.20584404]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.7138061 , -0.24086373, -0.7090214 , -0.67658436,\n",
      "         -1.0848746 ,  1.6582923 ,  0.06583206,  0.6854416 ,\n",
      "          0.89392   ,  1.3898661 , -1.4250346 , -0.20584404]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.7138061 , -0.24086373, -0.7090214 , -0.67658436,\n",
      "         -1.0848746 ,  1.6582923 ,  0.06583206,  0.6854416 ,\n",
      "          0.89392   ,  1.3898661 , -1.4250346 , -0.20584404]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d5e2f60>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d5e2048>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-True-12-1-1-1-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[250.],\n",
      "       [703.],\n",
      "       [982.],\n",
      "       [265.],\n",
      "       [220.],\n",
      "       [297.],\n",
      "       [742.],\n",
      "       [766.],\n",
      "       [842.],\n",
      "       [904.],\n",
      "       [105.],\n",
      "       [372.],\n",
      "       [577.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-1.9070543 ,  1.0752114 , -0.2562794 ,  1.2517192 ,\n",
      "         -0.45458308, -1.5091447 ,  0.7049458 , -0.80138886,\n",
      "         -0.26003924,  1.0978991 , -2.1940794 ,  0.40494555]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8ac3ca58>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-1.9070543 ,  1.0752114 , -0.2562794 ,  1.2517192 ,\n",
      "         -0.45458308, -1.5091447 ,  0.7049458 , -0.80138886,\n",
      "         -0.26003924,  1.0978991 , -2.1940794 ,  0.40494555]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-1.9070543 ,  1.0752114 , -0.2562794 ,  1.2517192 ,\n",
      "         -0.45458308, -1.5091447 ,  0.7049458 , -0.80138886,\n",
      "         -0.26003924,  1.0978991 , -2.1940794 ,  0.40494555]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-1.9070543 ,  1.0752114 , -0.2562794 ,  1.2517192 ,\n",
      "         -0.45458308, -1.5091447 ,  0.7049458 , -0.80138886,\n",
      "         -0.26003924,  1.0978991 , -2.1940794 ,  0.40494555]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8ac3c6d8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8ac3cc88>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1000-True-12-1-1-2-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[934.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-1.0991095 ,  1.2543682 , -1.5750749 , -1.3899041 ,\n",
      "          1.164123  ,  0.00957253, -0.03338727, -0.201030...4556448, -0.8375297 ,  0.5502908 ,\n",
      "          0.38895264,  1.0160891 ,  0.4515461 ,  0.2870734 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8ae150b8>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-1.0991095 ,  1.2543682 , -1.5750749 , -1.3899041 ,\n",
      "          1.164123  ,  0.00957253, -0.03338727, -0.201030...4556448, -0.8375297 ,  0.5502908 ,\n",
      "          0.38895264,  1.0160891 ,  0.4515461 ,  0.2870734 ]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-1.0991095 ,  1.2543682 , -1.5750749 , -1.3899041 ,\n",
      "          1.164123  ,  0.00957253, -0.03338727, -0.201030...4556448, -0.8375297 ,  0.5502908 ,\n",
      "          0.38895264,  1.0160891 ,  0.4515461 ,  0.2870734 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-1.0991095 ,  1.2543682 , -1.5750749 , -1.3899041 ,\n",
      "          1.164123  ,  0.00957253, -0.03338727, -0.201030...4556448, -0.8375297 ,  0.5502908 ,\n",
      "          0.38895264,  1.0160891 ,  0.4515461 ,  0.2870734 ]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8ae155c0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8ae156d8>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-True-12-1-1-2-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[365.],\n",
      "       [237.],\n",
      "       [348.],\n",
      "       [571.],\n",
      "       [350.],\n",
      "       [590.],\n",
      "       [735.],\n",
      "       [788.],\n",
      "       [101.],\n",
      "       [709.],\n",
      "       [584.],\n",
      "       [ 66.],\n",
      "       [315.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 0.1689262 , -1.001154  , -0.7587033 , -1.6255932 ,\n",
      "          1.2565783 ,  0.6995649 ,  0.5030429 ,  0.626314...8791448, -0.03268041, -0.85283965,\n",
      "         -0.2872356 ,  1.3098229 ,  1.3357455 ,  0.00447015]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d587630>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 0.1689262 , -1.001154  , -0.7587033 , -1.6255932 ,\n",
      "          1.2565783 ,  0.6995649 ,  0.5030429 ,  0.626314...8791448, -0.03268041, -0.85283965,\n",
      "         -0.2872356 ,  1.3098229 ,  1.3357455 ,  0.00447015]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 0.1689262 , -1.001154  , -0.7587033 , -1.6255932 ,\n",
      "          1.2565783 ,  0.6995649 ,  0.5030429 ,  0.626314...8791448, -0.03268041, -0.85283965,\n",
      "         -0.2872356 ,  1.3098229 ,  1.3357455 ,  0.00447015]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 0.1689262 , -1.001154  , -0.7587033 , -1.6255932 ,\n",
      "          1.2565783 ,  0.6995649 ,  0.5030429 ,  0.626314...8791448, -0.03268041, -0.85283965,\n",
      "         -0.2872356 ,  1.3098229 ,  1.3357455 ,  0.00447015]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d587e48>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d587ba8>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-True-12-1-15-1-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[675., 464., 908., 704., 827., 868., 803., 180., 491., 137., 721.,\n",
      "         46., 815., 122.,  97.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-9.65714812e-01, -1.00428097e-01,  1.28595755e-01,\n",
      "          5.17875969e-01,  5.41597120e-02, -7.14268625e-01...,  6.89614832e-01,  3.31585050e-01,\n",
      "          2.36305296e-01,  1.94019282e+00,  6.47512972e-01]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8b0def60>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-9.65714812e-01, -1.00428097e-01,  1.28595755e-01,\n",
      "          5.17875969e-01,  5.41597120e-02, -7.14268625e-01...,  6.89614832e-01,  3.31585050e-01,\n",
      "          2.36305296e-01,  1.94019282e+00,  6.47512972e-01]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-9.65714812e-01, -1.00428097e-01,  1.28595755e-01,\n",
      "          5.17875969e-01,  5.41597120e-02, -7.14268625e-01...,  6.89614832e-01,  3.31585050e-01,\n",
      "          2.36305296e-01,  1.94019282e+00,  6.47512972e-01]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-9.65714812e-01, -1.00428097e-01,  1.28595755e-01,\n",
      "          5.17875969e-01,  5.41597120e-02, -7.14268625e-01...,  6.89614832e-01,  3.31585050e-01,\n",
      "          2.36305296e-01,  1.94019282e+00,  6.47512972e-01]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8b0deb70>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8b0dedd8>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-True-12-1-15-1-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[934., 462., 386., 661.,  52., 575., 308., 879., 202., 329., 623.,\n",
      "        818., 390.,  20., 716.],\n",
      "       [432...    [697.,  52., 831., 781., 960., 739., 774., 556.,  36., 390., 571.,\n",
      "        449., 387.,   9., 940.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 1.0681874 ,  1.5543566 , -0.36628494, -1.2630687 ,\n",
      "         -0.75062406,  0.0628167 , -0.99637085, -1.154189...7910711, -0.9085547 , -0.52800107,\n",
      "          1.3525474 ,  0.38367668,  1.0986147 ,  0.3798772 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8b2583c8>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 1.0681874 ,  1.5543566 , -0.36628494, -1.2630687 ,\n",
      "         -0.75062406,  0.0628167 , -0.99637085, -1.154189...7910711, -0.9085547 , -0.52800107,\n",
      "          1.3525474 ,  0.38367668,  1.0986147 ,  0.3798772 ]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 1.0681874 ,  1.5543566 , -0.36628494, -1.2630687 ,\n",
      "         -0.75062406,  0.0628167 , -0.99637085, -1.154189...7910711, -0.9085547 , -0.52800107,\n",
      "          1.3525474 ,  0.38367668,  1.0986147 ,  0.3798772 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 1.0681874 ,  1.5543566 , -0.36628494, -1.2630687 ,\n",
      "         -0.75062406,  0.0628167 , -0.99637085, -1.154189...7910711, -0.9085547 , -0.52800107,\n",
      "          1.3525474 ,  0.38367668,  1.0986147 ,  0.3798772 ]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8b258f28>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8b258b00>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-True-12-1-15-2-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[799., 248., 308., 291., 930., 510., 702., 545.,  20., 855., 813.,\n",
      "        735.,  61., 785., 794.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.5013286 , -0.8286399 , -1.0954722 ,  0.68925804,\n",
      "         -0.30188966,  0.7918972 ,  0.20716813, -1.496440...8698018,  1.8945973 ,  1.5213785 ,\n",
      "         -0.52427495, -0.6757263 , -1.3963691 , -2.0050604 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8ae3c5f8>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.5013286 , -0.8286399 , -1.0954722 ,  0.68925804,\n",
      "         -0.30188966,  0.7918972 ,  0.20716813, -1.496440...8698018,  1.8945973 ,  1.5213785 ,\n",
      "         -0.52427495, -0.6757263 , -1.3963691 , -2.0050604 ]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.5013286 , -0.8286399 , -1.0954722 ,  0.68925804,\n",
      "         -0.30188966,  0.7918972 ,  0.20716813, -1.496440...8698018,  1.8945973 ,  1.5213785 ,\n",
      "         -0.52427495, -0.6757263 , -1.3963691 , -2.0050604 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.5013286 , -0.8286399 , -1.0954722 ,  0.68925804,\n",
      "         -0.30188966,  0.7918972 ,  0.20716813, -1.496440...8698018,  1.8945973 ,  1.5213785 ,\n",
      "         -0.52427495, -0.6757263 , -1.3963691 , -2.0050604 ]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8ae3ce80>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8ae3cb70>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-True-12-1-15-2-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[977., 293., 966., 420., 230., 562., 184., 544., 442., 110., 668.,\n",
      "        307., 177., 414., 843.],\n",
      "       [906...    [263., 245., 192., 831., 910., 190., 186., 331., 604., 414., 459.,\n",
      "         79., 797.,   1., 250.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 5.90693116e-01,  1.22664817e-01,  1.73309720e+00,\n",
      "          6.09328389e-01, -4.12741929e-01,  2.27794141e-01...,  1.63803279e+00, -6.47923231e-01,\n",
      "         -1.82867026e+00,  1.81311920e-01, -1.13568097e-01]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8ac3cc88>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 5.90693116e-01,  1.22664817e-01,  1.73309720e+00,\n",
      "          6.09328389e-01, -4.12741929e-01,  2.27794141e-01...,  1.63803279e+00, -6.47923231e-01,\n",
      "         -1.82867026e+00,  1.81311920e-01, -1.13568097e-01]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 5.90693116e-01,  1.22664817e-01,  1.73309720e+00,\n",
      "          6.09328389e-01, -4.12741929e-01,  2.27794141e-01...,  1.63803279e+00, -6.47923231e-01,\n",
      "         -1.82867026e+00,  1.81311920e-01, -1.13568097e-01]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 5.90693116e-01,  1.22664817e-01,  1.73309720e+00,\n",
      "          6.09328389e-01, -4.12741929e-01,  2.27794141e-01...,  1.63803279e+00, -6.47923231e-01,\n",
      "         -1.82867026e+00,  1.81311920e-01, -1.13568097e-01]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8ac3cfd0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8ac3c0f0>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-True-12-34-1-1-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[566.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-1.6667969 , -1.3250464 ,  0.98492754,  1.0869243 ,\n",
      "          0.21766606,  0.4910481 ,  1.1988369 ,  0.64854455,\n",
      "         -0.39885178,  0.25866136,  1.8743582 , -1.4345589 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8ae5afd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-1.6667969 , -1.3250464 ,  0.98492754,  1.0869243 ,\n",
      "          0.21766606,  0.4910481 ,  1.1988369 ,  0.64854455,\n",
      "         -0.39885178,  0.25866136,  1.8743582 , -1.4345589 ]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-1.6667969 , -1.3250464 ,  0.98492754,  1.0869243 ,\n",
      "          0.21766606,  0.4910481 ,  1.1988369 ,  0.64854455,\n",
      "         -0.39885178,  0.25866136,  1.8743582 , -1.4345589 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-1.6667969 , -1.3250464 ,  0.98492754,  1.0869243 ,\n",
      "          0.21766606,  0.4910481 ,  1.1988369 ,  0.64854455,\n",
      "         -0.39885178,  0.25866136,  1.8743582 , -1.4345589 ]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8ae5aac8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8ae5add8>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-True-12-34-1-1-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[435.],\n",
      "       [499.],\n",
      "       [213.],\n",
      "       [907.],\n",
      "       [478.],\n",
      "       [369.],\n",
      "       [383.],\n",
      "       [258.],\n",
      "       [257.],\n",
      "       [804.],\n",
      "       [924.],\n",
      "       [898.],\n",
      "       [912.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 0.7536368 ,  0.76691604,  3.2105064 ,  0.4822608 ,\n",
      "          0.9521163 ,  0.744351  , -0.9634694 ,  0.6114993 ,\n",
      "         -0.46803448, -0.21328144,  2.505515  ,  1.6485617 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8ad32fd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 0.7536368 ,  0.76691604,  3.2105064 ,  0.4822608 ,\n",
      "          0.9521163 ,  0.744351  , -0.9634694 ,  0.6114993 ,\n",
      "         -0.46803448, -0.21328144,  2.505515  ,  1.6485617 ]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 0.7536368 ,  0.76691604,  3.2105064 ,  0.4822608 ,\n",
      "          0.9521163 ,  0.744351  , -0.9634694 ,  0.6114993 ,\n",
      "         -0.46803448, -0.21328144,  2.505515  ,  1.6485617 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 0.7536368 ,  0.76691604,  3.2105064 ,  0.4822608 ,\n",
      "          0.9521163 ,  0.744351  , -0.9634694 ,  0.6114993 ,\n",
      "         -0.46803448, -0.21328144,  2.505515  ,  1.6485617 ]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8ad32ac8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8ad32cf8>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-True-12-34-1-2-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[104.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-2.0066702e-01,  7.6966476e-01, -1.8232611e-01,  4.2683050e-01,\n",
      "          9.8343152e-01, -1.8782246e-01, -3.3...8e+00, -1.3647034e+00,\n",
      "         -8.9923337e-02,  6.7360826e-02, -1.3777690e+00, -2.4909416e-05]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d4c6be0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-2.0066702e-01,  7.6966476e-01, -1.8232611e-01,  4.2683050e-01,\n",
      "          9.8343152e-01, -1.8782246e-01, -3.3...8e+00, -1.3647034e+00,\n",
      "         -8.9923337e-02,  6.7360826e-02, -1.3777690e+00, -2.4909416e-05]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-2.0066702e-01,  7.6966476e-01, -1.8232611e-01,  4.2683050e-01,\n",
      "          9.8343152e-01, -1.8782246e-01, -3.3...8e+00, -1.3647034e+00,\n",
      "         -8.9923337e-02,  6.7360826e-02, -1.3777690e+00, -2.4909416e-05]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-2.0066702e-01,  7.6966476e-01, -1.8232611e-01,  4.2683050e-01,\n",
      "          9.8343152e-01, -1.8782246e-01, -3.3...8e+00, -1.3647034e+00,\n",
      "         -8.9923337e-02,  6.7360826e-02, -1.3777690e+00, -2.4909416e-05]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d4c6780>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d4c61d0>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-True-12-34-1-2-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[586.],\n",
      "       [ 11.],\n",
      "       [203.],\n",
      "       [ 44.],\n",
      "       [540.],\n",
      "       [823.],\n",
      "       [210.],\n",
      "       [954.],\n",
      "       [156.],\n",
      "       [811.],\n",
      "       [824.],\n",
      "       [732.],\n",
      "       [805.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.50250554, -0.7415666 ,  1.0027959 , -0.3707321 ,\n",
      "         -2.7051864 ,  0.6855072 ,  0.07577466, -1.568926...190121 , -0.57094276, -0.7773199 ,\n",
      "          0.9305322 ,  0.6432953 ,  0.12133072, -1.0996108 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8b2454e0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.50250554, -0.7415666 ,  1.0027959 , -0.3707321 ,\n",
      "         -2.7051864 ,  0.6855072 ,  0.07577466, -1.568926...190121 , -0.57094276, -0.7773199 ,\n",
      "          0.9305322 ,  0.6432953 ,  0.12133072, -1.0996108 ]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.50250554, -0.7415666 ,  1.0027959 , -0.3707321 ,\n",
      "         -2.7051864 ,  0.6855072 ,  0.07577466, -1.568926...190121 , -0.57094276, -0.7773199 ,\n",
      "          0.9305322 ,  0.6432953 ,  0.12133072, -1.0996108 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.50250554, -0.7415666 ,  1.0027959 , -0.3707321 ,\n",
      "         -2.7051864 ,  0.6855072 ,  0.07577466, -1.568926...190121 , -0.57094276, -0.7773199 ,\n",
      "          0.9305322 ,  0.6432953 ,  0.12133072, -1.0996108 ]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8b2453c8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8b245e10>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-True-12-34-15-1-1] _____\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[407., 708., 686., 355., 884., 686., 695., 626., 782., 426., 327.,\n",
      "        753.,  95., 414., 439.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 0.4860054 , -1.6011932 , -0.33752802,  0.8111815 ,\n",
      "         -0.22616865,  0.5214507 , -0.97383535, -0.526102...967581 , -0.3274214 , -1.7533367 ,\n",
      "          0.15208913, -0.53292716, -0.95903903, -0.49045333]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d508470>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 0.4860054 , -1.6011932 , -0.33752802,  0.8111815 ,\n",
      "         -0.22616865,  0.5214507 , -0.97383535, -0.526102...967581 , -0.3274214 , -1.7533367 ,\n",
      "          0.15208913, -0.53292716, -0.95903903, -0.49045333]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 0.4860054 , -1.6011932 , -0.33752802,  0.8111815 ,\n",
      "         -0.22616865,  0.5214507 , -0.97383535, -0.526102...967581 , -0.3274214 , -1.7533367 ,\n",
      "          0.15208913, -0.53292716, -0.95903903, -0.49045333]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 0.4860054 , -1.6011932 , -0.33752802,  0.8111815 ,\n",
      "         -0.22616865,  0.5214507 , -0.97383535, -0.526102...967581 , -0.3274214 , -1.7533367 ,\n",
      "          0.15208913, -0.53292716, -0.95903903, -0.49045333]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d5086a0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d508940>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m____ test_language_model_implementation[cuda-lstm-1000-True-12-34-15-1-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[984., 609., 691., 483., 139., 865., 831., 550., 844., 276., 894.,\n",
      "        193., 489., 918., 899.],\n",
      "       [160...    [158., 276., 359.,  28., 756., 990., 264., 698., 459., 671., 775.,\n",
      "        387., 820., 945., 567.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.65919214,  0.06181195, -0.13365759,  1.2852452 ,\n",
      "         -0.25239217, -1.7359922 , -1.6281452 ,  1.558709...374017 ,  0.96047723,  1.1328028 ,\n",
      "         -1.7458587 , -0.93301666, -0.21680404, -0.11257702]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8acb34a8>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.65919214,  0.06181195, -0.13365759,  1.2852452 ,\n",
      "         -0.25239217, -1.7359922 , -1.6281452 ,  1.558709...374017 ,  0.96047723,  1.1328028 ,\n",
      "         -1.7458587 , -0.93301666, -0.21680404, -0.11257702]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.65919214,  0.06181195, -0.13365759,  1.2852452 ,\n",
      "         -0.25239217, -1.7359922 , -1.6281452 ,  1.558709...374017 ,  0.96047723,  1.1328028 ,\n",
      "         -1.7458587 , -0.93301666, -0.21680404, -0.11257702]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.65919214,  0.06181195, -0.13365759,  1.2852452 ,\n",
      "         -0.25239217, -1.7359922 , -1.6281452 ,  1.558709...374017 ,  0.96047723,  1.1328028 ,\n",
      "         -1.7458587 , -0.93301666, -0.21680404, -0.11257702]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8acb3780>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8acb3278>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-True-12-34-15-2-1] _____\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[428., 465., 652., 371., 794., 653., 425., 687., 718., 339., 866.,\n",
      "        675., 633., 477., 425.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 1.09547961e+00, -8.84383142e-01, -7.56642282e-01,\n",
      "         -1.16909409e+00, -2.36852670e+00,  9.67211366e-01..., -1.08414602e+00, -3.28215271e-01,\n",
      "          2.60959387e-01,  8.16624045e-01,  2.05115294e+00]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8ad01f60>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 1.09547961e+00, -8.84383142e-01, -7.56642282e-01,\n",
      "         -1.16909409e+00, -2.36852670e+00,  9.67211366e-01..., -1.08414602e+00, -3.28215271e-01,\n",
      "          2.60959387e-01,  8.16624045e-01,  2.05115294e+00]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 1.09547961e+00, -8.84383142e-01, -7.56642282e-01,\n",
      "         -1.16909409e+00, -2.36852670e+00,  9.67211366e-01..., -1.08414602e+00, -3.28215271e-01,\n",
      "          2.60959387e-01,  8.16624045e-01,  2.05115294e+00]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 1.09547961e+00, -8.84383142e-01, -7.56642282e-01,\n",
      "         -1.16909409e+00, -2.36852670e+00,  9.67211366e-01..., -1.08414602e+00, -3.28215271e-01,\n",
      "          2.60959387e-01,  8.16624045e-01,  2.05115294e+00]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8ad01860>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8ad01b00>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m____ test_language_model_implementation[cuda-lstm-1000-True-12-34-15-2-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = True, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = True\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[430., 551., 918.,   3., 701., 873., 682., 900., 168., 265., 460.,\n",
      "        851., 671., 148., 784.],\n",
      "       [888...    [837., 526., 346.,  17., 281.,  17.,  65., 583., 501.,  29., 185.,\n",
      "        173., 622., 663., 356.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-8.3115137e-01,  6.2781286e-01,  8.0249429e-01, -1.2124137e+00,\n",
      "         -1.4607522e+00,  1.7994241e-01, -1.2...1e-01,  6.5016562e-01,\n",
      "          6.5391278e-01, -7.6754108e-02, -7.2728819e-01, -1.4987494e+00]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d5e2cc0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-8.3115137e-01,  6.2781286e-01,  8.0249429e-01, -1.2124137e+00,\n",
      "         -1.4607522e+00,  1.7994241e-01, -1.2...1e-01,  6.5016562e-01,\n",
      "          6.5391278e-01, -7.6754108e-02, -7.2728819e-01, -1.4987494e+00]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-8.3115137e-01,  6.2781286e-01,  8.0249429e-01, -1.2124137e+00,\n",
      "         -1.4607522e+00,  1.7994241e-01, -1.2...1e-01,  6.5016562e-01,\n",
      "          6.5391278e-01, -7.6754108e-02, -7.2728819e-01, -1.4987494e+00]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-8.3115137e-01,  6.2781286e-01,  8.0249429e-01, -1.2124137e+00,\n",
      "         -1.4607522e+00,  1.7994241e-01, -1.2...1e-01,  6.5016562e-01,\n",
      "          6.5391278e-01, -7.6754108e-02, -7.2728819e-01, -1.4987494e+00]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d5e2a20>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d5e2dd8>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1000-False-1-1-1-1-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[564.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[0.9272222]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8ace2518>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[0.9272222]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[0.9272222]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[0.9272222]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8ace2da0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8ace2048>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-False-1-1-1-1-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[619.],\n",
      "       [852.],\n",
      "       [130.],\n",
      "       [123.],\n",
      "       [855.],\n",
      "       [975.],\n",
      "       [446.],\n",
      "       [852.],\n",
      "       [575.],\n",
      "       [308.],\n",
      "       [521.],\n",
      "       [410.],\n",
      "       [578.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[0.02780652]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8b0c9e10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[0.02780652]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[0.02780652]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[0.02780652]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8b0c96a0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8b0c94e0>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m______ test_language_model_implementation[cuda-lstm-1000-False-1-1-1-2-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[283.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[0.17224811]],\n",
      "\n",
      "       [[0.6422051 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d3cc710>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[0.17224811]],\n",
      "\n",
      "       [[0.6422051 ]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[0.17224811]],\n",
      "\n",
      "       [[0.6422051 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[0.17224811]],\n",
      "\n",
      "       [[0.6422051 ]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d3cc2b0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d3cc588>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-False-1-1-1-2-13] ______\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[614.],\n",
      "       [399.],\n",
      "       [ 71.],\n",
      "       [814.],\n",
      "       [432.],\n",
      "       [597.],\n",
      "       [183.],\n",
      "       [426.],\n",
      "       [947.],\n",
      "       [679.],\n",
      "       [946.],\n",
      "       [392.],\n",
      "       [246.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-1.5552299]],\n",
      "\n",
      "       [[-1.50454  ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d3d2d68>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-1.5552299]],\n",
      "\n",
      "       [[-1.50454  ]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-1.5552299]],\n",
      "\n",
      "       [[-1.50454  ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-1.5552299]],\n",
      "\n",
      "       [[-1.50454  ]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d3d2ef0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d3d2908>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-False-1-1-15-1-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[898., 473., 678., 311.,  88., 807., 298., 972., 137., 545., 389.,\n",
      "        794., 498., 583.,  49.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 0.51138943],\n",
      "        [-0.03414934],\n",
      "        [-0.30454758],\n",
      "        [ 2.876814  ],\n",
      "        [-0.752105  ],\n",
      "   ...2334465 ],\n",
      "        [ 0.6652737 ],\n",
      "        [ 0.20186949],\n",
      "        [ 0.8439124 ],\n",
      "        [-0.80926925]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d40cdd8>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 0.51138943],\n",
      "        [-0.03414934],\n",
      "        [-0.30454758],\n",
      "        [ 2.876814  ],\n",
      "        [-0.752105  ],\n",
      "   ...2334465 ],\n",
      "        [ 0.6652737 ],\n",
      "        [ 0.20186949],\n",
      "        [ 0.8439124 ],\n",
      "        [-0.80926925]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 0.51138943],\n",
      "        [-0.03414934],\n",
      "        [-0.30454758],\n",
      "        [ 2.876814  ],\n",
      "        [-0.752105  ],\n",
      "   ...2334465 ],\n",
      "        [ 0.6652737 ],\n",
      "        [ 0.20186949],\n",
      "        [ 0.8439124 ],\n",
      "        [-0.80926925]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 0.51138943],\n",
      "        [-0.03414934],\n",
      "        [-0.30454758],\n",
      "        [ 2.876814  ],\n",
      "        [-0.752105  ],\n",
      "   ...2334465 ],\n",
      "        [ 0.6652737 ],\n",
      "        [ 0.20186949],\n",
      "        [ 0.8439124 ],\n",
      "        [-0.80926925]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d40c518>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d40c278>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-False-1-1-15-1-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[550., 379., 608., 391., 690., 413., 587., 507.,  46., 508., 368.,\n",
      "        306., 445., 376., 791.],\n",
      "       [463...    [ 13., 877., 589., 351., 263., 835., 800., 332., 542., 296., 242.,\n",
      "         93., 861., 160., 850.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-1.7166108 ],\n",
      "        [ 0.32721403],\n",
      "        [ 0.9756467 ],\n",
      "        [ 0.6003078 ],\n",
      "        [ 0.2164864 ],\n",
      "   ...38340178],\n",
      "        [ 1.3261706 ],\n",
      "        [ 2.2401228 ],\n",
      "        [ 0.12449867],\n",
      "        [ 0.9051684 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8b2557f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-1.7166108 ],\n",
      "        [ 0.32721403],\n",
      "        [ 0.9756467 ],\n",
      "        [ 0.6003078 ],\n",
      "        [ 0.2164864 ],\n",
      "   ...38340178],\n",
      "        [ 1.3261706 ],\n",
      "        [ 2.2401228 ],\n",
      "        [ 0.12449867],\n",
      "        [ 0.9051684 ]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-1.7166108 ],\n",
      "        [ 0.32721403],\n",
      "        [ 0.9756467 ],\n",
      "        [ 0.6003078 ],\n",
      "        [ 0.2164864 ],\n",
      "   ...38340178],\n",
      "        [ 1.3261706 ],\n",
      "        [ 2.2401228 ],\n",
      "        [ 0.12449867],\n",
      "        [ 0.9051684 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-1.7166108 ],\n",
      "        [ 0.32721403],\n",
      "        [ 0.9756467 ],\n",
      "        [ 0.6003078 ],\n",
      "        [ 0.2164864 ],\n",
      "   ...38340178],\n",
      "        [ 1.3261706 ],\n",
      "        [ 2.2401228 ],\n",
      "        [ 0.12449867],\n",
      "        [ 0.9051684 ]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8b255dd8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8b255b38>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-False-1-1-15-2-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[329., 655., 322., 602.,  11., 767., 772., 164., 944.,  55., 647.,\n",
      "        855., 350., 194., 895.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.13732886],\n",
      "        [-1.1084292 ],\n",
      "        [-0.46207207],\n",
      "        [ 1.086624  ],\n",
      "        [-0.07534718],\n",
      "   ...2199476 ],\n",
      "        [-0.689692  ],\n",
      "        [-0.32876462],\n",
      "        [ 0.9368756 ],\n",
      "        [ 0.52216554]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8ada3240>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.13732886],\n",
      "        [-1.1084292 ],\n",
      "        [-0.46207207],\n",
      "        [ 1.086624  ],\n",
      "        [-0.07534718],\n",
      "   ...2199476 ],\n",
      "        [-0.689692  ],\n",
      "        [-0.32876462],\n",
      "        [ 0.9368756 ],\n",
      "        [ 0.52216554]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.13732886],\n",
      "        [-1.1084292 ],\n",
      "        [-0.46207207],\n",
      "        [ 1.086624  ],\n",
      "        [-0.07534718],\n",
      "   ...2199476 ],\n",
      "        [-0.689692  ],\n",
      "        [-0.32876462],\n",
      "        [ 0.9368756 ],\n",
      "        [ 0.52216554]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.13732886],\n",
      "        [-1.1084292 ],\n",
      "        [-0.46207207],\n",
      "        [ 1.086624  ],\n",
      "        [-0.07534718],\n",
      "   ...2199476 ],\n",
      "        [-0.689692  ],\n",
      "        [-0.32876462],\n",
      "        [ 0.9368756 ],\n",
      "        [ 0.52216554]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8ada3f98>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8ada3b70>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-False-1-1-15-2-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[323., 668., 387., 176., 979., 304., 325., 156., 224., 438., 440.,\n",
      "        186., 823., 324., 538.],\n",
      "       [406...    [511., 160., 214., 666., 666., 160., 508., 940., 837., 242., 856.,\n",
      "        205., 561., 340., 405.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 2.2655444e+00],\n",
      "        [-1.5832118e+00],\n",
      "        [ 1.1370721e+00],\n",
      "        [-1.4239240e+00],\n",
      "        [ 3.79...       [-4.6513751e-01],\n",
      "        [ 5.2969712e-01],\n",
      "        [-6.1115515e-01],\n",
      "        [ 3.4611627e-01]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d78c208>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 2.2655444e+00],\n",
      "        [-1.5832118e+00],\n",
      "        [ 1.1370721e+00],\n",
      "        [-1.4239240e+00],\n",
      "        [ 3.79...       [-4.6513751e-01],\n",
      "        [ 5.2969712e-01],\n",
      "        [-6.1115515e-01],\n",
      "        [ 3.4611627e-01]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 2.2655444e+00],\n",
      "        [-1.5832118e+00],\n",
      "        [ 1.1370721e+00],\n",
      "        [-1.4239240e+00],\n",
      "        [ 3.79...       [-4.6513751e-01],\n",
      "        [ 5.2969712e-01],\n",
      "        [-6.1115515e-01],\n",
      "        [ 3.4611627e-01]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 2.2655444e+00],\n",
      "        [-1.5832118e+00],\n",
      "        [ 1.1370721e+00],\n",
      "        [-1.4239240e+00],\n",
      "        [ 3.79...       [-4.6513751e-01],\n",
      "        [ 5.2969712e-01],\n",
      "        [-6.1115515e-01],\n",
      "        [ 3.4611627e-01]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d78c9e8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d78c240>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-False-1-34-1-1-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[215.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.22720382]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8ae51208>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.22720382]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.22720382]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.22720382]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8ae51518>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8ae51400>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-False-1-34-1-1-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[ 12.],\n",
      "       [721.],\n",
      "       [ 75.],\n",
      "       [646.],\n",
      "       [ 93.],\n",
      "       [273.],\n",
      "       [835.],\n",
      "       [478.],\n",
      "       [533.],\n",
      "       [961.],\n",
      "       [775.],\n",
      "       [116.],\n",
      "       [460.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-1.9013019]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d544780>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-1.9013019]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-1.9013019]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-1.9013019]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d544c88>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d5444e0>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-False-1-34-1-2-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[927.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[1.6413583]],\n",
      "\n",
      "       [[1.4867277]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8b16a630>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[1.6413583]],\n",
      "\n",
      "       [[1.4867277]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[1.6413583]],\n",
      "\n",
      "       [[1.4867277]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[1.6413583]],\n",
      "\n",
      "       [[1.4867277]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8b16a710>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8b16a940>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-False-1-34-1-2-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[141.],\n",
      "       [685.],\n",
      "       [473.],\n",
      "       [506.],\n",
      "       [178.],\n",
      "       [981.],\n",
      "       [ 65.],\n",
      "       [163.],\n",
      "       [628.],\n",
      "       [392.],\n",
      "       [383.],\n",
      "       [192.],\n",
      "       [930.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-1.6871709 ]],\n",
      "\n",
      "       [[ 0.76479757]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d6acb70>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-1.6871709 ]],\n",
      "\n",
      "       [[ 0.76479757]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-1.6871709 ]],\n",
      "\n",
      "       [[ 0.76479757]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-1.6871709 ]],\n",
      "\n",
      "       [[ 0.76479757]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d6ac978>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d6acf28>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-False-1-34-15-1-1] _____\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[294., 367., 271., 507., 709., 829., 659., 688., 284., 191., 144.,\n",
      "        261., 535., 923., 159.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 0.23590918],\n",
      "        [ 0.34589934],\n",
      "        [-0.6117064 ],\n",
      "        [ 2.2789056 ],\n",
      "        [-0.25866723],\n",
      "   ...15578696],\n",
      "        [ 0.26430735],\n",
      "        [-0.39048657],\n",
      "        [ 1.2098739 ],\n",
      "        [-0.7644635 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d59ada0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 0.23590918],\n",
      "        [ 0.34589934],\n",
      "        [-0.6117064 ],\n",
      "        [ 2.2789056 ],\n",
      "        [-0.25866723],\n",
      "   ...15578696],\n",
      "        [ 0.26430735],\n",
      "        [-0.39048657],\n",
      "        [ 1.2098739 ],\n",
      "        [-0.7644635 ]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 0.23590918],\n",
      "        [ 0.34589934],\n",
      "        [-0.6117064 ],\n",
      "        [ 2.2789056 ],\n",
      "        [-0.25866723],\n",
      "   ...15578696],\n",
      "        [ 0.26430735],\n",
      "        [-0.39048657],\n",
      "        [ 1.2098739 ],\n",
      "        [-0.7644635 ]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 0.23590918],\n",
      "        [ 0.34589934],\n",
      "        [-0.6117064 ],\n",
      "        [ 2.2789056 ],\n",
      "        [-0.25866723],\n",
      "   ...15578696],\n",
      "        [ 0.26430735],\n",
      "        [-0.39048657],\n",
      "        [ 1.2098739 ],\n",
      "        [-0.7644635 ]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d59a2e8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d59a908>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m____ test_language_model_implementation[cuda-lstm-1000-False-1-34-15-1-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[920., 511.,  21., 681., 282., 670., 226., 640., 690., 566., 118.,\n",
      "        579., 399., 808., 131.],\n",
      "       [646...    [544., 857., 559.,  93., 258., 256., 800., 791., 119., 881., 894.,\n",
      "        265., 923., 961., 439.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.49766627],\n",
      "        [ 0.7672754 ],\n",
      "        [ 0.3499684 ],\n",
      "        [-2.1536634 ],\n",
      "        [ 1.0046171 ],\n",
      "   ...5861499 ],\n",
      "        [ 1.6191143 ],\n",
      "        [ 1.4700346 ],\n",
      "        [-1.0027895 ],\n",
      "        [-0.21761966]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d3595c0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.49766627],\n",
      "        [ 0.7672754 ],\n",
      "        [ 0.3499684 ],\n",
      "        [-2.1536634 ],\n",
      "        [ 1.0046171 ],\n",
      "   ...5861499 ],\n",
      "        [ 1.6191143 ],\n",
      "        [ 1.4700346 ],\n",
      "        [-1.0027895 ],\n",
      "        [-0.21761966]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.49766627],\n",
      "        [ 0.7672754 ],\n",
      "        [ 0.3499684 ],\n",
      "        [-2.1536634 ],\n",
      "        [ 1.0046171 ],\n",
      "   ...5861499 ],\n",
      "        [ 1.6191143 ],\n",
      "        [ 1.4700346 ],\n",
      "        [-1.0027895 ],\n",
      "        [-0.21761966]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.49766627],\n",
      "        [ 0.7672754 ],\n",
      "        [ 0.3499684 ],\n",
      "        [-2.1536634 ],\n",
      "        [ 1.0046171 ],\n",
      "   ...5861499 ],\n",
      "        [ 1.6191143 ],\n",
      "        [ 1.4700346 ],\n",
      "        [-1.0027895 ],\n",
      "        [-0.21761966]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d359f60>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d359978>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-False-1-34-15-2-1] _____\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[215., 209., 372., 310.,  75., 398., 366., 528., 423., 531., 356.,\n",
      "        826., 401., 960., 397.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 1.9211054 ],\n",
      "        [ 0.10442182],\n",
      "        [ 1.0727307 ],\n",
      "        [-0.54887116],\n",
      "        [ 0.48162174],\n",
      "   ...25071496],\n",
      "        [ 0.19778271],\n",
      "        [ 0.44749984],\n",
      "        [ 2.1660905 ],\n",
      "        [-0.79024416]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d269b70>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 1.9211054 ],\n",
      "        [ 0.10442182],\n",
      "        [ 1.0727307 ],\n",
      "        [-0.54887116],\n",
      "        [ 0.48162174],\n",
      "   ...25071496],\n",
      "        [ 0.19778271],\n",
      "        [ 0.44749984],\n",
      "        [ 2.1660905 ],\n",
      "        [-0.79024416]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 1.9211054 ],\n",
      "        [ 0.10442182],\n",
      "        [ 1.0727307 ],\n",
      "        [-0.54887116],\n",
      "        [ 0.48162174],\n",
      "   ...25071496],\n",
      "        [ 0.19778271],\n",
      "        [ 0.44749984],\n",
      "        [ 2.1660905 ],\n",
      "        [-0.79024416]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 1.9211054 ],\n",
      "        [ 0.10442182],\n",
      "        [ 1.0727307 ],\n",
      "        [-0.54887116],\n",
      "        [ 0.48162174],\n",
      "   ...25071496],\n",
      "        [ 0.19778271],\n",
      "        [ 0.44749984],\n",
      "        [ 2.1660905 ],\n",
      "        [-0.79024416]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d269748>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d269048>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m____ test_language_model_implementation[cuda-lstm-1000-False-1-34-15-2-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 1, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 1\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[237., 531., 807., 168., 408., 360.,  10., 225., 564., 434., 767.,\n",
      "        393., 329., 448., 468.],\n",
      "       [614...    [966., 541., 327., 872., 308., 969., 115., 912., 654., 946., 848.,\n",
      "        177., 268.,  65., 489.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 0.5838339 ],\n",
      "        [ 0.1497075 ],\n",
      "        [-1.3932012 ],\n",
      "        [ 0.62941283],\n",
      "        [-0.40277472],\n",
      "   ...05995597],\n",
      "        [-0.3066807 ],\n",
      "        [ 0.04291795],\n",
      "        [ 1.0060195 ],\n",
      "        [ 0.87645894]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d3e81d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 0.5838339 ],\n",
      "        [ 0.1497075 ],\n",
      "        [-1.3932012 ],\n",
      "        [ 0.62941283],\n",
      "        [-0.40277472],\n",
      "   ...05995597],\n",
      "        [-0.3066807 ],\n",
      "        [ 0.04291795],\n",
      "        [ 1.0060195 ],\n",
      "        [ 0.87645894]]], dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 0.5838339 ],\n",
      "        [ 0.1497075 ],\n",
      "        [-1.3932012 ],\n",
      "        [ 0.62941283],\n",
      "        [-0.40277472],\n",
      "   ...05995597],\n",
      "        [-0.3066807 ],\n",
      "        [ 0.04291795],\n",
      "        [ 1.0060195 ],\n",
      "        [ 0.87645894]]], dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 0.5838339 ],\n",
      "        [ 0.1497075 ],\n",
      "        [-1.3932012 ],\n",
      "        [ 0.62941283],\n",
      "        [-0.40277472],\n",
      "   ...05995597],\n",
      "        [-0.3066807 ],\n",
      "        [ 0.04291795],\n",
      "        [ 1.0060195 ],\n",
      "        [ 0.87645894]]], dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d3e84e0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d3e85c0>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 1)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-False-12-1-1-1-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[632.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 1.388944  , -0.76524436,  0.83874834, -0.40492004,\n",
      "          0.2675963 , -0.3443797 ,  0.6086456 , -0.14311802,\n",
      "          1.1985304 ,  0.15665591, -1.6804028 ,  0.01562924]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8b037f60>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 1.388944  , -0.76524436,  0.83874834, -0.40492004,\n",
      "          0.2675963 , -0.3443797 ,  0.6086456 , -0.14311802,\n",
      "          1.1985304 ,  0.15665591, -1.6804028 ,  0.01562924]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 1.388944  , -0.76524436,  0.83874834, -0.40492004,\n",
      "          0.2675963 , -0.3443797 ,  0.6086456 , -0.14311802,\n",
      "          1.1985304 ,  0.15665591, -1.6804028 ,  0.01562924]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 1.388944  , -0.76524436,  0.83874834, -0.40492004,\n",
      "          0.2675963 , -0.3443797 ,  0.6086456 , -0.14311802,\n",
      "          1.1985304 ,  0.15665591, -1.6804028 ,  0.01562924]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8b0375f8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8b0373c8>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-False-12-1-1-1-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[819.],\n",
      "       [788.],\n",
      "       [549.],\n",
      "       [570.],\n",
      "       [850.],\n",
      "       [669.],\n",
      "       [756.],\n",
      "       [303.],\n",
      "       [226.],\n",
      "       [364.],\n",
      "       [500.],\n",
      "       [756.],\n",
      "       [707.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 0.5434824 , -0.90302956, -0.01138967,  0.7810198 ,\n",
      "         -0.6125424 ,  0.4171599 , -0.63671726, -0.55767286,\n",
      "          1.678727  , -1.0472424 , -0.57300663,  0.97506386]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8b221080>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 0.5434824 , -0.90302956, -0.01138967,  0.7810198 ,\n",
      "         -0.6125424 ,  0.4171599 , -0.63671726, -0.55767286,\n",
      "          1.678727  , -1.0472424 , -0.57300663,  0.97506386]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 0.5434824 , -0.90302956, -0.01138967,  0.7810198 ,\n",
      "         -0.6125424 ,  0.4171599 , -0.63671726, -0.55767286,\n",
      "          1.678727  , -1.0472424 , -0.57300663,  0.97506386]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 0.5434824 , -0.90302956, -0.01138967,  0.7810198 ,\n",
      "         -0.6125424 ,  0.4171599 , -0.63671726, -0.55767286,\n",
      "          1.678727  , -1.0472424 , -0.57300663,  0.97506386]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8b221828>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8b221320>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-False-12-1-1-2-1] ______\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[632.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.22465594,  1.7891347 ,  0.6485463 , -1.1920544 ,\n",
      "         -1.9294453 ,  2.1346529 ,  1.1554763 , -0.573944...843988 , -2.0699487 ,  0.80934614,\n",
      "          1.3003781 ,  0.49971232, -0.2858143 ,  0.7385824 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d4cdd30>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.22465594,  1.7891347 ,  0.6485463 , -1.1920544 ,\n",
      "         -1.9294453 ,  2.1346529 ,  1.1554763 , -0.573944...843988 , -2.0699487 ,  0.80934614,\n",
      "          1.3003781 ,  0.49971232, -0.2858143 ,  0.7385824 ]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.22465594,  1.7891347 ,  0.6485463 , -1.1920544 ,\n",
      "         -1.9294453 ,  2.1346529 ,  1.1554763 , -0.573944...843988 , -2.0699487 ,  0.80934614,\n",
      "          1.3003781 ,  0.49971232, -0.2858143 ,  0.7385824 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.22465594,  1.7891347 ,  0.6485463 , -1.1920544 ,\n",
      "         -1.9294453 ,  2.1346529 ,  1.1554763 , -0.573944...843988 , -2.0699487 ,  0.80934614,\n",
      "          1.3003781 ,  0.49971232, -0.2858143 ,  0.7385824 ]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d4cdeb8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d4cda58>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-False-12-1-1-2-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[333.],\n",
      "       [753.],\n",
      "       [111.],\n",
      "       [646.],\n",
      "       [907.],\n",
      "       [732.],\n",
      "       [ 34.],\n",
      "       [235.],\n",
      "       [835.],\n",
      "       [389.],\n",
      "       [336.],\n",
      "       [806.],\n",
      "       [670.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-1.0188289 ,  0.89513063, -0.2776584 , -0.3819723 ,\n",
      "         -1.0882984 , -0.39703542,  0.31273592,  1.318599...8743197,  2.0705097 , -0.3238069 ,\n",
      "         -1.4388506 , -0.7190753 ,  0.2698152 ,  0.29297078]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d3ad7f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-1.0188289 ,  0.89513063, -0.2776584 , -0.3819723 ,\n",
      "         -1.0882984 , -0.39703542,  0.31273592,  1.318599...8743197,  2.0705097 , -0.3238069 ,\n",
      "         -1.4388506 , -0.7190753 ,  0.2698152 ,  0.29297078]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-1.0188289 ,  0.89513063, -0.2776584 , -0.3819723 ,\n",
      "         -1.0882984 , -0.39703542,  0.31273592,  1.318599...8743197,  2.0705097 , -0.3238069 ,\n",
      "         -1.4388506 , -0.7190753 ,  0.2698152 ,  0.29297078]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-1.0188289 ,  0.89513063, -0.2776584 , -0.3819723 ,\n",
      "         -1.0882984 , -0.39703542,  0.31273592,  1.318599...8743197,  2.0705097 , -0.3238069 ,\n",
      "         -1.4388506 , -0.7190753 ,  0.2698152 ,  0.29297078]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d3adb00>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d3adeb8>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-False-12-1-15-1-1] _____\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[821., 577., 596., 993., 884., 154., 158., 130., 392., 554., 727.,\n",
      "        125., 140., 743., 446.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 0.42344618, -2.587184  , -0.29531583,  0.4444802 ,\n",
      "         -1.4465609 , -0.88451576,  0.11218829, -0.340389...5795924,  1.3459939 ,  0.5844008 ,\n",
      "          0.8643748 ,  1.976839  , -0.6755344 ,  0.10073332]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d5e06d8>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 0.42344618, -2.587184  , -0.29531583,  0.4444802 ,\n",
      "         -1.4465609 , -0.88451576,  0.11218829, -0.340389...5795924,  1.3459939 ,  0.5844008 ,\n",
      "          0.8643748 ,  1.976839  , -0.6755344 ,  0.10073332]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 0.42344618, -2.587184  , -0.29531583,  0.4444802 ,\n",
      "         -1.4465609 , -0.88451576,  0.11218829, -0.340389...5795924,  1.3459939 ,  0.5844008 ,\n",
      "          0.8643748 ,  1.976839  , -0.6755344 ,  0.10073332]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 0.42344618, -2.587184  , -0.29531583,  0.4444802 ,\n",
      "         -1.4465609 , -0.88451576,  0.11218829, -0.340389...5795924,  1.3459939 ,  0.5844008 ,\n",
      "          0.8643748 ,  1.976839  , -0.6755344 ,  0.10073332]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d5e0438>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d5e04e0>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m____ test_language_model_implementation[cuda-lstm-1000-False-12-1-15-1-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[870., 821., 649., 492., 667., 885.,  49., 510.,  57., 201.,  39.,\n",
      "        852., 333., 425., 972.],\n",
      "       [ 77...    [853., 278., 292., 384.,  99., 624., 233., 278., 390., 779., 698.,\n",
      "        448., 969., 824., 751.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-0.5015644 ,  1.2296699 , -1.5446382 , -0.5011927 ,\n",
      "         -0.09567667, -1.8259264 ,  1.913582  , -0.278159...5999722,  1.9063629 ,  0.7180814 ,\n",
      "          0.66050684,  0.9080658 , -0.8474498 ,  0.23751181]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d45b748>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-0.5015644 ,  1.2296699 , -1.5446382 , -0.5011927 ,\n",
      "         -0.09567667, -1.8259264 ,  1.913582  , -0.278159...5999722,  1.9063629 ,  0.7180814 ,\n",
      "          0.66050684,  0.9080658 , -0.8474498 ,  0.23751181]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-0.5015644 ,  1.2296699 , -1.5446382 , -0.5011927 ,\n",
      "         -0.09567667, -1.8259264 ,  1.913582  , -0.278159...5999722,  1.9063629 ,  0.7180814 ,\n",
      "          0.66050684,  0.9080658 , -0.8474498 ,  0.23751181]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-0.5015644 ,  1.2296699 , -1.5446382 , -0.5011927 ,\n",
      "         -0.09567667, -1.8259264 ,  1.913582  , -0.278159...5999722,  1.9063629 ,  0.7180814 ,\n",
      "          0.66050684,  0.9080658 , -0.8474498 ,  0.23751181]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d45bb38>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d45b9b0>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-False-12-1-15-2-1] _____\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[724., 162., 860., 257., 140., 812., 597., 735., 990., 587., 576.,\n",
      "        994., 927., 785., 985.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-1.51543140e-01,  1.28207314e+00, -1.59692419e+00,\n",
      "          1.20008981e+00, -1.81616163e+00,  1.09141421e+00...,  5.65833092e-01,  1.85627425e+00,\n",
      "         -5.66362143e-01,  5.50619423e-01, -7.88102567e-01]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d418eb8>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-1.51543140e-01,  1.28207314e+00, -1.59692419e+00,\n",
      "          1.20008981e+00, -1.81616163e+00,  1.09141421e+00...,  5.65833092e-01,  1.85627425e+00,\n",
      "         -5.66362143e-01,  5.50619423e-01, -7.88102567e-01]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-1.51543140e-01,  1.28207314e+00, -1.59692419e+00,\n",
      "          1.20008981e+00, -1.81616163e+00,  1.09141421e+00...,  5.65833092e-01,  1.85627425e+00,\n",
      "         -5.66362143e-01,  5.50619423e-01, -7.88102567e-01]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-1.51543140e-01,  1.28207314e+00, -1.59692419e+00,\n",
      "          1.20008981e+00, -1.81616163e+00,  1.09141421e+00...,  5.65833092e-01,  1.85627425e+00,\n",
      "         -5.66362143e-01,  5.50619423e-01, -7.88102567e-01]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d418c50>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d4184e0>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m____ test_language_model_implementation[cuda-lstm-1000-False-12-1-15-2-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 1\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 1\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[723., 962., 466., 248., 353., 154., 470., 470., 398., 821., 255.,\n",
      "        743., 337., 264., 726.],\n",
      "       [562...    [141., 204., 737., 844., 672., 250., 735., 804., 912., 551., 290.,\n",
      "        577., 524., 138., 705.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 0.07629122,  1.7428094 ,  0.38245586, -0.53633744,\n",
      "         -0.64437795,  0.05095977,  0.07594675,  0.452247...7105873, -0.33630577, -0.81370604,\n",
      "         -0.14923298,  2.2280686 , -1.3461709 ,  0.7302462 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d5f1cf8>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 0.07629122,  1.7428094 ,  0.38245586, -0.53633744,\n",
      "         -0.64437795,  0.05095977,  0.07594675,  0.452247...7105873, -0.33630577, -0.81370604,\n",
      "         -0.14923298,  2.2280686 , -1.3461709 ,  0.7302462 ]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 0.07629122,  1.7428094 ,  0.38245586, -0.53633744,\n",
      "         -0.64437795,  0.05095977,  0.07594675,  0.452247...7105873, -0.33630577, -0.81370604,\n",
      "         -0.14923298,  2.2280686 , -1.3461709 ,  0.7302462 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 0.07629122,  1.7428094 ,  0.38245586, -0.53633744,\n",
      "         -0.64437795,  0.05095977,  0.07594675,  0.452247...7105873, -0.33630577, -0.81370604,\n",
      "         -0.14923298,  2.2280686 , -1.3461709 ,  0.7302462 ]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d5f1470>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d5f1eb8>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-False-12-34-1-1-1] _____\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[633.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 0.35962197,  2.3663435 ,  1.6574209 ,  1.394777  ,\n",
      "         -1.346097  , -1.611356  , -0.82053065,  1.6524422 ,\n",
      "         -0.60838664, -1.7328794 ,  0.2919224 ,  1.0094744 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d5e0518>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 0.35962197,  2.3663435 ,  1.6574209 ,  1.394777  ,\n",
      "         -1.346097  , -1.611356  , -0.82053065,  1.6524422 ,\n",
      "         -0.60838664, -1.7328794 ,  0.2919224 ,  1.0094744 ]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 0.35962197,  2.3663435 ,  1.6574209 ,  1.394777  ,\n",
      "         -1.346097  , -1.611356  , -0.82053065,  1.6524422 ,\n",
      "         -0.60838664, -1.7328794 ,  0.2919224 ,  1.0094744 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 0.35962197,  2.3663435 ,  1.6574209 ,  1.394777  ,\n",
      "         -1.346097  , -1.611356  , -0.82053065,  1.6524422 ,\n",
      "         -0.60838664, -1.7328794 ,  0.2919224 ,  1.0094744 ]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d5e0f60>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d5e0e10>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m____ test_language_model_implementation[cuda-lstm-1000-False-12-34-1-1-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[155.],\n",
      "       [  4.],\n",
      "       [236.],\n",
      "       [371.],\n",
      "       [176.],\n",
      "       [741.],\n",
      "       [393.],\n",
      "       [198.],\n",
      "       [297.],\n",
      "       [224.],\n",
      "       [451.],\n",
      "       [601.],\n",
      "       [730.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-1.3438392 ,  0.9547628 ,  0.10100742,  0.7766729 ,\n",
      "          1.6927975 , -0.77051556,  0.39050722,  0.9969335 ,\n",
      "          0.9657367 , -0.30037007,  1.4082862 ,  2.374305  ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d55eef0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-1.3438392 ,  0.9547628 ,  0.10100742,  0.7766729 ,\n",
      "          1.6927975 , -0.77051556,  0.39050722,  0.9969335 ,\n",
      "          0.9657367 , -0.30037007,  1.4082862 ,  2.374305  ]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-1.3438392 ,  0.9547628 ,  0.10100742,  0.7766729 ,\n",
      "          1.6927975 , -0.77051556,  0.39050722,  0.9969335 ,\n",
      "          0.9657367 , -0.30037007,  1.4082862 ,  2.374305  ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-1.3438392 ,  0.9547628 ,  0.10100742,  0.7766729 ,\n",
      "          1.6927975 , -0.77051556,  0.39050722,  0.9969335 ,\n",
      "          0.9657367 , -0.30037007,  1.4082862 ,  2.374305  ]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d55e1d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d55efd0>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m_____ test_language_model_implementation[cuda-lstm-1000-False-12-34-1-2-1] _____\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[397.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-1.6156385 , -0.08126643,  1.8059918 ,  0.3455656 ,\n",
      "          1.8365146 , -0.38325456, -0.4102737 , -0.483481...6724154,  0.3641616 , -0.40732273,\n",
      "          1.1137831 , -0.2052587 ,  1.3017876 , -0.0927721 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8b1f6358>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-1.6156385 , -0.08126643,  1.8059918 ,  0.3455656 ,\n",
      "          1.8365146 , -0.38325456, -0.4102737 , -0.483481...6724154,  0.3641616 , -0.40732273,\n",
      "          1.1137831 , -0.2052587 ,  1.3017876 , -0.0927721 ]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-1.6156385 , -0.08126643,  1.8059918 ,  0.3455656 ,\n",
      "          1.8365146 , -0.38325456, -0.4102737 , -0.483481...6724154,  0.3641616 , -0.40732273,\n",
      "          1.1137831 , -0.2052587 ,  1.3017876 , -0.0927721 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-1.6156385 , -0.08126643,  1.8059918 ,  0.3455656 ,\n",
      "          1.8365146 , -0.38325456, -0.4102737 , -0.483481...6724154,  0.3641616 , -0.40732273,\n",
      "          1.1137831 , -0.2052587 ,  1.3017876 , -0.0927721 ]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8b1f64e0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8b1f6048>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m____ test_language_model_implementation[cuda-lstm-1000-False-12-34-1-2-13] _____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 1, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 1\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[250.],\n",
      "       [629.],\n",
      "       [564.],\n",
      "       [662.],\n",
      "       [177.],\n",
      "       [717.],\n",
      "       [709.],\n",
      "       [500.],\n",
      "       [126.],\n",
      "       [614.],\n",
      "       [100.],\n",
      "       [181.],\n",
      "       [ 69.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-1.3066019 , -0.87876225,  1.2483509 , -0.2183744 ,\n",
      "         -0.590254  ,  0.4179806 , -1.4771485 ,  1.614698...4257194,  0.26137403,  0.48842788,\n",
      "          0.7181879 , -0.70505667,  0.58952147, -0.29969308]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8b037710>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-1.3066019 , -0.87876225,  1.2483509 , -0.2183744 ,\n",
      "         -0.590254  ,  0.4179806 , -1.4771485 ,  1.614698...4257194,  0.26137403,  0.48842788,\n",
      "          0.7181879 , -0.70505667,  0.58952147, -0.29969308]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-1.3066019 , -0.87876225,  1.2483509 , -0.2183744 ,\n",
      "         -0.590254  ,  0.4179806 , -1.4771485 ,  1.614698...4257194,  0.26137403,  0.48842788,\n",
      "          0.7181879 , -0.70505667,  0.58952147, -0.29969308]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-1.3066019 , -0.87876225,  1.2483509 , -0.2183744 ,\n",
      "         -0.590254  ,  0.4179806 , -1.4771485 ,  1.614698...4257194,  0.26137403,  0.48842788,\n",
      "          0.7181879 , -0.70505667,  0.58952147, -0.29969308]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8b0376a0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 1, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8b037550>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 1, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m____ test_language_model_implementation[cuda-lstm-1000-False-12-34-15-1-1] _____\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[ 67., 575., 832., 509., 281.,  18., 365., 663.,  95., 123., 292.,\n",
      "        923., 302., 539., 708.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 1.5386734 ,  1.8683058 ,  0.5080669 , -1.4183364 ,\n",
      "         -1.654312  , -1.3801564 ,  1.7506928 , -1.584529...2329262, -0.40238795,  1.4115063 ,\n",
      "         -0.12396099,  0.3916568 ,  1.2968179 ,  0.7823726 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d5145f8>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 1.5386734 ,  1.8683058 ,  0.5080669 , -1.4183364 ,\n",
      "         -1.654312  , -1.3801564 ,  1.7506928 , -1.584529...2329262, -0.40238795,  1.4115063 ,\n",
      "         -0.12396099,  0.3916568 ,  1.2968179 ,  0.7823726 ]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 1.5386734 ,  1.8683058 ,  0.5080669 , -1.4183364 ,\n",
      "         -1.654312  , -1.3801564 ,  1.7506928 , -1.584529...2329262, -0.40238795,  1.4115063 ,\n",
      "         -0.12396099,  0.3916568 ,  1.2968179 ,  0.7823726 ]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 1.5386734 ,  1.8683058 ,  0.5080669 , -1.4183364 ,\n",
      "         -1.654312  , -1.3801564 ,  1.7506928 , -1.584529...2329262, -0.40238795,  1.4115063 ,\n",
      "         -0.12396099,  0.3916568 ,  1.2968179 ,  0.7823726 ]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d5140b8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d514860>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m____ test_language_model_implementation[cuda-lstm-1000-False-12-34-15-1-13] ____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 1, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 1\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[771., 810., 912., 922., 632., 478.,  50., 185., 680., 956., 378.,\n",
      "        868., 748.,  44., 849.],\n",
      "       [635...    [677., 176., 175.,  14., 459.,  27., 610., 649., 767., 880., 914.,\n",
      "         76., 477., 218., 381.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 0.21686646, -0.3372052 ,  0.14367637, -0.39406192,\n",
      "         -0.13881642, -0.35374323, -0.23482503, -0.152881...5210365,  0.5637346 ,  1.4084085 ,\n",
      "         -0.6449753 , -0.03192849,  0.10765493,  0.08213001]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d41b160>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 0.21686646, -0.3372052 ,  0.14367637, -0.39406192,\n",
      "         -0.13881642, -0.35374323, -0.23482503, -0.152881...5210365,  0.5637346 ,  1.4084085 ,\n",
      "         -0.6449753 , -0.03192849,  0.10765493,  0.08213001]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 0.21686646, -0.3372052 ,  0.14367637, -0.39406192,\n",
      "         -0.13881642, -0.35374323, -0.23482503, -0.152881...5210365,  0.5637346 ,  1.4084085 ,\n",
      "         -0.6449753 , -0.03192849,  0.10765493,  0.08213001]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 0.21686646, -0.3372052 ,  0.14367637, -0.39406192,\n",
      "         -0.13881642, -0.35374323, -0.23482503, -0.152881...5210365,  0.5637346 ,  1.4084085 ,\n",
      "         -0.6449753 , -0.03192849,  0.10765493,  0.08213001]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d41beb8>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (1, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d41b1d0>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (1, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m____ test_language_model_implementation[cuda-lstm-1000-False-12-34-15-2-1] _____\u001b[0m\n",
      "\n",
      "seq_length = 1, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 1\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[978., 133.,  69., 503., 595., 980., 813., 395., 113., 355., 560.,\n",
      "         18., 836., 409.,   6.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[ 0.7066065 ,  1.2766488 ,  0.94271946,  0.9662822 ,\n",
      "         -0.48906347, -0.35709256, -1.7767147 ,  1.568041...8246005,  1.5393779 , -0.42222878,\n",
      "         -0.5034033 ,  0.8515449 ,  0.35435647,  0.97507167]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8ae15ac8>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[ 0.7066065 ,  1.2766488 ,  0.94271946,  0.9662822 ,\n",
      "         -0.48906347, -0.35709256, -1.7767147 ,  1.568041...8246005,  1.5393779 , -0.42222878,\n",
      "         -0.5034033 ,  0.8515449 ,  0.35435647,  0.97507167]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[ 0.7066065 ,  1.2766488 ,  0.94271946,  0.9662822 ,\n",
      "         -0.48906347, -0.35709256, -1.7767147 ,  1.568041...8246005,  1.5393779 , -0.42222878,\n",
      "         -0.5034033 ,  0.8515449 ,  0.35435647,  0.97507167]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[ 0.7066065 ,  1.2766488 ,  0.94271946,  0.9662822 ,\n",
      "         -0.48906347, -0.35709256, -1.7767147 ,  1.568041...8246005,  1.5393779 , -0.42222878,\n",
      "         -0.5034033 ,  0.8515449 ,  0.35435647,  0.97507167]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8ae15be0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8ae15c88>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[31m\u001b[1m____ test_language_model_implementation[cuda-lstm-1000-False-12-34-15-2-13] ____\u001b[0m\n",
      "\n",
      "seq_length = 13, num_layers = 2, batch_size = 15, embedding_size = 34\n",
      "hidden_size = 12, init_hidden = False, output_size = 1000, seq_model = 'lstm'\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_LENGTHS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, NUM_LAYERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BATCH_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33membedding_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EMBEDDING_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, HIDDEN_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minit_hidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, INIT_HIDDEN)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OUTPUT_SIZES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SEQ_MODEL)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_implementation\u001b[39;49;00m(seq_length, num_layers, batch_size, embedding_size, hidden_size,\n",
      "                            init_hidden, output_size, seq_model, device):\n",
      "        \u001b[90m#TODO add test for just nn.embedding?\u001b[39;49;00m\n",
      "        x = np.random.randint(\u001b[94m0\u001b[39;49;00m, output_size, (seq_length, batch_size)).astype(np.float32)\n",
      ">       h0 = ndl.Tensor(np.random.randn(num_layers, batch_size, hidden_size).astype(np.float32), device=device)\n",
      "\n",
      "batch_size = 15\n",
      "device     = cuda()\n",
      "embedding_size = 34\n",
      "hidden_size = 12\n",
      "init_hidden = False\n",
      "num_layers = 2\n",
      "output_size = 1000\n",
      "seq_length = 13\n",
      "seq_model  = 'lstm'\n",
      "x          = array([[278., 893., 295., 134., 272., 517., 180., 653.,  94., 182., 439.,\n",
      "        450., 330., 310., 495.],\n",
      "       [ 27...    [638., 628., 199., 906., 681., 570., 389., 825., 234., 865., 631.,\n",
      "        168.,  48., 422., 813.]], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:198: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:222: in __init__\n",
      "    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
      "        array      = array([[[-4.77665544e-01,  4.98866260e-01,  7.32702851e-01,\n",
      "         -1.30887401e+00, -1.71155512e+00,  3.22051823e-01...,  1.92327881e+00, -2.90142119e-01,\n",
      "          5.93998790e-01,  3.24658811e-01,  8.73721123e-01]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        kwargs     = {}\n",
      "        requires_grad = True\n",
      "        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7fed8d6ef828>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:235: in _array_from_numpy\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = None\n",
      "        numpy_array = array([[[-4.77665544e-01,  4.98866260e-01,  7.32702851e-01,\n",
      "         -1.30887401e+00, -1.71155512e+00,  3.22051823e-01...,  1.92327881e+00, -2.90142119e-01,\n",
      "          5.93998790e-01,  3.24658811e-01,  8.73721123e-01]]],\n",
      "      dtype=float32)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:645: in array\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\n",
      "        a          = array([[[-4.77665544e-01,  4.98866260e-01,  7.32702851e-01,\n",
      "         -1.30887401e+00, -1.71155512e+00,  3.22051823e-01...,  1.92327881e+00, -2.90142119e-01,\n",
      "          5.93998790e-01,  3.24658811e-01,  8.73721123e-01]]],\n",
      "      dtype=float32)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:107: in __init__\n",
      "    array = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\n",
      "        device     = cuda()\n",
      "        other      = array([[[-4.77665544e-01,  4.98866260e-01,  7.32702851e-01,\n",
      "         -1.30887401e+00, -1.71155512e+00,  3.22051823e-01...,  1.92327881e+00, -2.90142119e-01,\n",
      "          5.93998790e-01,  3.24658811e-01,  8.73721123e-01]]],\n",
      "      dtype=float32)\n",
      "        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fed8d6efa58>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 15, 12), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\n",
      "        array._offset = offset\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      ">           array._handle = array.device.Array(prod(shape))\n",
      "\u001b[1m\u001b[31mE           RuntimeError: an illegal memory access was encountered\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fed8d6ef5f8>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = (2, 15, 12)\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:144: RuntimeError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-rnn-1000-False-12-34-1-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-rnn-1000-False-12-34-1-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-rnn-1000-False-12-34-15-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-rnn-1000-False-12-34-15-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-rnn-1000-False-12-34-15-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-rnn-1000-False-12-34-15-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-1-1-1-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-1-1-1-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-1-1-1-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-1-1-1-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-1-1-15-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-1-1-15-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-1-1-15-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-1-1-15-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-1-34-1-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-1-34-1-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-1-34-1-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-1-34-1-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-1-34-15-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-1-34-15-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-1-34-15-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-1-34-15-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-12-1-1-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-12-1-1-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-12-1-1-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-12-1-1-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-12-1-15-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-12-1-15-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-12-1-15-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-12-1-15-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-12-34-1-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-12-34-1-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-12-34-1-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-12-34-1-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-12-34-15-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-12-34-15-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-12-34-15-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-True-12-34-15-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-1-1-1-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-1-1-1-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-1-1-1-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-1-1-1-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-1-1-15-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-1-1-15-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-1-1-15-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-1-1-15-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-1-34-1-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-1-34-1-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-1-34-1-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-1-34-1-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-1-34-15-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-1-34-15-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-1-34-15-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-1-34-15-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-12-1-1-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-12-1-1-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-12-1-1-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-12-1-1-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-12-1-15-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-12-1-15-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-12-1-15-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-12-1-15-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-12-34-1-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-12-34-1-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-12-34-1-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-12-34-1-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-12-34-15-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-12-34-15-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-12-34-15-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1-False-12-34-15-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-1-1-1-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-1-1-1-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-1-1-1-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-1-1-1-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-1-1-15-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-1-1-15-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-1-1-15-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-1-1-15-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-1-34-1-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-1-34-1-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-1-34-1-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-1-34-1-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-1-34-15-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-1-34-15-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-1-34-15-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-1-34-15-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-12-1-1-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-12-1-1-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-12-1-1-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-12-1-1-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-12-1-15-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-12-1-15-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-12-1-15-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-12-1-15-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-12-34-1-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-12-34-1-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-12-34-1-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-12-34-1-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-12-34-15-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-12-34-15-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-12-34-15-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-True-12-34-15-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-1-1-1-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-1-1-1-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-1-1-1-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-1-1-1-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-1-1-15-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-1-1-15-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-1-1-15-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-1-1-15-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-1-34-1-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-1-34-1-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-1-34-1-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-1-34-1-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-1-34-15-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-1-34-15-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-1-34-15-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-1-34-15-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-12-1-1-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-12-1-1-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-12-1-1-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-12-1-1-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-12-1-15-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-12-1-15-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-12-1-15-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-12-1-15-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-12-34-1-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-12-34-1-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-12-34-1-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-12-34-1-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-12-34-15-1-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-12-34-15-1-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-12-34-15-2-1]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_implementation[cuda-lstm-1000-False-12-34-15-2-13]\u001b[0m - RuntimeError: an illegal memory access was encountered\n",
      "\u001b[31m============== \u001b[31m\u001b[1m134 failed\u001b[0m, \u001b[32m378 passed\u001b[0m, \u001b[33m1291 deselected\u001b[0m\u001b[31m in 24.23s\u001b[0m\u001b[31m ===============\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"language_model_implementation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.0, pytest-7.2.0, pluggy-1.0.0 -- /home/gehao/anaconda3/envs/lxy/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/gehao/lxy/dls/homework4\n",
      "plugins: hydra-core-1.0.7, anyio-3.6.2\n",
      "collected 1803 items / 1801 deselected / 2 selected                            \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_sequence_models.py::test_language_model_training[cpu] \u001b[31mFAILED\u001b[0m\u001b[31m  [ 50%]\u001b[0m\n",
      "tests/test_sequence_models.py::test_language_model_training[cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m______________________ test_language_model_training[cpu] _______________________\u001b[0m\n",
      "\n",
      "device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_training\u001b[39;49;00m(device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        corpus = ndl.data.Corpus(\u001b[33m\"\u001b[39;49;00m\u001b[33mdata/ptb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, max_lines=\u001b[94m20\u001b[39;49;00m)\n",
      "        seq_len = \u001b[94m10\u001b[39;49;00m\n",
      "        num_examples = \u001b[94m100\u001b[39;49;00m\n",
      "        batch_size = \u001b[94m16\u001b[39;49;00m\n",
      "        seq_model = \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "        num_layers = \u001b[94m2\u001b[39;49;00m\n",
      "        hidden_size = \u001b[94m10\u001b[39;49;00m\n",
      "        n_epochs=\u001b[94m2\u001b[39;49;00m\n",
      "        train_data = ndl.data.batchify(corpus.train, batch_size=batch_size, device=device, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        model = LanguageModel(\u001b[94m30\u001b[39;49;00m, \u001b[96mlen\u001b[39;49;00m(corpus.dictionary), hidden_size=hidden_size, num_layers=num_layers, seq_model=seq_model, device=device)\n",
      "        train_acc, train_loss = train_ptb(model, train_data, seq_len=seq_len, n_epochs=n_epochs, device=device)\n",
      "        test_acc, test_loss = evaluate_ptb(model, train_data, seq_len=seq_len, device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mstr\u001b[39;49;00m(device) == \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu()\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      ">           np.testing.assert_allclose(\u001b[94m5.711512\u001b[39;49;00m, train_loss, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 1 / 1 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference: 0.04005289\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference: 0.00696382\u001b[0m\n",
      "\u001b[1m\u001b[31mE            x: array(5.711512)\u001b[0m\n",
      "\u001b[1m\u001b[31mE            y: array([5.751565], dtype=float32)\u001b[0m\n",
      "\n",
      "batch_size = 16\n",
      "corpus     = <needle.data.Corpus object at 0x7f83f82f0080>\n",
      "device     = cpu()\n",
      "hidden_size = 10\n",
      "model      = <models.LanguageModel object at 0x7f83f830bbe0>\n",
      "n_epochs   = 2\n",
      "num_examples = 100\n",
      "num_layers = 2\n",
      "seq_len    = 10\n",
      "seq_model  = 'rnn'\n",
      "test_acc   = 0.0525\n",
      "test_loss  = array([5.4645195], dtype=float32)\n",
      "train_acc  = 0.04\n",
      "train_data = array([[  0,  26,  24,  60,  79,  91, 108, 120, 131, 147, 158, 165, 181,\n",
      "         87, 197,  26],\n",
      "       [  1,  27,  47...      196, 169, 214],\n",
      "       [ 25,  46,  35,  78,  78,  26, 101, 130,  26, 157,  73, 180, 105,\n",
      "         32, 206, 148]])\n",
      "train_loss = array([5.751565], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:240: AssertionError\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "0  correct: 0.005  loss: [6.103507]\n",
      "1  correct: 0.04  loss: [5.751565]\n",
      "\u001b[31m\u001b[1m______________________ test_language_model_training[cuda] ______________________\u001b[0m\n",
      "\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_language_model_training\u001b[39;49;00m(device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        corpus = ndl.data.Corpus(\u001b[33m\"\u001b[39;49;00m\u001b[33mdata/ptb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, max_lines=\u001b[94m20\u001b[39;49;00m)\n",
      "        seq_len = \u001b[94m10\u001b[39;49;00m\n",
      "        num_examples = \u001b[94m100\u001b[39;49;00m\n",
      "        batch_size = \u001b[94m16\u001b[39;49;00m\n",
      "        seq_model = \u001b[33m'\u001b[39;49;00m\u001b[33mrnn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "        num_layers = \u001b[94m2\u001b[39;49;00m\n",
      "        hidden_size = \u001b[94m10\u001b[39;49;00m\n",
      "        n_epochs=\u001b[94m2\u001b[39;49;00m\n",
      "        train_data = ndl.data.batchify(corpus.train, batch_size=batch_size, device=device, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        model = LanguageModel(\u001b[94m30\u001b[39;49;00m, \u001b[96mlen\u001b[39;49;00m(corpus.dictionary), hidden_size=hidden_size, num_layers=num_layers, seq_model=seq_model, device=device)\n",
      "        train_acc, train_loss = train_ptb(model, train_data, seq_len=seq_len, n_epochs=n_epochs, device=device)\n",
      "        test_acc, test_loss = evaluate_ptb(model, train_data, seq_len=seq_len, device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mstr\u001b[39;49;00m(device) == \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu()\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "            np.testing.assert_allclose(\u001b[94m5.711512\u001b[39;49;00m, train_loss, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "            np.testing.assert_allclose(\u001b[94m5.388685\u001b[39;49;00m, test_loss, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "        \u001b[94melif\u001b[39;49;00m \u001b[96mstr\u001b[39;49;00m(device) == \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda()\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      ">           np.testing.assert_allclose(\u001b[94m5.711512\u001b[39;49;00m, train_loss, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 1 / 1 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference: 0.04005337\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference: 0.00696391\u001b[0m\n",
      "\u001b[1m\u001b[31mE            x: array(5.711512)\u001b[0m\n",
      "\u001b[1m\u001b[31mE            y: array([5.751565], dtype=float32)\u001b[0m\n",
      "\n",
      "batch_size = 16\n",
      "corpus     = <needle.data.Corpus object at 0x7f83f8138240>\n",
      "device     = cuda()\n",
      "hidden_size = 10\n",
      "model      = <models.LanguageModel object at 0x7f83f8112748>\n",
      "n_epochs   = 2\n",
      "num_examples = 100\n",
      "num_layers = 2\n",
      "seq_len    = 10\n",
      "seq_model  = 'rnn'\n",
      "test_acc   = 0.0525\n",
      "test_loss  = array([5.4645205], dtype=float32)\n",
      "train_acc  = 0.04\n",
      "train_data = array([[  0,  26,  24,  60,  79,  91, 108, 120, 131, 147, 158, 165, 181,\n",
      "         87, 197,  26],\n",
      "       [  1,  27,  47...      196, 169, 214],\n",
      "       [ 25,  46,  35,  78,  78,  26, 101, 130,  26, 157,  73, 180, 105,\n",
      "         32, 206, 148]])\n",
      "train_loss = array([5.7515655], dtype=float32)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_sequence_models.py\u001b[0m:243: AssertionError\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "0  correct: 0.005  loss: [6.1035066]\n",
      "1  correct: 0.04  loss: [5.7515655]\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_training[cpu]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/test_sequence_models.py::\u001b[1mtest_language_model_training[cuda]\u001b[0m - AssertionError: \n",
      "\u001b[31m====================== \u001b[31m\u001b[1m2 failed\u001b[0m, \u001b[33m1801 deselected\u001b[0m\u001b[31m in 2.68s\u001b[0m\u001b[31m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"language_model_training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.0, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /home/gehao/lxy/dls/homework4\n",
      "plugins: hydra-core-1.0.7, anyio-3.6.2\n",
      "collected 10 items / 9 deselected / 1 selected                                 \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_sequence_models.py \n",
      "Submitting language_model...\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "Grader test 3 passed\n",
      "Grader test 4 passed\n",
      "Grader test 5 passed\n",
      "Grader test 6 passed\n",
      "Grader test 7 passed\n",
      "Grader test 8 passed\n",
      "Grader test 9 passed\n",
      "Grader test 10 passed\n",
      "0  correct: 0.0196078431372549  loss: [5.833036]\n",
      "1  correct: 0.04656862745098039  loss: [5.47369]\n",
      "Grader test 11 passed\n",
      "Grader test 12 passed\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m9 deselected\u001b[0m\u001b[32m in 23.39s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"language_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can train your language model on the Penn Treebank dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import needle as ndl\n",
    "sys.path.append('./apps')\n",
    "from models import LanguageModel\n",
    "from simple_training import train_ptb, evaluate_ptb\n",
    "\n",
    "device = ndl.cpu()\n",
    "corpus = ndl.data.Corpus(\"data/ptb\")\n",
    "train_data = ndl.data.batchify(corpus.train, batch_size=16, device=ndl.cpu(), dtype=\"float32\")\n",
    "model = LanguageModel(30, len(corpus.dictionary), hidden_size=10, num_layers=2, seq_model='rnn', device=ndl.cpu())\n",
    "train_ptb(model, train_data, seq_len=1, n_epochs=1, device=device)\n",
    "evaluate_ptb(model, train_data, seq_len=40, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('lxy': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "601b499b2199c856df16ec98313964189b457c31a1c25fec9d7f71bee01fcfcc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
